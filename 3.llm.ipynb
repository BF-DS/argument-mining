{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "\n",
    "# eigene Module\n",
    "from src.dataimport import list_files_with_extension_directory, load_text, list_files\n",
    "from src.llmlib import num_tokens_from_string, generate_batch_input, split_jsonl_file, upload_batch_file, create_batch, check_batch_status, retrieve_and_save_batch_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dateien laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT_FILES_PATH = 'data/original/'\n",
    "JSON_FILES_PATH = 'data/transformed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Text-Dateien: 402\n",
      "Anzahl Brat-Dateien: 402\n"
     ]
    }
   ],
   "source": [
    "txt_files_directory_list = list_files_with_extension_directory(TXT_FILES_PATH, '.txt')\n",
    "json_files_directory_list = list_files_with_extension_directory(JSON_FILES_PATH, '.json')\n",
    "\n",
    "print(f\"Anzahl Text-Dateien: {len(txt_files_directory_list)}\")\n",
    "print(f\"Anzahl Brat-Dateien: {len(json_files_directory_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(402, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt_path</th>\n",
       "      <th>json_path</th>\n",
       "      <th>txt_file</th>\n",
       "      <th>json_file</th>\n",
       "      <th>txt</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/original/essay001.txt</td>\n",
       "      <td>data/transformed/essay001.json</td>\n",
       "      <td>essay001.txt</td>\n",
       "      <td>essay001.json</td>\n",
       "      <td>Should students be taught to compete or to coo...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/original/essay002.txt</td>\n",
       "      <td>data/transformed/essay002.json</td>\n",
       "      <td>essay002.txt</td>\n",
       "      <td>essay002.json</td>\n",
       "      <td>More people are migrating to other countries t...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/original/essay003.txt</td>\n",
       "      <td>data/transformed/essay003.json</td>\n",
       "      <td>essay003.txt</td>\n",
       "      <td>essay003.json</td>\n",
       "      <td>International tourism is now more common than ...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/original/essay004.txt</td>\n",
       "      <td>data/transformed/essay004.json</td>\n",
       "      <td>essay004.txt</td>\n",
       "      <td>essay004.json</td>\n",
       "      <td>International tourism is now more common than ...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/original/essay005.txt</td>\n",
       "      <td>data/transformed/essay005.json</td>\n",
       "      <td>essay005.txt</td>\n",
       "      <td>essay005.json</td>\n",
       "      <td>Living and studying overseas\\n\\nIt is every st...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     txt_path                       json_path      txt_file  \\\n",
       "0  data/original/essay001.txt  data/transformed/essay001.json  essay001.txt   \n",
       "1  data/original/essay002.txt  data/transformed/essay002.json  essay002.txt   \n",
       "2  data/original/essay003.txt  data/transformed/essay003.json  essay003.txt   \n",
       "3  data/original/essay004.txt  data/transformed/essay004.json  essay004.txt   \n",
       "4  data/original/essay005.txt  data/transformed/essay005.json  essay005.txt   \n",
       "\n",
       "       json_file                                                txt  \\\n",
       "0  essay001.json  Should students be taught to compete or to coo...   \n",
       "1  essay002.json  More people are migrating to other countries t...   \n",
       "2  essay003.json  International tourism is now more common than ...   \n",
       "3  essay004.json  International tourism is now more common than ...   \n",
       "4  essay005.json  Living and studying overseas\\n\\nIt is every st...   \n",
       "\n",
       "                                                json  \n",
       "0  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "1  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "2  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "3  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "4  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe mit Name und Inhalt der Text- und ann-Dateien erstellen \n",
    "df = pd.DataFrame()\n",
    "df['txt_path'] = txt_files_directory_list\n",
    "df['json_path'] = json_files_directory_list\n",
    "df['txt_file'] = df['txt_path'].apply(lambda x: os.path.basename(x))\n",
    "df['json_file'] = df['json_path'].apply(lambda x: os.path.basename(x))\n",
    "df['txt'] = df['txt_path'].apply(load_text)\n",
    "df['json'] = df['json_path'].apply(load_text)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufteilung in Trainings- und Testdatensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DataFrame: (40, 6)\n",
      "\n",
      "Test DataFrame: (362, 6)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, train_size=40, random_state=42)\n",
    "\n",
    "# Formen der DataFrames\n",
    "print(f\"Training DataFrame: {train_df.shape}\")\n",
    "print(f\"\\nTest DataFrame: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt_path</th>\n",
       "      <th>json_path</th>\n",
       "      <th>txt_file</th>\n",
       "      <th>json_file</th>\n",
       "      <th>txt</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>data/original/essay021.txt</td>\n",
       "      <td>data/transformed/essay021.json</td>\n",
       "      <td>essay021.txt</td>\n",
       "      <td>essay021.json</td>\n",
       "      <td>Advertisements affects on consumer goods\\n\\nEv...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>data/original/essay022.txt</td>\n",
       "      <td>data/transformed/essay022.json</td>\n",
       "      <td>essay022.txt</td>\n",
       "      <td>essay022.json</td>\n",
       "      <td>Young people should go to university or not\\n\\...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>data/original/essay049.txt</td>\n",
       "      <td>data/transformed/essay049.json</td>\n",
       "      <td>essay049.txt</td>\n",
       "      <td>essay049.json</td>\n",
       "      <td>Do modern communication technologies benefit a...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>data/original/essay051.txt</td>\n",
       "      <td>data/transformed/essay051.json</td>\n",
       "      <td>essay051.txt</td>\n",
       "      <td>essay051.json</td>\n",
       "      <td>Universities should give money to sport activi...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>data/original/essay055.txt</td>\n",
       "      <td>data/transformed/essay055.json</td>\n",
       "      <td>essay055.txt</td>\n",
       "      <td>essay055.json</td>\n",
       "      <td>Should teenagers learn all school subjects/foc...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      txt_path                       json_path      txt_file  \\\n",
       "20  data/original/essay021.txt  data/transformed/essay021.json  essay021.txt   \n",
       "21  data/original/essay022.txt  data/transformed/essay022.json  essay022.txt   \n",
       "48  data/original/essay049.txt  data/transformed/essay049.json  essay049.txt   \n",
       "50  data/original/essay051.txt  data/transformed/essay051.json  essay051.txt   \n",
       "54  data/original/essay055.txt  data/transformed/essay055.json  essay055.txt   \n",
       "\n",
       "        json_file                                                txt  \\\n",
       "20  essay021.json  Advertisements affects on consumer goods\\n\\nEv...   \n",
       "21  essay022.json  Young people should go to university or not\\n\\...   \n",
       "48  essay049.json  Do modern communication technologies benefit a...   \n",
       "50  essay051.json  Universities should give money to sport activi...   \n",
       "54  essay055.json  Should teenagers learn all school subjects/foc...   \n",
       "\n",
       "                                                 json  \n",
       "20  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "21  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "48  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "50  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "54  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ausgabe der ersten Zeilen des Trainings-Datensatzes\n",
    "train_df = train_df.sort_values(by='txt_file')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt_path</th>\n",
       "      <th>json_path</th>\n",
       "      <th>txt_file</th>\n",
       "      <th>json_file</th>\n",
       "      <th>txt</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/original/essay001.txt</td>\n",
       "      <td>data/transformed/essay001.json</td>\n",
       "      <td>essay001.txt</td>\n",
       "      <td>essay001.json</td>\n",
       "      <td>Should students be taught to compete or to coo...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/original/essay002.txt</td>\n",
       "      <td>data/transformed/essay002.json</td>\n",
       "      <td>essay002.txt</td>\n",
       "      <td>essay002.json</td>\n",
       "      <td>More people are migrating to other countries t...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/original/essay003.txt</td>\n",
       "      <td>data/transformed/essay003.json</td>\n",
       "      <td>essay003.txt</td>\n",
       "      <td>essay003.json</td>\n",
       "      <td>International tourism is now more common than ...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/original/essay004.txt</td>\n",
       "      <td>data/transformed/essay004.json</td>\n",
       "      <td>essay004.txt</td>\n",
       "      <td>essay004.json</td>\n",
       "      <td>International tourism is now more common than ...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/original/essay005.txt</td>\n",
       "      <td>data/transformed/essay005.json</td>\n",
       "      <td>essay005.txt</td>\n",
       "      <td>essay005.json</td>\n",
       "      <td>Living and studying overseas\\n\\nIt is every st...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     txt_path                       json_path      txt_file  \\\n",
       "0  data/original/essay001.txt  data/transformed/essay001.json  essay001.txt   \n",
       "1  data/original/essay002.txt  data/transformed/essay002.json  essay002.txt   \n",
       "2  data/original/essay003.txt  data/transformed/essay003.json  essay003.txt   \n",
       "3  data/original/essay004.txt  data/transformed/essay004.json  essay004.txt   \n",
       "4  data/original/essay005.txt  data/transformed/essay005.json  essay005.txt   \n",
       "\n",
       "       json_file                                                txt  \\\n",
       "0  essay001.json  Should students be taught to compete or to coo...   \n",
       "1  essay002.json  More people are migrating to other countries t...   \n",
       "2  essay003.json  International tourism is now more common than ...   \n",
       "3  essay004.json  International tourism is now more common than ...   \n",
       "4  essay005.json  Living and studying overseas\\n\\nIt is every st...   \n",
       "\n",
       "                                                json  \n",
       "0  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "1  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "2  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "3  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "4  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ausgabe der ersten Zeilen des Test-Datensatzes\n",
    "test_df = test_df.sort_values(by='txt_file')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behandlung von Duplikaten\n",
    "Es wurde fälschlicherweise angenommen, dass die Aufsätze keine Duplikate enthalten, da es sich um einen professionell erstellten Datensatz handelt. Es wurde, nachdem die Anfragen bereits an das LLM gesendet worden waren, jedoch festgestellt, dass ein Text dreimal und ein weiterer Text zweimal vorkommt. Die abweichende Annotation dieser Texte resultiert vermutlich aus der zuvor beschriebenen subjektiven Einschätzung der Annotatoren. Glücklicherweise befindet sich nur einer dieser Texte im Trainingsdatensatz, weshalb die Prompts und somit auch die Anfragen an das LLM nicht überarbeitet werden mussten. Die nachträgliche Behandlung der Duplikate spart Kosten und Zeit im Vergleich zu einer erneuten Übersendung der Anfragen an das LLM. \n",
    "\n",
    "Die Duplikate, welche sich im Testdatensatz befinden, werden bei der Evaluation des Modells entfernt (siehe 4. Notebook). Gemäß der Betrachtungen aus der EDA (1. Notebook) sind die Aufsätze mit der Nummer 171 und 210 aus dem Testdatensatz gleich zur Nummer 400 aus dem Trainingsdatensatz, weshalb sie beide aus dem Testdatensatz entfernt werden. Die Aufsätze mit der Nummer 209 und 377 sind gleich. Da beide Texte im Testdatensatz vorhanden sind, wird nur einer von beiden entfernt. Es sind folglich drei Aufsätze nachträglich zu entfernen. 3 von 402 Aufsätzen entsprechen ca. 0,75 % der Datenmenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplikate in Trainingsdaten: ['essay400.txt']\n",
      "Duplikate in Testdaten: ['essay171.txt', 'essay209.txt', 'essay210.txt', 'essay377.txt']\n"
     ]
    }
   ],
   "source": [
    "duplicate_files = ['essay171.txt', 'essay209.txt', 'essay210.txt', 'essay377.txt', 'essay400.txt'] # Liste aus der EDA\n",
    "# Überprüfung, welche der Duplikate in den Trainings- und Testdaten enthalten sind\n",
    "print(f\"Duplikate in Trainingsdaten: {train_df[train_df['txt_file'].isin(duplicate_files)]['txt_file'].tolist()}\")\n",
    "print(f\"Duplikate in Testdaten: {test_df[test_df['txt_file'].isin(duplicate_files)]['txt_file'].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt-Bausteine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chain-of-thought.txt',\n",
       " 'output-structure.txt',\n",
       " 'persona.txt',\n",
       " 'task-description.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUILDING_BLOCKS_PATH = 'prompts/building-blocks/'\n",
    "PROMPTS_PATH = 'prompts/final-prompts/'\n",
    "\n",
    "list_files(BUILDING_BLOCKS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-Shot Prompt\n",
    "task_description = load_text(BUILDING_BLOCKS_PATH + 'task-description.txt')\n",
    "persona = load_text(BUILDING_BLOCKS_PATH + 'persona.txt')\n",
    "cot = load_text(BUILDING_BLOCKS_PATH + 'chain-of-thought.txt')\n",
    "output_structure = load_text(BUILDING_BLOCKS_PATH + 'output-structure.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot (ZS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = task_description\n",
    "zs_persona = persona + task_description\n",
    "zs_cot = task_description + '\\n' + cot\n",
    "zs_persona_cot = persona + task_description + '\\n' + cot\n",
    "\n",
    "# Prompts als Textdateien speichern\n",
    "with open(PROMPTS_PATH + 'zero-shot.txt', 'w') as f:\n",
    "    f.write(zs)\n",
    "\n",
    "with open(PROMPTS_PATH + 'zero-shot-persona.txt', 'w') as f:\n",
    "    f.write(zs_persona)\n",
    "\n",
    "with open(PROMPTS_PATH + 'zero-shot-cot.txt', 'w') as f:\n",
    "    f.write(zs_cot)\n",
    "\n",
    "with open(PROMPTS_PATH + 'zero-shot-persona-cot.txt', 'w') as f:\n",
    "    f.write(zs_persona_cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Shot (OS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Beispiel aus dem Trainingsdatensatz auswählen für One-Shot-Prompt\n",
    "examples_1 = train_df.sample(1, random_state=42)\n",
    "\n",
    "# Zu dem Beispiel gehörige Text- und JSON-Daten extrahieren\n",
    "os_txt = examples_1['txt'].values[0]\n",
    "os_json = examples_1['json'].values[0]\n",
    "os_example = f\"## Input:\\n{os_txt}\\n## Output:\\n{os_json}\"\n",
    "\n",
    "os = task_description + 'Here is one example of a text and its corresponding json data:\\n' + os_example\n",
    "os_persona = persona + task_description + '\\n' + os_example\n",
    "os_cot = task_description + '\\n' + cot + '\\n' + os_example\n",
    "os_persona_cot = persona + task_description + '\\n' + cot + '\\n' + os_example\n",
    "\n",
    "# Prompts als Textdateien speichern\n",
    "with open(PROMPTS_PATH + 'one-shot.txt', 'w') as f:\n",
    "    f.write(os)\n",
    "\n",
    "with open(PROMPTS_PATH + 'one-shot-persona.txt', 'w') as f:\n",
    "    f.write(os_persona)\n",
    "\n",
    "with open(PROMPTS_PATH + 'one-shot-cot.txt', 'w') as f:\n",
    "    f.write(os_cot)\n",
    "\n",
    "with open(PROMPTS_PATH + 'one-shot-persona-cot.txt', 'w') as f:\n",
    "    f.write(os_persona_cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot (FS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 Beispiel aus dem Trainingsdatensatz auswählen für Few-Shot-Prompt\n",
    "examples_10 = train_df.sample(10, random_state=42)\n",
    "\n",
    "few_shot_examples_10 = f\"\\nHere are 10 examples of text and their corresponding json data:\\n\" # Ergänzung einer Einführung für die 10 Beispiele\n",
    "example_counter = 1\n",
    "# Zu den Beispielen gehörige Text- und JSON-Daten extrahieren\n",
    "for idx, row in examples_10.iterrows():\n",
    "    example_str = f\"\\n# Example {example_counter}\\n## Input:\\n{row['txt']}\\n## Output:\\n{row['json']}\"\n",
    "    few_shot_examples_10 += example_str\n",
    "    example_counter += 1\n",
    "\n",
    "fs = task_description + few_shot_examples_10\n",
    "fs_persona = persona + task_description + few_shot_examples_10\n",
    "fs_cot = task_description + '\\n' + cot + few_shot_examples_10\n",
    "fs_persona_cot = persona + task_description + '\\n' + cot + few_shot_examples_10\n",
    "\n",
    "# Prompts als Textdateien speichern\n",
    "with open(PROMPTS_PATH + 'few-shot-10.txt', 'w') as f:\n",
    "    f.write(fs)\n",
    "\n",
    "with open(PROMPTS_PATH + 'few-shot-10-persona.txt', 'w') as f:\n",
    "    f.write(fs_persona)\n",
    "\n",
    "with open(PROMPTS_PATH + 'few-shot-10-cot.txt', 'w') as f:\n",
    "    f.write(fs_cot)\n",
    "\n",
    "with open(PROMPTS_PATH + 'few-shot-10-persona-cot.txt', 'w') as f:\n",
    "    f.write(fs_persona_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 Beispiel aus dem Trainingsdatensatz auswählen für Few-Shot-Prompt\n",
    "examples_20 = train_df.sample(20, random_state=42)\n",
    "\n",
    "few_shot_examples_20 = f\"\\nHere are 20 examples of text and their corresponding json data:\\n\" # Ergänzung einer Einführung für die 20 Beispiele\n",
    "example_counter = 1\n",
    "# Zu den Beispielen gehörige Text- und JSON-Daten extrahieren\n",
    "for idx, row in examples_20.iterrows():\n",
    "    example_str = f\"\\n# Example {example_counter}\\n## Input:\\n{row['txt']}\\n## Output:\\n{row['json']}\"\n",
    "    few_shot_examples_20 += example_str\n",
    "    example_counter += 1\n",
    "\n",
    "fs = task_description + few_shot_examples_20\n",
    "fs_persona = persona + task_description + few_shot_examples_20\n",
    "fs_cot = task_description + '\\n' + cot + few_shot_examples_20\n",
    "fs_persona_cot = persona + task_description + '\\n' + cot + few_shot_examples_20\n",
    "\n",
    "# Prompts als Textdateien speichern\n",
    "with open(PROMPTS_PATH + 'few-shot-20.txt', 'w') as f:\n",
    "    f.write(fs)\n",
    "\n",
    "with open(PROMPTS_PATH + 'few-shot-20-persona.txt', 'w') as f:\n",
    "    f.write(fs_persona)\n",
    "\n",
    "with open(PROMPTS_PATH + 'few-shot-20-cot.txt', 'w') as f:\n",
    "    f.write(fs_cot)\n",
    "\n",
    "with open(PROMPTS_PATH + 'few-shot-20-persona-cot.txt', 'w') as f:\n",
    "    f.write(fs_persona_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40 Beispiel aus dem Trainingsdatensatz auswählen für Few-Shot-Prompt\n",
    "examples_40 = train_df.sample(40, random_state=42)\n",
    "\n",
    "few_shot_str_40 = f\"\\nHere are 40 examples of text and their corresponding json data:\\n\" # Ergänzung einer Einführung für die 20 Beispiele\n",
    "example_counter = 1\n",
    "# Zu den Beispielen gehörige Text- und JSON-Daten extrahieren\n",
    "for idx, row in examples_40.iterrows():\n",
    "    example_str = f\"\\n# Example {example_counter}\\n## Input:\\n{row['txt']}\\n## Output:\\n{row['json']}\"\n",
    "    few_shot_str_40 += example_str\n",
    "    example_counter += 1\n",
    "\n",
    "fs = task_description + few_shot_str_40\n",
    "fs_persona = persona + task_description + few_shot_str_40\n",
    "fs_cot = task_description + '\\n' + cot + few_shot_str_40\n",
    "fs_persona_cot = persona + task_description + '\\n' + cot + few_shot_str_40\n",
    "\n",
    "# Prompts als Textdateien speichern\n",
    "with open(PROMPTS_PATH + 'few-shot-40.txt', 'w') as f:\n",
    "    f.write(fs)\n",
    "\n",
    "with open(PROMPTS_PATH + 'few-shot-40-persona.txt', 'w') as f:\n",
    "    f.write(fs_persona)\n",
    "\n",
    "with open(PROMPTS_PATH + 'few-shot-40-cot.txt', 'w') as f:\n",
    "    f.write(fs_cot)\n",
    "\n",
    "with open(PROMPTS_PATH + 'few-shot-40-persona-cot.txt', 'w') as f:\n",
    "    f.write(fs_persona_cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promptvariationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es gibt 20 Prompts\n"
     ]
    }
   ],
   "source": [
    "# Eigentlich wird die os-Bibliothek bereits obengeladen. Da es aber vereinzelt zu Fehlermeldungen kam, wird sie hier nochmals geladen.\n",
    "import os\n",
    "\n",
    "prompt_files_directory_list = list_files_with_extension_directory(PROMPTS_PATH, '.txt')\n",
    "prompt_files_directory_list\n",
    "prompt_files_list = [os.path.basename(x) for x in prompt_files_directory_list]\n",
    "# Entfernen der txt-Endung\n",
    "prompt_names = [x.split('.')[0] for x in prompt_files_list]\n",
    "\n",
    "# Vorbereiten des DataFrames mit den Prompt-Variationen\n",
    "prompt_df = pd.DataFrame()\n",
    "prompt_df['prompt_name'] = prompt_names\n",
    "prompt_df['prompt_txt'] = prompt_files_directory_list\n",
    "prompt_df['prompt_txt'] = prompt_df['prompt_txt'].apply(load_text)\n",
    "print(F\"Es gibt {prompt_df.shape[0]} Prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berechnung der Tokenanzahl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Encoding 'o200k_base'>\n",
      "Beispieltext: This is a sample sentence.\n",
      "Encodierter Text (Integer): [2500, 382, 261, 10176, 21872, 13]\n",
      "Encodierter Text (Bytes): [b'This', b' is', b' a', b' sample', b' sentence', b'.']\n",
      "Anzahl Tokens: 6\n"
     ]
    }
   ],
   "source": [
    "model = 'gpt-4o-mini'\n",
    "\n",
    "# Beispiel zur Nachvollziehbarkeit der Tokenisierung\n",
    "encoding = tiktoken.encoding_for_model(model)\n",
    "print(encoding)\n",
    "\n",
    "sample_txt = \"This is a sample sentence.\"\n",
    "# Text encodieren\n",
    "token_integer = encoding.encode(sample_txt) # mit .decode() kann der Text wieder dekodiert werden. \n",
    "token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integer] # Integer Token können wiederum in Bytes umgewandelt werden, die sie repräsentieren.\n",
    "print(f\"Beispieltext: {sample_txt}\")\n",
    "print(f\"Encodierter Text (Integer): {token_integer}\")\n",
    "print(f\"Encodierter Text (Bytes): {token_bytes}\")\n",
    "\n",
    "\n",
    "count_tokens = num_tokens_from_string(sample_txt, model)\n",
    "print(f\"Anzahl Tokens: {count_tokens}\")\n",
    "\n",
    "# Codebausteine zur Berechnung der Tokenanzahl und Encoding entnommen aus: https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>prompt_txt</th>\n",
       "      <th>token_count</th>\n",
       "      <th>max_lines_jsonl</th>\n",
       "      <th>#batches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>zero-shot</td>\n",
       "      <td>You will be given a text. Extract the argument...</td>\n",
       "      <td>82</td>\n",
       "      <td>243902.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>zero-shot-persona</td>\n",
       "      <td>You are a expert in Argument Mining and theref...</td>\n",
       "      <td>105</td>\n",
       "      <td>190476.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>zero-shot-cot</td>\n",
       "      <td>You will be given a text. Extract the argument...</td>\n",
       "      <td>480</td>\n",
       "      <td>41667.0</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>zero-shot-persona-cot</td>\n",
       "      <td>You are a expert in Argument Mining and theref...</td>\n",
       "      <td>503</td>\n",
       "      <td>39761.0</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>one-shot</td>\n",
       "      <td>You will be given a text. Extract the argument...</td>\n",
       "      <td>1780</td>\n",
       "      <td>11236.0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>one-shot-persona</td>\n",
       "      <td>You are a expert in Argument Mining and theref...</td>\n",
       "      <td>1790</td>\n",
       "      <td>11173.0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>one-shot-cot</td>\n",
       "      <td>You will be given a text. Extract the argument...</td>\n",
       "      <td>2166</td>\n",
       "      <td>9234.0</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>one-shot-persona-cot</td>\n",
       "      <td>You are a expert in Argument Mining and theref...</td>\n",
       "      <td>2189</td>\n",
       "      <td>9137.0</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>few-shot-10</td>\n",
       "      <td>You will be given a text. Extract the argument...</td>\n",
       "      <td>13848</td>\n",
       "      <td>1444.0</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>few-shot-10-persona</td>\n",
       "      <td>You are a expert in Argument Mining and theref...</td>\n",
       "      <td>13871</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>few-shot-10-cot</td>\n",
       "      <td>You will be given a text. Extract the argument...</td>\n",
       "      <td>14247</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>few-shot-10-persona-cot</td>\n",
       "      <td>You are a expert in Argument Mining and theref...</td>\n",
       "      <td>14270</td>\n",
       "      <td>1402.0</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>few-shot-20</td>\n",
       "      <td>You will be given a text. Extract the argument...</td>\n",
       "      <td>27681</td>\n",
       "      <td>723.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>few-shot-20-persona</td>\n",
       "      <td>You are a expert in Argument Mining and theref...</td>\n",
       "      <td>27704</td>\n",
       "      <td>722.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>few-shot-20-cot</td>\n",
       "      <td>You will be given a text. Extract the argument...</td>\n",
       "      <td>28080</td>\n",
       "      <td>712.0</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>few-shot-20-persona-cot</td>\n",
       "      <td>You are a expert in Argument Mining and theref...</td>\n",
       "      <td>28103</td>\n",
       "      <td>712.0</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>few-shot-40</td>\n",
       "      <td>You will be given a text. Extract the argument...</td>\n",
       "      <td>54048</td>\n",
       "      <td>370.0</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>few-shot-40-persona</td>\n",
       "      <td>You are a expert in Argument Mining and theref...</td>\n",
       "      <td>54071</td>\n",
       "      <td>370.0</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>few-shot-40-cot</td>\n",
       "      <td>You will be given a text. Extract the argument...</td>\n",
       "      <td>54447</td>\n",
       "      <td>367.0</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>few-shot-40-persona-cot</td>\n",
       "      <td>You are a expert in Argument Mining and theref...</td>\n",
       "      <td>54470</td>\n",
       "      <td>367.0</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                prompt_name  \\\n",
       "19                zero-shot   \n",
       "18        zero-shot-persona   \n",
       "16            zero-shot-cot   \n",
       "17    zero-shot-persona-cot   \n",
       "15                 one-shot   \n",
       "14         one-shot-persona   \n",
       "12             one-shot-cot   \n",
       "13     one-shot-persona-cot   \n",
       "3               few-shot-10   \n",
       "2       few-shot-10-persona   \n",
       "0           few-shot-10-cot   \n",
       "1   few-shot-10-persona-cot   \n",
       "7               few-shot-20   \n",
       "6       few-shot-20-persona   \n",
       "4           few-shot-20-cot   \n",
       "5   few-shot-20-persona-cot   \n",
       "11              few-shot-40   \n",
       "10      few-shot-40-persona   \n",
       "8           few-shot-40-cot   \n",
       "9   few-shot-40-persona-cot   \n",
       "\n",
       "                                           prompt_txt  token_count  \\\n",
       "19  You will be given a text. Extract the argument...           82   \n",
       "18  You are a expert in Argument Mining and theref...          105   \n",
       "16  You will be given a text. Extract the argument...          480   \n",
       "17  You are a expert in Argument Mining and theref...          503   \n",
       "15  You will be given a text. Extract the argument...         1780   \n",
       "14  You are a expert in Argument Mining and theref...         1790   \n",
       "12  You will be given a text. Extract the argument...         2166   \n",
       "13  You are a expert in Argument Mining and theref...         2189   \n",
       "3   You will be given a text. Extract the argument...        13848   \n",
       "2   You are a expert in Argument Mining and theref...        13871   \n",
       "0   You will be given a text. Extract the argument...        14247   \n",
       "1   You are a expert in Argument Mining and theref...        14270   \n",
       "7   You will be given a text. Extract the argument...        27681   \n",
       "6   You are a expert in Argument Mining and theref...        27704   \n",
       "4   You will be given a text. Extract the argument...        28080   \n",
       "5   You are a expert in Argument Mining and theref...        28103   \n",
       "11  You will be given a text. Extract the argument...        54048   \n",
       "10  You are a expert in Argument Mining and theref...        54071   \n",
       "8   You will be given a text. Extract the argument...        54447   \n",
       "9   You are a expert in Argument Mining and theref...        54470   \n",
       "\n",
       "    max_lines_jsonl  #batches  \n",
       "19         243902.0      0.00  \n",
       "18         190476.0      0.00  \n",
       "16          41667.0      0.01  \n",
       "17          39761.0      0.01  \n",
       "15          11236.0      0.03  \n",
       "14          11173.0      0.03  \n",
       "12           9234.0      0.04  \n",
       "13           9137.0      0.04  \n",
       "3            1444.0      0.25  \n",
       "2            1442.0      0.25  \n",
       "0            1404.0      0.26  \n",
       "1            1402.0      0.26  \n",
       "7             723.0      0.50  \n",
       "6             722.0      0.50  \n",
       "4             712.0      0.51  \n",
       "5             712.0      0.51  \n",
       "11            370.0      0.98  \n",
       "10            370.0      0.98  \n",
       "8             367.0      0.99  \n",
       "9             367.0      0.99  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe mit Tokenanzahl und Anzahl der Batches für die Verarbeitung des Testdatensatzes\n",
    "prompt_df['token_count'] = prompt_df['prompt_txt'].apply(num_tokens_from_string, model_name=model)\n",
    "prompt_df = prompt_df.sort_values(by='token_count')\n",
    "# Schätzung für die Anzahl der Batches, die für die Verarbeitung des Testdatensatzes benötigt werden\n",
    "# Die Tokens für den User-Input sind hier nicht berücksichtigt. Der reale Wert wird daher höher sein.\n",
    "prompt_df['max_lines_jsonl'] = round(20_000_000 / prompt_df['token_count'], 0) # 20 Mio. entspricht der max. Anzahl an Tokens pro JSONL-Datei (enqueued tokens) für den verwendeten Client\n",
    "prompt_df['#batches'] = round(test_df.shape[0] / prompt_df['max_lines_jsonl'], 2) # Anzahl der Batches, die für die Verarbeitung benötigt werden\n",
    "prompt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schätzungen der Anfragen und Kosten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schätzungen der Anfragen bzw. Batches\n",
    "**Hinweis**\n",
    "\n",
    "Die Anfragen können nicht in einem gemeinsamen Batch auf einmal übermittelt werden, da es ansonsten zu einem Fehler kommt. Für die API von OpenAI bestehen Anfragebegrenzungen, die es bei der Erstellung der Batches zu berücksichtigen gilt. Sofern die Anfragebegrenzungen überschritten wurden, kann es beispielsweise zu folgender Fehlermeldung kommen:\n",
    "```\n",
    "Enqueued token limit reached for gpt-4o-mini in organization. Limit: 20,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.\n",
    "```\n",
    "Nach eigener Erfahrung tritt solch eine Fehlermeldung unter zwei Bedingungen auf. Zum einen, wenn die Anfragebegrenzung überschritten wurde, und zum anderen, wenn die Anfragen zu schnell hintereinander gestellt werden. Es wird deshalb empfohlen, die Anfragen nacheinander in Auftrag zu geben, sobald der vorherige Batch abgeschlossen ist. Sofern eine Batch-Datei hochgeladen wurde und der Batch an sich jedoch fehlschlägt, kann die Batch-Datei weiterhin anhand ihrer ID verwendet werden. Die Funktion check_batch_status kann beliebig oft ohne zusätzliche Kosten verwendet werden, um den Status eines Batches zu überprüfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es gibt insgesamt 7240 Kombinationen, die verarbeitet werden müssen.\n",
      "Davon entfallen 1448 auf Zero-Shot-Prompts.\n",
      "Davon entfallen 1448 auf One-Shot-Prompts.\n",
      "Davon entfallen 1448 auf Few-Shot-Prompts mit 10 Beispielen.\n",
      "Davon entfallen 1448 auf Few-Shot-Prompts mit 20 Beispielen.\n",
      "Davon entfallen 1448 auf Few-Shot-Prompts mit 40 Beispielen.\n"
     ]
    }
   ],
   "source": [
    "# Unterteilung der Prompts anhand der Anzahl der enthaltenen Beispiele\n",
    "zs_prompt_df = prompt_df[prompt_df['prompt_name'].str.contains('zero-shot')]\n",
    "os_prompt_df = prompt_df[prompt_df['prompt_name'].str.contains('one-shot')]\n",
    "fs10_prompt_df = prompt_df[prompt_df['prompt_name'].str.contains('few-shot-10')]\n",
    "fs20_prompt_df = prompt_df[prompt_df['prompt_name'].str.contains('few-shot-20')]\n",
    "fs40_prompt_df = prompt_df[prompt_df['prompt_name'].str.contains('few-shot-40')]\n",
    "\n",
    "# Anzahl der Zeilen für die verschiedenen Prompts\n",
    "zs_rows = zs_prompt_df.shape[0]\n",
    "os_rows = os_prompt_df.shape[0]\n",
    "fs10_rows = fs10_prompt_df.shape[0]\n",
    "fs20_rows = fs20_prompt_df.shape[0]\n",
    "fs40_rows = fs40_prompt_df.shape[0]\n",
    "\n",
    "# Gesamtanzahl der Kombinationen für die Verarbeitung des Testdatensatzes bestimmen\n",
    "combinations = test_df.shape[0] * (zs_rows + os_rows + fs10_rows + fs20_rows + fs40_rows)\n",
    "print(f\"Es gibt insgesamt {combinations} Kombinationen, die verarbeitet werden müssen.\")\n",
    "print(f\"Davon entfallen {test_df.shape[0] * zs_rows} auf Zero-Shot-Prompts.\")\n",
    "print(f\"Davon entfallen {test_df.shape[0] * os_rows} auf One-Shot-Prompts.\")\n",
    "print(f\"Davon entfallen {test_df.shape[0] * fs10_rows} auf Few-Shot-Prompts mit 10 Beispielen.\")\n",
    "print(f\"Davon entfallen {test_df.shape[0] * fs20_rows} auf Few-Shot-Prompts mit 20 Beispielen.\")\n",
    "print(f\"Davon entfallen {test_df.shape[0] * fs40_rows} auf Few-Shot-Prompts mit 40 Beispielen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schätzung der Anzahl der Batches für die Verarbeitung des Testdatensatzes:\n",
      "Zero-Shot-Prompts: 0.02 Batches mit 1170 Tokens\n",
      "One-Shot-Prompts: 0.14 Batches mit 7925 Tokens\n",
      "Few-Shot-Prompts mit 10 Beispielen: 1.02 Batches mit 56236 Tokens\n",
      "Few-Shot-Prompts mit 20 Beispielen: 2.02 Batches mit 111568 Tokens\n",
      "Few-Shot-Prompts mit 40 Beispielen: 3.94 Batches mit 217036 Tokens\n",
      "Es gibt insgesamt 7.14 Batches\n"
     ]
    }
   ],
   "source": [
    "# Berechnung des Tokenverbrauchs pro Batch. Das limit des verwendeten Clients liegt bei 2,000,000 enqueued tokens\n",
    "zs_prompt_df_sum = zs_prompt_df['token_count'].sum()\n",
    "os_prompt_df_sum = os_prompt_df['token_count'].sum()\n",
    "fs10_df_sum = fs10_prompt_df['token_count'].sum()\n",
    "fs20_df_sum = fs20_prompt_df['token_count'].sum()\n",
    "fs40_df_sum = fs40_prompt_df['token_count'].sum()\n",
    "#requests = 1448 # Anzahl der Anfragen, die in einem Batch verarbeitet werden\n",
    "\n",
    "# Anzahl der Batch-Anfragen\n",
    "zs_batches = round(zs_prompt_df['#batches'].sum(), 2)\n",
    "os_batches = round(os_prompt_df['#batches'].sum(), 2)\n",
    "fs10_batches = round(fs10_prompt_df['#batches'].sum(), 2)\n",
    "fs20_batches = round(fs20_prompt_df['#batches'].sum(), 2)\n",
    "fs40_batches = round(fs40_prompt_df['#batches'].sum(), 2)\n",
    "\n",
    "print(\"Schätzung der Anzahl der Batches für die Verarbeitung des Testdatensatzes:\")\n",
    "print(f\"Zero-Shot-Prompts: {zs_batches} Batches mit {zs_prompt_df_sum} Tokens\")\n",
    "print(f\"One-Shot-Prompts: {os_batches} Batches mit {os_prompt_df_sum} Tokens\")\n",
    "print(f\"Few-Shot-Prompts mit 10 Beispielen: {fs10_batches} Batches mit {fs10_df_sum} Tokens\")\n",
    "print(f\"Few-Shot-Prompts mit 20 Beispielen: {fs20_batches} Batches mit {fs20_df_sum} Tokens\")\n",
    "print(f\"Few-Shot-Prompts mit 40 Beispielen: {fs40_batches} Batches mit {fs40_df_sum} Tokens\")\n",
    "print(f\"Es gibt insgesamt {(zs_batches + os_batches + fs10_batches + fs20_batches + fs40_batches):.2f} Batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird deutlich, dass die Unterteilung der Batches anhand der Anzahl an Beispielen im Prompt aufgrund der unterschiedlichen Komplexität und damit Tokenanzahl nicht sinnvoll ist. So könnten die ZS- und OS-Prompts in einem Batch zusammengefasst werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schätzung der anfallenden Kosten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Schätzung der Kosten für die Verarbeitung des Testdatensatzes\n",
      "------------------------------------------------------------\n",
      "Input-Token:\n",
      "Die Summe der Input-Tokenanzahl aller Prompts beträgt: 393,935 Tokens\n",
      "Multipliziert mit der Anzahl der Testdurchläufe ergibt das: 142,604,470 Tokens für Input-Token\n",
      "Das entspicht bei einer maximalen Anzahl von 20,000,000 Tokens pro JSONL-Datei: 7.13 Batches\n",
      "Die Kosten für die Input-Tokens betragen: 21.39 $\n",
      "\n",
      "Output-Token:\n",
      "Die Output-Tokenanzahl multipliziert mit der Anzahl der Aufsätze im Testdatensaatz beträgt zwischen: 162,900 und 586,440 Tokens\n",
      "Die Kosten für die Output-Tokens betragen zwischen: 0.10 $ und 0.35 $\n",
      "\n",
      "Gesamt:\n",
      "Die Gesamtkosten liegen schätzungsweise in einem Bereich von 21.49 $ und 21.74 $\n",
      "Bei der Anwendung der Batch-API gibt es einen Rabatt von 50% auf die Tokenpreise. Damit würden die Kosten zwischen 10.74 $ und 10.87 $ liegen.\n"
     ]
    }
   ],
   "source": [
    "# Schätzung der anfallenden Kosten anhand der Tokenanzahl\n",
    "print(f\"{\"-\" * 60}\") # Trennlinie\n",
    "print(\"Schätzung der Kosten für die Verarbeitung des Testdatensatzes\")\n",
    "print(f\"{\"-\" * 60}\")\n",
    "prompt_token_sum = prompt_df['token_count'].sum()\n",
    "print(f\"Input-Token:\\nDie Summe der Input-Tokenanzahl aller Prompts beträgt: {prompt_token_sum:,} Tokens\")\n",
    "test_token_sum = prompt_token_sum * test_df.shape[0]\n",
    "print(f\"Multipliziert mit der Anzahl der Testdurchläufe ergibt das: {test_token_sum:,} Tokens für Input-Token\")\n",
    "max_enqueued_tokens = 20_000_000 # Maximale Anzahl an Tokens, die von der Batch API\n",
    "print(f\"Das entspicht bei einer maximalen Anzahl von {max_enqueued_tokens:,} Tokens pro JSONL-Datei: {test_token_sum/20_000_000:.2f} Batches\") \n",
    "input_token_price = 0.15 # input token price per 1 Mio tokens\n",
    "output_token_price = 0.6 # output token price per 1 Mio tokens\n",
    "input_token_cost = input_token_price * test_token_sum/1_000_000\n",
    "print(f\"Die Kosten für die Input-Tokens betragen: {input_token_cost:.2f} $\") \n",
    "min_output_token_count = 450 # Minimalwert für Tokenanzahl für ann-Dateien aus EDA.\n",
    "max_output_token_count = 1_620 # aufgerundeter Maximalwert für Tokenanzahl für ann-Dateien aus EDA. Umfang der Ausgabe des LLMs kann auch außerhalb des Bereichs liegen. \n",
    "min_output_token_sum = min_output_token_count * test_df.shape[0]\n",
    "max_output_token_sum = max_output_token_count * test_df.shape[0]\n",
    "print(f\"\\nOutput-Token:\\nDie Output-Tokenanzahl multipliziert mit der Anzahl der Aufsätze im Testdatensaatz beträgt zwischen: {min_output_token_sum:,} und {max_output_token_sum:,} Tokens\")\n",
    "min_output_token_cost = output_token_price * min_output_token_sum/1_000_000\n",
    "max_output_token_cost = output_token_price * max_output_token_sum/1_000_000\n",
    "print(f\"Die Kosten für die Output-Tokens betragen zwischen: {min_output_token_cost:.2f} $ und {max_output_token_cost:.2f} $\")\n",
    "total_cost_min = input_token_cost + min_output_token_cost\n",
    "total_cost_max = input_token_cost + max_output_token_cost\n",
    "print(f\"\\nGesamt:\\nDie Gesamtkosten liegen schätzungsweise in einem Bereich von {total_cost_min:.2f} $ und {total_cost_max:.2f} $\")\n",
    "print(f\"Bei der Anwendung der Batch-API gibt es einen Rabatt von 50% auf die Tokenpreise. Damit würden die Kosten zwischen {total_cost_min/2:.2f} $ und {total_cost_max/2:.2f} $ liegen.\")\n",
    "\n",
    "# Quelle für Tokenpreise: https://openai.com/api/pricing/ (Stand 01/2025) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Abfrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API-Key aus .env-Datei laden  \n",
    "load_dotenv()\n",
    "openai_api = os.getenv(\"OPENAI_API_KEY\")\n",
    "                       \n",
    "client = OpenAI(api_key=openai_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## strukturiertes Ausgabeformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_format = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"MajorClaims\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"ID\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"Text\": {\n",
    "                        \"type\": \"string\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"ID\", \"Text\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        },\n",
    "        \"Claims\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"ID\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"Text\": {\n",
    "                        \"type\": \"string\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"ID\", \"Text\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        },\n",
    "        \"Premises\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"ID\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"Text\": {\n",
    "                        \"type\": \"string\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"ID\", \"Text\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        },\n",
    "        \"ArgumentativeRelations\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"Origin\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"Relation\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"for\", \"against\", \"supports\", \"attacks\"]\n",
    "                    },\n",
    "                    \"Target\": {\n",
    "                        \"type\": \"string\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"Origin\", \"Relation\", \"Target\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"MajorClaims\", \"Claims\", \"Premises\", \"ArgumentativeRelations\"],\n",
    "    \"additionalProperties\": False\n",
    "    }\n",
    "\n",
    "# Der Ansatz unter Verwendung von Pydantic hat nicht funktioniert und wurde daher verworfen.\n",
    "\n",
    "# herangezogene Quellen zur Erstellung des strukturierten Ausgabeformats:\n",
    "# - https://platform.openai.com/docs/guides/structured-outputs\n",
    "# - https://cookbook.openai.com/examples/structured_outputs_intro\n",
    "# - https://python.langchain.com/docs/concepts/structured_outputs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch API Input Dateien erstellen (JSONL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_INPUT_PATH = \"batch_api/input/\"\n",
    "BATCH_OUTPUT_PATH = \"batch_api/output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Datei wurde in mehrere Dateien aufgeteilt, welche unter folgendem Pfad gespeichert sind: batch_api/input/\n"
     ]
    }
   ],
   "source": [
    "# Batch API Input-Datei erstellen (JSONL-Format)\n",
    "full_jsonl_path = generate_batch_input(test_df, prompt_df, \"full_batch_input\", response_format, BATCH_INPUT_PATH)\n",
    "\n",
    "# Aufteilung der zuvor erstellten JSONL-Datei in mehrere Dateien zur schrittweisen Verarbeitung durch die Batch-API\n",
    "# Als Kriterium zum Aufteilen wird die maximale Anzahl an Tokens pro JSONL-Datei (20 Mio.) verwendet.\n",
    "split_jsonl_file(full_jsonl_path)\n",
    "print(f\"Die Datei wurde in mehrere Dateien aufgeteilt, welche unter folgendem Pfad gespeichert sind: {BATCH_INPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['batch_input_1.jsonl',\n",
       " 'batch_input_2.jsonl',\n",
       " 'batch_input_3.jsonl',\n",
       " 'batch_input_4.jsonl',\n",
       " 'batch_input_5.jsonl',\n",
       " 'batch_input_6.jsonl',\n",
       " 'batch_input_7.jsonl',\n",
       " 'batch_input_8.jsonl',\n",
       " 'batch_input_9.jsonl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input_files_list = list_files(BATCH_INPUT_PATH)\n",
    "batch_input_files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste der Anfragen pro JSONL-Datei: [3816, 878, 622, 549, 323, 322, 320, 320, 90]\n",
      "Summe der Anfragen: 7240\n"
     ]
    }
   ],
   "source": [
    "# Anzahl der Zeilen in den JSONL-Dateien. Eine Zeile entspricht einer Anfrage an die Batch-API\n",
    "line_counts = [sum(1 for line in open(BATCH_INPUT_PATH + file)) for file in batch_input_files_list]\n",
    "print(f\"Liste der Anfragen pro JSONL-Datei: {line_counts}\")\n",
    "print(f\"Summe der Anfragen: {sum(line_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je komplexer die Prompts, desto mehr Token werden benötigt. Folglich besteht ein Batch mit komplexen Prompts aus weniger Anfragen als ein Batch mit einfachen Prompts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches hochladen, erstellen und abfragen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beschreibung der Status-Codes:\n",
    "\n",
    "| Status       | Description                                                                 |\n",
    "|--------------|-----------------------------------------------------------------------------|\n",
    "| validating   | the input file is being validated before the batch can begin                |\n",
    "| failed       | the input file has failed the validation process                            |\n",
    "| in_progress  | the input file was successfully validated and the batch is currently being run |\n",
    "| finalizing   | the batch has completed and the results are being prepared                  |\n",
    "| completed    | the batch has been completed and the results are ready                      |\n",
    "| expired      | the batch was not able to be completed within the 24-hour time window       |\n",
    "| cancelling   | the batch is being cancelled (may take up to 10 minutes)                    |\n",
    "| cancelled    | the batch was cancelled                                                     |\n",
    "\n",
    "Tabelle entnommen aus: https://platform.openai.com/docs/guides/batch/batch-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-73X2eNMUnxhwLndGXvNdZ5', bytes=84798929, created_at=1736407855, filename='batch_input_1.jsonl', object='file', purpose='batch', status='processed', status_details=None)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload der Batch Input-Datei auf die OpenAI-Plattform\n",
    "batch_file_1 = upload_batch_file(BATCH_INPUT_PATH + \"batch_input_1.jsonl\", client)\n",
    "batch_file_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_677f7b3b1b48819086a5a27b7173c2c3', completion_window='24h', created_at=1736407867, endpoint='/v1/chat/completions', input_file_id='file-73X2eNMUnxhwLndGXvNdZ5', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1736494267, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Batch 1/9 for Argument Mining'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Batches\n",
    "metadata_dict = {\"description\": \"Batch 1/9 for Argument Mining\"}\n",
    "batch_1 = create_batch(batch_file_1.id, metadata_dict, client) # sofern die Batch Datei bereits hochgeladen wurde, aber nicht erfolgreich war, kann die Batch-ID erneut verwendet werden. Ein erneuter Uplaod ist nicht notwendig und würde der Datei eine neue ID zuweisen.\n",
    "print(batch_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: completed\n",
      "\n",
      "Beschreibung des Batches: Batch 1/9 for Argument Mining\n",
      "Anfragen gesamt: 3816\n",
      "Davon erfolgreich: 3816\n",
      "Davon fehlerhaft: 0\n",
      "Erfolgreiche Abfragen können abgerufen werden mit ID: file-Bwq5PPm4rnvL2jk7KFt2cL\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_677f7b3b1b48819086a5a27b7173c2c3', completion_window='24h', created_at=1736407867, endpoint='/v1/chat/completions', input_file_id='file-73X2eNMUnxhwLndGXvNdZ5', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736411765, error_file_id=None, errors=None, expired_at=None, expires_at=1736494267, failed_at=None, finalizing_at=1736411356, in_progress_at=1736407871, metadata={'description': 'Batch 1/9 for Argument Mining'}, output_file_id='file-Bwq5PPm4rnvL2jk7KFt2cL', request_counts=BatchRequestCounts(completed=3816, failed=0, total=3816))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "# Die Bearbeitung des Batches kann bis zu 24 Stunden dauern, funktioniert aber in der Regel schneller.\n",
    "batch_1_status = check_batch_status(batch_1.id, client)\n",
    "print(batch_1_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"batch_req_677f88dcb8c881908e9275650fab34fa\", \"custom_id\": \"zero-shot_essay001.txt\", \"response\": {\"status_code\": 200, \"request_id\": \"82330270e662bb47722cf42d0c0ab869\", \"body\": {\"id\": \"chatcmpl-AnhIbjUujngIZN41Ogf87fCAje0x7\", \"object\": \"chat.completion\", \"created\": 1736407877, \"model\": \"gpt-4o-mini-2024-07-18\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"{\\\"MajorClaims\\\":[{\\\"ID\\\":\\\"MC1\\\",\\\"Text\\\":\\\"We should attach more importance to cooperation during primary education.\\\"}],\\\"Claims\\\":[{\\\"ID\\\":\\\"C1\\\",\\\"Text\\\":\\\"Competition can effectively promote the development of economy.\\\"},{\\\"ID\\\":\\\"C2\\\",\\\"Text\\\":\\\"Cooperation helps children learn interpersonal skills.\\\"},{\\\"ID\\\":\\\"C3\\\",\\\"Text\\\":\\\"Competition makes society more effective.\\\"},{\\\"ID\\\":\\\"C4\\\",\\\"Text\\\":\\\"Victory in competition often requires cooperation.\\\"}],\\\"Premises\\\":[{\\\"ID\\\":\\\"P1\\\",\\\"Text\\\":\\\"Companies improve their products and services to survive in competition.\\\"},{\\\"ID\\\":\\\"P2\\\",\\\"Text\\\":\\\"Inter\n"
     ]
    }
   ],
   "source": [
    "# Abrufen und Speichern der Batch-Ergebnisse (Output-Datei)\n",
    "batch_1_results = retrieve_and_save_batch_results(batch_1_status.output_file_id, BATCH_OUTPUT_PATH, \"output-batch-1\", client)\n",
    "print(batch_1_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-NyvUx7xYfyFu6C4QNZ6qYs', bytes=80171245, created_at=1736498896, filename='batch_input_2.jsonl', object='file', purpose='batch', status='processed', status_details=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload der Batch Input-Datei auf die OpenAI-Plattform\n",
    "batch_file_2 = upload_batch_file(BATCH_INPUT_PATH + \"batch_input_2.jsonl\", client)\n",
    "batch_file_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_6780ded3cd58819080b492f6647492b4', completion_window='24h', created_at=1736498899, endpoint='/v1/chat/completions', input_file_id='file-NyvUx7xYfyFu6C4QNZ6qYs', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1736585299, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Batch 2/9 for Argument Mining'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Batches\n",
    "metadata_dict = {\"description\": \"Batch 2/9 for Argument Mining\"}\n",
    "batch_2 = create_batch(batch_file_2.id, metadata_dict, client) # sofern die Batch Datei bereits hochgeladen wurde, aber nicht erfolgreich war, kann die Batch-ID erneut verwendet werden. Ein erneuter Uplaod ist nicht notwendig und würde der Datei eine neue ID zuweisen.\n",
    "print(batch_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: completed\n",
      "\n",
      "Beschreibung des Batches: Batch 2/9 for Argument Mining\n",
      "Anfragen gesamt: 878\n",
      "Davon erfolgreich: 878\n",
      "Davon fehlerhaft: 0\n",
      "Erfolgreiche Abfragen können abgerufen werden mit ID: file-Fg7gsDHnWPhPKX5BLzQhaH\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_6780ded3cd58819080b492f6647492b4', completion_window='24h', created_at=1736498899, endpoint='/v1/chat/completions', input_file_id='file-NyvUx7xYfyFu6C4QNZ6qYs', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736499712, error_file_id=None, errors=None, expired_at=None, expires_at=1736585299, failed_at=None, finalizing_at=1736499609, in_progress_at=1736498902, metadata={'description': 'Batch 2/9 for Argument Mining'}, output_file_id='file-Fg7gsDHnWPhPKX5BLzQhaH', request_counts=BatchRequestCounts(completed=878, failed=0, total=878))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "batch_2_status = check_batch_status(batch_2.id, client)\n",
    "print(batch_2_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"batch_req_6780e1bbdd408190b622fa829f21b366\", \"custom_id\": \"few-shot-10-cot_essay220.txt\", \"response\": {\"status_code\": 200, \"request_id\": \"7efa009d49bf99540b29b3fb45a5b4d6\", \"body\": {\"id\": \"chatcmpl-Ao4yrOhe57gU4dz9cROmIgRKN2NNL\", \"object\": \"chat.completion\", \"created\": 1736498909, \"model\": \"gpt-4o-mini-2024-07-18\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"{\\n  \\\"MajorClaims\\\": [\\n    {\\n      \\\"ID\\\": \\\"MC1\\\",\\n      \\\"Text\\\": \\\"Learning to be independent is essential for young adults\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"MC2\\\",\\n      \\\"Text\\\": \\\"staying longer with parents is a better choice\\\"\\n    }\\n  ],\\n  \\\"Claims\\\": [\\n    {\\n      \\\"ID\\\": \\\"C1\\\",\\n      \\\"Text\\\": \\\"staying with the parents for longer time does more benefits than disadvantages to the young adult\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C2\\\",\\n      \\\"Text\\\": \\\"the young adult can have more experience with his parents\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C3\\\",\\n      \\\"Text\\\": \\\"living at ho\n"
     ]
    }
   ],
   "source": [
    "# Abrufen und Speichern der Batch-Ergebnisse (Output-Datei)\n",
    "batch_2_results = retrieve_and_save_batch_results(batch_2_status.output_file_id, BATCH_OUTPUT_PATH, \"output-batch-2\", client)\n",
    "print(batch_2_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-1VeM9QWs9utTGbREGjvauV', bytes=79782274, created_at=1736585156, filename='batch_input_3.jsonl', object='file', purpose='batch', status='processed', status_details=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload der Batch Input-Datei auf die OpenAI-Plattform\n",
    "batch_file_3 = upload_batch_file(BATCH_INPUT_PATH + \"batch_input_3.jsonl\", client)\n",
    "batch_file_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_67822fc8058c8190b864f42b92ad646d', completion_window='24h', created_at=1736585160, endpoint='/v1/chat/completions', input_file_id='file-1VeM9QWs9utTGbREGjvauV', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1736671560, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Batch 3/9 for Argument Mining'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Batches\n",
    "metadata_dict = {\"description\": \"Batch 3/9 for Argument Mining\"}\n",
    "batch_3 = create_batch(batch_file_3.id, metadata_dict, client) # sofern die Batch Datei bereits hochgeladen wurde, aber nicht erfolgreich war, kann die Batch-ID erneut verwendet werden. Ein erneuter Uplaod ist nicht notwendig und würde der Datei eine neue ID zuweisen.\n",
    "print(batch_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: completed\n",
      "\n",
      "Beschreibung des Batches: Batch 3/9 for Argument Mining\n",
      "Anfragen gesamt: 622\n",
      "Davon erfolgreich: 622\n",
      "Davon fehlerhaft: 0\n",
      "Erfolgreiche Abfragen können abgerufen werden mit ID: file-Dn8EPeVM9BjT1VpRaaAnvS\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_67822fc8058c8190b864f42b92ad646d', completion_window='24h', created_at=1736585160, endpoint='/v1/chat/completions', input_file_id='file-1VeM9QWs9utTGbREGjvauV', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736586694, error_file_id=None, errors=None, expired_at=None, expires_at=1736671560, failed_at=None, finalizing_at=1736586509, in_progress_at=1736585162, metadata={'description': 'Batch 3/9 for Argument Mining'}, output_file_id='file-Dn8EPeVM9BjT1VpRaaAnvS', request_counts=BatchRequestCounts(completed=622, failed=0, total=622))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "batch_3_status = check_batch_status(batch_3.id, client)\n",
    "print(batch_3_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"batch_req_6782350f50148190ae1ecbe23a044ce3\", \"custom_id\": \"few-shot-20_essay389.txt\", \"response\": {\"status_code\": 200, \"request_id\": \"429b27845229d782546ed0cbc4fb17c3\", \"body\": {\"id\": \"chatcmpl-AoRVz57qqhjznstY4shTiOqFnQRQC\", \"object\": \"chat.completion\", \"created\": 1736585531, \"model\": \"gpt-4o-mini-2024-07-18\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"{\\n  \\\"MajorClaims\\\": [\\n    {\\n      \\\"ID\\\": \\\"MC1\\\",\\n      \\\"Text\\\": \\\"education plays an important role in the socioeconomic status of a country\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"MC2\\\",\\n      \\\"Text\\\": \\\"education is the single most important factor in the development of a country\\\"\\n    }\\n  ],\\n  \\\"Claims\\\": [\\n    {\\n      \\\"ID\\\": \\\"C1\\\",\\n      \\\"Text\\\": \\\"education is undeniably an economic necessity\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C2\\\",\\n      \\\"Text\\\": \\\"not many can afford to send their children to school in a developing country\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C3\\\",\\n      \\\"Text\\\": \\\"\n"
     ]
    }
   ],
   "source": [
    "# Abrufen und Speichern der Batch-Ergebnisse (Output-Datei)\n",
    "batch_3_results = retrieve_and_save_batch_results(batch_3_status.output_file_id, BATCH_OUTPUT_PATH, \"output-batch-3\", client)\n",
    "print(batch_3_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-CUxcAUKiZJZ64KkV8YAsG4', bytes=79467621, created_at=1736670809, filename='batch_input_4.jsonl', object='file', purpose='batch', status='processed', status_details=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload der Batch Input-Datei auf die OpenAI-Plattform\n",
    "batch_file_4 = upload_batch_file(BATCH_INPUT_PATH + \"batch_input_4.jsonl\", client)\n",
    "batch_file_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_67837fec4f04819088f396053ef790f2', completion_window='24h', created_at=1736671212, endpoint='/v1/chat/completions', input_file_id='file-CUxcAUKiZJZ64KkV8YAsG4', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1736757612, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Batch 4/9 for Argument Mining'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Batches\n",
    "metadata_dict = {\"description\": \"Batch 4/9 for Argument Mining\"}\n",
    "batch_4 = create_batch(batch_file_4.id, metadata_dict, client) # sofern die Batch Datei bereits hochgeladen wurde, aber nicht erfolgreich war, kann die Batch-ID erneut verwendet werden. Ein erneuter Uplaod ist nicht notwendig und würde der Datei eine neue ID zuweisen.\n",
    "print(batch_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: completed\n",
      "\n",
      "Beschreibung des Batches: Batch 4/9 for Argument Mining\n",
      "Anfragen gesamt: 549\n",
      "Davon erfolgreich: 549\n",
      "Davon fehlerhaft: 0\n",
      "Erfolgreiche Abfragen können abgerufen werden mit ID: file-CTPtvsw1e79hupNn1R1DXf\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_67837fec4f04819088f396053ef790f2', completion_window='24h', created_at=1736671212, endpoint='/v1/chat/completions', input_file_id='file-CUxcAUKiZJZ64KkV8YAsG4', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736673370, error_file_id=None, errors=None, expired_at=None, expires_at=1736757612, failed_at=None, finalizing_at=1736673312, in_progress_at=1736671215, metadata={'description': 'Batch 4/9 for Argument Mining'}, output_file_id='file-CTPtvsw1e79hupNn1R1DXf', request_counts=BatchRequestCounts(completed=549, failed=0, total=549))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "batch_4_status = check_batch_status(batch_4.id, client)\n",
    "print(batch_4_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"batch_req_67838820cb908190bb1db05670129112\", \"custom_id\": \"few-shot-20-cot_essay276.txt\", \"response\": {\"status_code\": 200, \"request_id\": \"856774c31c28b8df1dfb7b6a6a981df5\", \"body\": {\"id\": \"chatcmpl-AonoLHzczRc9vHXpuohv76KsztCIX\", \"object\": \"chat.completion\", \"created\": 1736671237, \"model\": \"gpt-4o-mini-2024-07-18\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"{\\n  \\\"MajorClaims\\\": [\\n    {\\n      \\\"ID\\\": \\\"MC1\\\",\\n      \\\"Text\\\": \\\"dancing is an important part of culture\\\"\\n    }\\n  ],\\n  \\\"Claims\\\": [\\n    {\\n      \\\"ID\\\": \\\"C1\\\",\\n      \\\"Text\\\": \\\"dancing are significant part of culture that could show to something that people believe\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C2\\\",\\n      \\\"Text\\\": \\\"dancing can represent to civilization of that culture\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C3\\\",\\n      \\\"Text\\\": \\\"dancing is one of the ways people entertain themselves\\\"\\n    }\\n  ],\\n  \\\"Premises\\\": [\\n    {\\n      \\\"ID\\\": \\\"P1\\\",\\n      \\\"Text\\\": \\\"some cultur\n"
     ]
    }
   ],
   "source": [
    "# Abrufen und Speichern der Batch-Ergebnisse (Output-Datei)\n",
    "batch_4_results = retrieve_and_save_batch_results(batch_4_status.output_file_id, BATCH_OUTPUT_PATH, \"output-batch-4\", client)\n",
    "print(batch_4_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-BAg7ZagvUiaPgNUeoPjxVc', bytes=79286180, created_at=1736757463, filename='batch_input_5.jsonl', object='file', purpose='batch', status='processed', status_details=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload der Batch Input-Datei auf die OpenAI-Plattform\n",
    "batch_file_5 = upload_batch_file(BATCH_INPUT_PATH + \"batch_input_5.jsonl\", client)\n",
    "batch_file_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_6784d168bb288190bda2b11e3b42da3c', completion_window='24h', created_at=1736757608, endpoint='/v1/chat/completions', input_file_id='file-BAg7ZagvUiaPgNUeoPjxVc', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1736844008, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Batch 5/9 for Argument Mining'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Batches\n",
    "metadata_dict = {\"description\": \"Batch 5/9 for Argument Mining\"}\n",
    "batch_5 = create_batch(batch_file_5.id, metadata_dict, client) # sofern die Batch Datei bereits hochgeladen wurde, aber nicht erfolgreich war, kann die Batch-ID erneut verwendet werden. Ein erneuter Uplaod ist nicht notwendig und würde der Datei eine neue ID zuweisen.\n",
    "print(batch_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: completed\n",
      "\n",
      "Beschreibung des Batches: Batch 5/9 for Argument Mining\n",
      "Anfragen gesamt: 323\n",
      "Davon erfolgreich: 323\n",
      "Davon fehlerhaft: 0\n",
      "Erfolgreiche Abfragen können abgerufen werden mit ID: file-XCidZa5LAvqwrL52vCSaS2\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_6784d168bb288190bda2b11e3b42da3c', completion_window='24h', created_at=1736757608, endpoint='/v1/chat/completions', input_file_id='file-BAg7ZagvUiaPgNUeoPjxVc', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736791620, error_file_id=None, errors=None, expired_at=None, expires_at=1736844008, failed_at=None, finalizing_at=1736791592, in_progress_at=1736757613, metadata={'description': 'Batch 5/9 for Argument Mining'}, output_file_id='file-XCidZa5LAvqwrL52vCSaS2', request_counts=BatchRequestCounts(completed=323, failed=0, total=323))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "batch_5_status = check_batch_status(batch_5.id, client)\n",
    "print(batch_5_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"batch_req_6785562970d48190890e1b3db0550e4e\", \"custom_id\": \"few-shot-40_essay081.txt\", \"response\": {\"status_code\": 200, \"request_id\": \"961d841d6ad27aec5dccc2ac0ef053e5\", \"body\": {\"id\": \"chatcmpl-ApJ1kDVOSD1DoKECkc3fctxXCi9xt\", \"object\": \"chat.completion\", \"created\": 1736791232, \"model\": \"gpt-4o-mini-2024-07-18\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"{\\n  \\\"MajorClaims\\\": [\\n    {\\n      \\\"ID\\\": \\\"MC1\\\",\\n      \\\"Text\\\": \\\"artists must be given freedom so that they will produce some really marvelous masterpiece\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"MC2\\\",\\n      \\\"Text\\\": \\\"there should not be any restrictions on artists' work\\\"\\n    }\\n  ],\\n  \\\"Claims\\\": [\\n    {\\n      \\\"ID\\\": \\\"C1\\\",\\n      \\\"Text\\\": \\\"if there is control over artists' ideas, they will definitely lose their sense of creativity in the long run\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C2\\\",\\n      \\\"Text\\\": \\\"it is every human's right to be able to voice out their opinions in any ways as lo\n"
     ]
    }
   ],
   "source": [
    "# Abrufen und Speichern der Batch-Ergebnisse (Output-Datei)\n",
    "batch_5_results = retrieve_and_save_batch_results(batch_5_status.output_file_id, BATCH_OUTPUT_PATH, \"output-batch-5\", client)\n",
    "print(batch_5_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-58BfmzR7AzjHXBQKiRdeTG', bytes=79088105, created_at=1736788702, filename='batch_input_6.jsonl', object='file', purpose='batch', status='processed', status_details=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload der Batch Input-Datei auf die OpenAI-Plattform\n",
    "batch_file_6 = upload_batch_file(BATCH_INPUT_PATH + \"batch_input_6.jsonl\", client)\n",
    "batch_file_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_67860a74a03481909107e2a89f10a187', completion_window='24h', created_at=1736837748, endpoint='/v1/chat/completions', input_file_id='file-58BfmzR7AzjHXBQKiRdeTG', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1736924148, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Batch 6/9 for Argument Mining'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Batches\n",
    "metadata_dict = {\"description\": \"Batch 6/9 for Argument Mining\"}\n",
    "batch_6 = create_batch(batch_file_6.id, metadata_dict, client) # sofern die Batch Datei bereits hochgeladen wurde, aber nicht erfolgreich war, kann die Batch-ID erneut verwendet werden. Ein erneuter Uplaod ist nicht notwendig und würde der Datei eine neue ID zuweisen.\n",
    "print(batch_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: completed\n",
      "\n",
      "Beschreibung des Batches: Batch 6/9 for Argument Mining\n",
      "Anfragen gesamt: 322\n",
      "Davon erfolgreich: 322\n",
      "Davon fehlerhaft: 0\n",
      "Erfolgreiche Abfragen können abgerufen werden mit ID: file-11jo846QQ1pmGXxp5JDNyZ\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_67860a74a03481909107e2a89f10a187', completion_window='24h', created_at=1736837748, endpoint='/v1/chat/completions', input_file_id='file-58BfmzR7AzjHXBQKiRdeTG', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736839559, error_file_id=None, errors=None, expired_at=None, expires_at=1736924148, failed_at=None, finalizing_at=1736839526, in_progress_at=1736837752, metadata={'description': 'Batch 6/9 for Argument Mining'}, output_file_id='file-11jo846QQ1pmGXxp5JDNyZ', request_counts=BatchRequestCounts(completed=322, failed=0, total=322))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "batch_6_status = check_batch_status(batch_6.id, client)\n",
    "print(batch_6_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"batch_req_6786116710148190b712ba1d6fd42154\", \"custom_id\": \"few-shot-40-persona_essay037.txt\", \"response\": {\"status_code\": 200, \"request_id\": \"8dddee82cd827ecaaa9339f9b31859c8\", \"body\": {\"id\": \"chatcmpl-ApV8ul7jazbBrLlmszvK1SEYlAgly\", \"object\": \"chat.completion\", \"created\": 1736837804, \"model\": \"gpt-4o-mini-2024-07-18\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"{\\n  \\\"MajorClaims\\\": [\\n    {\\n      \\\"ID\\\": \\\"MC1\\\",\\n      \\\"Text\\\": \\\"international sporting occasions are essential in easing international tensions\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"MC2\\\",\\n      \\\"Text\\\": \\\"International sporting events will make the world more peaceful\\\"\\n    }\\n  ],\\n  \\\"Claims\\\": [\\n    {\\n      \\\"ID\\\": \\\"C1\\\",\\n      \\\"Text\\\": \\\"international sporting events are a good change to create a multi-nation community of fans having the same passion\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C2\\\",\\n      \\\"Text\\\": \\\"people around the world understand each other more\\\"\\n    },\\n    {\\\n"
     ]
    }
   ],
   "source": [
    "# Abrufen und Speichern der Batch-Ergebnisse (Output-Datei)\n",
    "batch_6_results = retrieve_and_save_batch_results(batch_6_status.output_file_id, BATCH_OUTPUT_PATH, \"output-batch-6\", client)\n",
    "print(batch_6_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-FVN2b1jfp2RS5JKC9ng7Em', bytes=79095032, created_at=1736924736, filename='batch_input_7.jsonl', object='file', purpose='batch', status='processed', status_details=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload der Batch Input-Datei auf die OpenAI-Plattform\n",
    "batch_file_7 = upload_batch_file(BATCH_INPUT_PATH + \"batch_input_7.jsonl\", client)\n",
    "batch_file_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_67875e439c0081909378f77faa6010ce', completion_window='24h', created_at=1736924739, endpoint='/v1/chat/completions', input_file_id='file-FVN2b1jfp2RS5JKC9ng7Em', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1737011139, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Batch 7/9 for Argument Mining'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Batches\n",
    "metadata_dict = {\"description\": \"Batch 7/9 for Argument Mining\"}\n",
    "batch_7 = create_batch(batch_file_7.id, metadata_dict, client) # sofern die Batch Datei bereits hochgeladen wurde, aber nicht erfolgreich war, kann die Batch-ID erneut verwendet werden. Ein erneuter Uplaod ist nicht notwendig und würde der Datei eine neue ID zuweisen.\n",
    "print(batch_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: completed\n",
      "\n",
      "Beschreibung des Batches: Batch 7/9 for Argument Mining\n",
      "Anfragen gesamt: 320\n",
      "Davon erfolgreich: 320\n",
      "Davon fehlerhaft: 0\n",
      "Erfolgreiche Abfragen können abgerufen werden mit ID: file-SntzBR9MciREe9ZepXmZ6o\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_67875e439c0081909378f77faa6010ce', completion_window='24h', created_at=1736924739, endpoint='/v1/chat/completions', input_file_id='file-FVN2b1jfp2RS5JKC9ng7Em', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736925178, error_file_id=None, errors=None, expired_at=None, expires_at=1737011139, failed_at=None, finalizing_at=1736925160, in_progress_at=1736924742, metadata={'description': 'Batch 7/9 for Argument Mining'}, output_file_id='file-SntzBR9MciREe9ZepXmZ6o', request_counts=BatchRequestCounts(completed=320, failed=0, total=320))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "batch_7_status = check_batch_status(batch_7.id, client)\n",
    "print(batch_7_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"batch_req_67875fe848308190b163df1129df2b14\", \"custom_id\": \"few-shot-40-persona_essay395.txt\", \"response\": {\"status_code\": 200, \"request_id\": \"e075021751db2fedcaf1daf7e6408d10\", \"body\": {\"id\": \"chatcmpl-AprlxQdnDoYgdeBHR0ieSwayx0UsH\", \"object\": \"chat.completion\", \"created\": 1736924793, \"model\": \"gpt-4o-mini-2024-07-18\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"{\\n  \\\"MajorClaims\\\": [\\n    {\\n      \\\"ID\\\": \\\"MC1\\\",\\n      \\\"Text\\\": \\\"these taxes are absolutely essential\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"MC2\\\",\\n      \\\"Text\\\": \\\"taxes paying for state schools are necessary to be compulsory for all members of society no matter where their children enroll in\\\"\\n    }\\n  ],\\n  \\\"Claims\\\": [\\n    {\\n      \\\"ID\\\": \\\"C1\\\",\\n      \\\"Text\\\": \\\"affluent people effectively contribute to narrowing down the gap between rich and poor\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C2\\\",\\n      \\\"Text\\\": \\\"the tax reduction for parents of children studying in private schools wou\n"
     ]
    }
   ],
   "source": [
    "# Abrufen und Speichern der Batch-Ergebnisse (Output-Datei)\n",
    "batch_7_results = retrieve_and_save_batch_results(batch_7_status.output_file_id, BATCH_OUTPUT_PATH, \"output-batch-7\", client)\n",
    "print(batch_7_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-W8TyyDEwHrXiPGFpwGjVLg', bytes=79137498, created_at=1737010757, filename='batch_input_8.jsonl', object='file', purpose='batch', status='processed', status_details=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload der Batch Input-Datei auf die OpenAI-Plattform\n",
    "batch_file_8 = upload_batch_file(BATCH_INPUT_PATH + \"batch_input_8.jsonl\", client)\n",
    "batch_file_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_6788ae8069248190807e7a0ce2a5ebc9', completion_window='24h', created_at=1737010816, endpoint='/v1/chat/completions', input_file_id='file-W8TyyDEwHrXiPGFpwGjVLg', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1737097216, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Batch 8/9 for Argument Mining'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Batches\n",
    "metadata_dict = {\"description\": \"Batch 8/9 for Argument Mining\"}\n",
    "batch_8 = create_batch(batch_file_8.id, metadata_dict, client) # sofern die Batch Datei bereits hochgeladen wurde, aber nicht erfolgreich war, kann die Batch-ID erneut verwendet werden. Ein erneuter Uplaod ist nicht notwendig und würde der Datei eine neue ID zuweisen.\n",
    "print(batch_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: completed\n",
      "\n",
      "Beschreibung des Batches: Batch 8/9 for Argument Mining\n",
      "Anfragen gesamt: 320\n",
      "Davon erfolgreich: 320\n",
      "Davon fehlerhaft: 0\n",
      "Erfolgreiche Abfragen können abgerufen werden mit ID: file-5jM9kQcHcNXSxJypakw1TC\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_6788ae8069248190807e7a0ce2a5ebc9', completion_window='24h', created_at=1737010816, endpoint='/v1/chat/completions', input_file_id='file-W8TyyDEwHrXiPGFpwGjVLg', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1737012493, error_file_id=None, errors=None, expired_at=None, expires_at=1737097216, failed_at=None, finalizing_at=1737012470, in_progress_at=1737010819, metadata={'description': 'Batch 8/9 for Argument Mining'}, output_file_id='file-5jM9kQcHcNXSxJypakw1TC', request_counts=BatchRequestCounts(completed=320, failed=0, total=320))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "batch_8_status = check_batch_status(batch_8.id, client)\n",
    "print(batch_8_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"batch_req_6788b4f714d081908df4f1d16c553782\", \"custom_id\": \"few-shot-40-cot_essay350.txt\", \"response\": {\"status_code\": 200, \"request_id\": \"cad64615ec7703d4819bcffc1e9412ed\", \"body\": {\"id\": \"chatcmpl-AqE9ifGMKq4tePxnXoXTf4ZFcSXZK\", \"object\": \"chat.completion\", \"created\": 1737010834, \"model\": \"gpt-4o-mini-2024-07-18\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"{\\n  \\\"MajorClaims\\\": [\\n    {\\n      \\\"ID\\\": \\\"MC1\\\",\\n      \\\"Text\\\": \\\"newspapers and magazines will be history with the time\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"MC2\\\",\\n      \\\"Text\\\": \\\"digital media will rule over paper and magazines\\\"\\n    }\\n  ],\\n  \\\"Claims\\\": [\\n    {\\n      \\\"ID\\\": \\\"C1\\\",\\n      \\\"Text\\\": \\\"traditional paper media will come to an end very soon due to the increased usage of internet based media\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C2\\\",\\n      \\\"Text\\\": \\\"newspapers and magazines can not respond as fast as digital media\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C3\\\",\\n      \\\"Text\\\":\n"
     ]
    }
   ],
   "source": [
    "# Abrufen und Speichern der Batch-Ergebnisse (Output-Datei)\n",
    "batch_8_results = retrieve_and_save_batch_results(batch_8_status.output_file_id, BATCH_OUTPUT_PATH, \"output-batch-8\", client)\n",
    "print(batch_8_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-NUfgkd9nNh7XB4D4BN1isN', bytes=22248980, created_at=1737093443, filename='batch_input_9.jsonl', object='file', purpose='batch', status='processed', status_details=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload der Batch Input-Datei auf die OpenAI-Plattform\n",
    "batch_file_9 = upload_batch_file(BATCH_INPUT_PATH + \"batch_input_9.jsonl\", client)\n",
    "batch_file_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_6789f1518d488190addaf826cf4151c3', completion_window='24h', created_at=1737093457, endpoint='/v1/chat/completions', input_file_id='file-NUfgkd9nNh7XB4D4BN1isN', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1737179857, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Batch 9/9 for Argument Mining'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Batches\n",
    "metadata_dict = {\"description\": \"Batch 9/9 for Argument Mining\"}\n",
    "batch_9 = create_batch(batch_file_9.id, metadata_dict, client) # sofern die Batch Datei bereits hochgeladen wurde, aber nicht erfolgreich war, kann die Batch-ID erneut verwendet werden. Ein erneuter Uplaod ist nicht notwendig und würde der Datei eine neue ID zuweisen.\n",
    "print(batch_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: completed\n",
      "\n",
      "Beschreibung des Batches: Batch 9/9 for Argument Mining\n",
      "Anfragen gesamt: 90\n",
      "Davon erfolgreich: 90\n",
      "Davon fehlerhaft: 0\n",
      "Erfolgreiche Abfragen können abgerufen werden mit ID: file-HqqCeMenJSYDTFvuStyVzx\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_6789f1518d488190addaf826cf4151c3', completion_window='24h', created_at=1737093457, endpoint='/v1/chat/completions', input_file_id='file-NUfgkd9nNh7XB4D4BN1isN', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1737094502, error_file_id=None, errors=None, expired_at=None, expires_at=1737179857, failed_at=None, finalizing_at=1737094496, in_progress_at=1737093458, metadata={'description': 'Batch 9/9 for Argument Mining'}, output_file_id='file-HqqCeMenJSYDTFvuStyVzx', request_counts=BatchRequestCounts(completed=90, failed=0, total=90))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "batch_9_status = check_batch_status(batch_9.id, client)\n",
    "print(batch_9_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"batch_req_6789f560ad1c81909d701102b78243ca\", \"custom_id\": \"few-shot-40-persona-cot_essay302.txt\", \"response\": {\"status_code\": 200, \"request_id\": \"60160904162abffef33aa4eea2d5a56b\", \"body\": {\"id\": \"chatcmpl-AqZeQvxI1L70irTZglv0gB1ygmFBj\", \"object\": \"chat.completion\", \"created\": 1737093462, \"model\": \"gpt-4o-mini-2024-07-18\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"{\\n  \\\"MajorClaims\\\": [\\n    {\\n      \\\"ID\\\": \\\"MC1\\\",\\n      \\\"Text\\\": \\\"saving some part of your earnings is essential\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"MC2\\\",\\n      \\\"Text\\\": \\\"there is no other choice than saving money for some time\\\"\\n    }\\n  ],\\n  \\\"Claims\\\": [\\n    {\\n      \\\"ID\\\": \\\"C1\\\",\\n      \\\"Text\\\": \\\"you can not predict future developments, neither in your professional nor your personal life\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C2\\\",\\n      \\\"Text\\\": \\\"there are several uncertainties about your professional future\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C3\\\",\\n      \\\"Text\\\": \\\"you \n"
     ]
    }
   ],
   "source": [
    "# Abrufen und Speichern der Batch-Ergebnisse (Output-Datei)\n",
    "batch_9_results = retrieve_and_save_batch_results(batch_9_status.output_file_id, BATCH_OUTPUT_PATH, \"output-batch-9\", client)\n",
    "print(batch_9_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Übersicht der Batches für Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: DAS IST EIN TEST\n",
      "Status: completed\n",
      "Batch-ID: batch_67a62b3adf9881909cfbb4033f3b08de\n",
      "----------------------------------------\n",
      "Metadata: Batch 9/9 for Argument Mining\n",
      "Status: completed\n",
      "Batch-ID: batch_6789f1518d488190addaf826cf4151c3\n",
      "----------------------------------------\n",
      "Metadata: Batch 8/9 for Argument Mining\n",
      "Status: completed\n",
      "Batch-ID: batch_6788ae8069248190807e7a0ce2a5ebc9\n",
      "----------------------------------------\n",
      "Metadata: Batch 7/9 for Argument Mining\n",
      "Status: completed\n",
      "Batch-ID: batch_67875e439c0081909378f77faa6010ce\n",
      "----------------------------------------\n",
      "Metadata: Batch 6/9 for Argument Mining\n",
      "Status: completed\n",
      "Batch-ID: batch_67860a74a03481909107e2a89f10a187\n",
      "----------------------------------------\n",
      "Metadata: Batch 6/9 for Argument Mining\n",
      "Status: failed\n",
      "Batch-ID: batch_67854ae1f8e08190bfbb2b6126ce4f2e\n",
      "Errors: Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 20,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list')\n",
      "----------------------------------------\n",
      "Metadata: Batch 5/9 for Argument Mining\n",
      "Status: completed\n",
      "Batch-ID: batch_6784d168bb288190bda2b11e3b42da3c\n",
      "----------------------------------------\n",
      "Metadata: Batch 4/9 for Argument Mining\n",
      "Status: completed\n",
      "Batch-ID: batch_67837fec4f04819088f396053ef790f2\n",
      "----------------------------------------\n",
      "Metadata: Batch 3/9 for Argument Mining\n",
      "Status: completed\n",
      "Batch-ID: batch_67822fc8058c8190b864f42b92ad646d\n",
      "----------------------------------------\n",
      "Metadata: Batch 2/9 for Argument Mining\n",
      "Status: completed\n",
      "Batch-ID: batch_6780ded3cd58819080b492f6647492b4\n",
      "----------------------------------------\n",
      "Metadata: Batch 1/9 for Argument Mining\n",
      "Status: completed\n",
      "Batch-ID: batch_677f7b3b1b48819086a5a27b7173c2c3\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Auflistung vergangener Batches\n",
    "batches_data = client.batches.list(limit=11).data\n",
    "for batch in batches_data:\n",
    "    print(f\"Metadata: {batch.metadata['description']}\")\n",
    "    print(f\"Status: {batch.status}\")\n",
    "    print(f\"Batch-ID: {batch.id}\")\n",
    "    if batch.status == \"failed\":\n",
    "        print(f\"Errors: {batch.errors}\")\n",
    "    print(f\"-\" * 40) \n",
    "\n",
    "# Info: Es scheint bisher keine möglichkeit zu geben Batches aus der Liste zu löschen. Das geht unter anderem aus folgenden Quellen hervor:\n",
    "# - https://platform.openai.com/docs/guides/batch/batch-api#6-cancelling-a-batch\n",
    "# - https://community.openai.com/t/how-to-restart-failed-batch-jobs-or-delete-batch-jobs/876352\n",
    "# Die Test-Batches oder fehlgeschlagenen Batches können daher nicht aus der Liste entfernt werden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
