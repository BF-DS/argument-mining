{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from src.dataimport import list_files_with_extension_directory, list_files_with_extension, load_text, list_files\n",
    "from src.llmlib import num_tokens_from_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT_FILES_PATH = 'data/original/brat-project-final/'\n",
    "JSON_FILES_PATH = 'data/transformed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Text-Dateien: 402\n",
      "Anzahl Brat-Dateien: 402\n"
     ]
    }
   ],
   "source": [
    "txt_files_directory_list = list_files_with_extension_directory(TXT_FILES_PATH, '.txt')\n",
    "# txt_files_directory_list\n",
    "\n",
    "json_files_directory_list = list_files_with_extension_directory(JSON_FILES_PATH, '.json')\n",
    "# json_files_directory_list\n",
    "\n",
    "print(f\"Anzahl Text-Dateien: {len(txt_files_directory_list)}\")\n",
    "print(f\"Anzahl Brat-Dateien: {len(json_files_directory_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(402, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt_path</th>\n",
       "      <th>json_path</th>\n",
       "      <th>txt_file</th>\n",
       "      <th>json_file</th>\n",
       "      <th>txt</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/original/brat-project-final/essay001.txt</td>\n",
       "      <td>data/transformed/essay001.json</td>\n",
       "      <td>essay001.txt</td>\n",
       "      <td>essay001.json</td>\n",
       "      <td>Should students be taught to compete or to coo...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/original/brat-project-final/essay002.txt</td>\n",
       "      <td>data/transformed/essay002.json</td>\n",
       "      <td>essay002.txt</td>\n",
       "      <td>essay002.json</td>\n",
       "      <td>More people are migrating to other countries t...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/original/brat-project-final/essay003.txt</td>\n",
       "      <td>data/transformed/essay003.json</td>\n",
       "      <td>essay003.txt</td>\n",
       "      <td>essay003.json</td>\n",
       "      <td>International tourism is now more common than ...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/original/brat-project-final/essay004.txt</td>\n",
       "      <td>data/transformed/essay004.json</td>\n",
       "      <td>essay004.txt</td>\n",
       "      <td>essay004.json</td>\n",
       "      <td>International tourism is now more common than ...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/original/brat-project-final/essay005.txt</td>\n",
       "      <td>data/transformed/essay005.json</td>\n",
       "      <td>essay005.txt</td>\n",
       "      <td>essay005.json</td>\n",
       "      <td>Living and studying overseas\\n\\nIt is every st...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        txt_path  \\\n",
       "0  data/original/brat-project-final/essay001.txt   \n",
       "1  data/original/brat-project-final/essay002.txt   \n",
       "2  data/original/brat-project-final/essay003.txt   \n",
       "3  data/original/brat-project-final/essay004.txt   \n",
       "4  data/original/brat-project-final/essay005.txt   \n",
       "\n",
       "                        json_path      txt_file      json_file  \\\n",
       "0  data/transformed/essay001.json  essay001.txt  essay001.json   \n",
       "1  data/transformed/essay002.json  essay002.txt  essay002.json   \n",
       "2  data/transformed/essay003.json  essay003.txt  essay003.json   \n",
       "3  data/transformed/essay004.json  essay004.txt  essay004.json   \n",
       "4  data/transformed/essay005.json  essay005.txt  essay005.json   \n",
       "\n",
       "                                                 txt  \\\n",
       "0  Should students be taught to compete or to coo...   \n",
       "1  More people are migrating to other countries t...   \n",
       "2  International tourism is now more common than ...   \n",
       "3  International tourism is now more common than ...   \n",
       "4  Living and studying overseas\\n\\nIt is every st...   \n",
       "\n",
       "                                                json  \n",
       "0  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "1  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "2  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "3  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "4  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe with file names\n",
    "df = pd.DataFrame()\n",
    "df['txt_path'] = txt_files_directory_list\n",
    "df['json_path'] = json_files_directory_list\n",
    "df['txt_file'] = df['txt_path'].apply(lambda x: os.path.basename(x))\n",
    "df['json_file'] = df['json_path'].apply(lambda x: os.path.basename(x))\n",
    "df['txt'] = df['txt_path'].apply(load_text)\n",
    "df['json'] = df['json_path'].apply(load_text)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n",
    "\n",
    "# save to csv\n",
    "#df.to_csv('dataframe.csv', index=False)\n",
    "# load dataframe\n",
    "# df = pd.read_csv('dataframe.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DataFrame: (40, 6)\n",
      "\n",
      "Test DataFrame: (362, 6)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataframe into training and test sets\n",
    "train_df, test_df = train_test_split(df, train_size=40, random_state=42)\n",
    "\n",
    "# Display the first few rows of the training and test sets\n",
    "print(f\"Training DataFrame: {train_df.shape}\")\n",
    "print(f\"\\nTest DataFrame: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt_path</th>\n",
       "      <th>json_path</th>\n",
       "      <th>txt_file</th>\n",
       "      <th>json_file</th>\n",
       "      <th>txt</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>data/original/brat-project-final/essay021.txt</td>\n",
       "      <td>data/transformed/essay021.json</td>\n",
       "      <td>essay021.txt</td>\n",
       "      <td>essay021.json</td>\n",
       "      <td>Advertisements affects on consumer goods\\n\\nEv...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>data/original/brat-project-final/essay022.txt</td>\n",
       "      <td>data/transformed/essay022.json</td>\n",
       "      <td>essay022.txt</td>\n",
       "      <td>essay022.json</td>\n",
       "      <td>Young people should go to university or not\\n\\...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>data/original/brat-project-final/essay049.txt</td>\n",
       "      <td>data/transformed/essay049.json</td>\n",
       "      <td>essay049.txt</td>\n",
       "      <td>essay049.json</td>\n",
       "      <td>Do modern communication technologies benefit a...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>data/original/brat-project-final/essay051.txt</td>\n",
       "      <td>data/transformed/essay051.json</td>\n",
       "      <td>essay051.txt</td>\n",
       "      <td>essay051.json</td>\n",
       "      <td>Universities should give money to sport activi...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>data/original/brat-project-final/essay055.txt</td>\n",
       "      <td>data/transformed/essay055.json</td>\n",
       "      <td>essay055.txt</td>\n",
       "      <td>essay055.json</td>\n",
       "      <td>Should teenagers learn all school subjects/foc...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         txt_path  \\\n",
       "20  data/original/brat-project-final/essay021.txt   \n",
       "21  data/original/brat-project-final/essay022.txt   \n",
       "48  data/original/brat-project-final/essay049.txt   \n",
       "50  data/original/brat-project-final/essay051.txt   \n",
       "54  data/original/brat-project-final/essay055.txt   \n",
       "\n",
       "                         json_path      txt_file      json_file  \\\n",
       "20  data/transformed/essay021.json  essay021.txt  essay021.json   \n",
       "21  data/transformed/essay022.json  essay022.txt  essay022.json   \n",
       "48  data/transformed/essay049.json  essay049.txt  essay049.json   \n",
       "50  data/transformed/essay051.json  essay051.txt  essay051.json   \n",
       "54  data/transformed/essay055.json  essay055.txt  essay055.json   \n",
       "\n",
       "                                                  txt  \\\n",
       "20  Advertisements affects on consumer goods\\n\\nEv...   \n",
       "21  Young people should go to university or not\\n\\...   \n",
       "48  Do modern communication technologies benefit a...   \n",
       "50  Universities should give money to sport activi...   \n",
       "54  Should teenagers learn all school subjects/foc...   \n",
       "\n",
       "                                                 json  \n",
       "20  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "21  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "48  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "50  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "54  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the dataframes\n",
    "train_df = train_df.sort_values(by='txt_file')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt_path</th>\n",
       "      <th>json_path</th>\n",
       "      <th>txt_file</th>\n",
       "      <th>json_file</th>\n",
       "      <th>txt</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/original/brat-project-final/essay001.txt</td>\n",
       "      <td>data/transformed/essay001.json</td>\n",
       "      <td>essay001.txt</td>\n",
       "      <td>essay001.json</td>\n",
       "      <td>Should students be taught to compete or to coo...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/original/brat-project-final/essay002.txt</td>\n",
       "      <td>data/transformed/essay002.json</td>\n",
       "      <td>essay002.txt</td>\n",
       "      <td>essay002.json</td>\n",
       "      <td>More people are migrating to other countries t...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/original/brat-project-final/essay003.txt</td>\n",
       "      <td>data/transformed/essay003.json</td>\n",
       "      <td>essay003.txt</td>\n",
       "      <td>essay003.json</td>\n",
       "      <td>International tourism is now more common than ...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/original/brat-project-final/essay004.txt</td>\n",
       "      <td>data/transformed/essay004.json</td>\n",
       "      <td>essay004.txt</td>\n",
       "      <td>essay004.json</td>\n",
       "      <td>International tourism is now more common than ...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/original/brat-project-final/essay005.txt</td>\n",
       "      <td>data/transformed/essay005.json</td>\n",
       "      <td>essay005.txt</td>\n",
       "      <td>essay005.json</td>\n",
       "      <td>Living and studying overseas\\n\\nIt is every st...</td>\n",
       "      <td>{\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        txt_path  \\\n",
       "0  data/original/brat-project-final/essay001.txt   \n",
       "1  data/original/brat-project-final/essay002.txt   \n",
       "2  data/original/brat-project-final/essay003.txt   \n",
       "3  data/original/brat-project-final/essay004.txt   \n",
       "4  data/original/brat-project-final/essay005.txt   \n",
       "\n",
       "                        json_path      txt_file      json_file  \\\n",
       "0  data/transformed/essay001.json  essay001.txt  essay001.json   \n",
       "1  data/transformed/essay002.json  essay002.txt  essay002.json   \n",
       "2  data/transformed/essay003.json  essay003.txt  essay003.json   \n",
       "3  data/transformed/essay004.json  essay004.txt  essay004.json   \n",
       "4  data/transformed/essay005.json  essay005.txt  essay005.json   \n",
       "\n",
       "                                                 txt  \\\n",
       "0  Should students be taught to compete or to coo...   \n",
       "1  More people are migrating to other countries t...   \n",
       "2  International tourism is now more common than ...   \n",
       "3  International tourism is now more common than ...   \n",
       "4  Living and studying overseas\\n\\nIt is every st...   \n",
       "\n",
       "                                                json  \n",
       "0  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "1  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "2  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "3  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  \n",
       "4  {\\n  \"MajorClaims\": [\\n    {\\n      \"ID\": \"MC1...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = test_df.sort_values(by='txt_file')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chain-of-thought.txt',\n",
       " 'output-structure.txt',\n",
       " 'persona.txt',\n",
       " 'task-description.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUILDING_BLOCKS_PATH = 'prompts/building-blocks/'\n",
    "PROMPTS_PATH = 'prompts/final-prompts/'\n",
    "\n",
    "list_files(BUILDING_BLOCKS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-shot prompt\n",
    "task_description = load_text(BUILDING_BLOCKS_PATH + 'task-description.txt')\n",
    "persona = load_text(BUILDING_BLOCKS_PATH + 'persona.txt')\n",
    "cot = load_text(BUILDING_BLOCKS_PATH + 'chain-of-thought.txt')\n",
    "output_structure = load_text(BUILDING_BLOCKS_PATH + 'output-structure.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Shot (ZS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs = task_description\n",
    "zs_persona = persona + task_description\n",
    "zs_cot = task_description + '\\n' + cot\n",
    "zs_persona_cot = persona + task_description + '\\n' + cot\n",
    "\n",
    "# save prompts to files\n",
    "with open(PROMPTS_PATH + 'zero-shot.txt', 'w') as f:\n",
    "    f.write(zs)\n",
    "\n",
    "with open(PROMPTS_PATH + 'zero-shot-persona.txt', 'w') as f:\n",
    "    f.write(zs_persona)\n",
    "\n",
    "with open(PROMPTS_PATH + 'zero-shot-cot.txt', 'w') as f:\n",
    "    f.write(zs_cot)\n",
    "\n",
    "with open(PROMPTS_PATH + 'zero-shot-persona-cot.txt', 'w') as f:\n",
    "    f.write(zs_persona_cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Shot (OS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-shot prompt - 1 example from the training set\n",
    "examples_1 = train_df.sample(1, random_state=42)\n",
    "\n",
    "# extract the text and json from the row\n",
    "os_txt = examples_1['txt'].values[0]\n",
    "os_json = examples_1['json'].values[0]\n",
    "os_example = f\"## Input:\\n{os_txt}\\n## Output:\\n{os_json}\"\n",
    "\n",
    "os = task_description + 'Here is one example of a text and its corresponding json data:\\n' + os_example\n",
    "os_persona = persona + task_description + '\\n' + os_example\n",
    "os_cot = task_description + '\\n' + cot + '\\n' + os_example\n",
    "os_persona_cot = persona + task_description + '\\n' + cot + '\\n' + os_example\n",
    "\n",
    "# save the prompts to files\n",
    "with open(PROMPTS_PATH + 'one-shot.txt', 'w') as f:\n",
    "    f.write(os)\n",
    "\n",
    "with open(PROMPTS_PATH + 'one-shot-persona.txt', 'w') as f:\n",
    "    f.write(os_persona)\n",
    "\n",
    "with open(PROMPTS_PATH + 'one-shot-cot.txt', 'w') as f:\n",
    "    f.write(os_cot)\n",
    "\n",
    "with open(PROMPTS_PATH + 'one-shot-persona-cot.txt', 'w') as f:\n",
    "    f.write(os_persona_cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot (FS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test mit LangChain FewshotPromptTemplate\n",
    "M.E nicht mehr notwendig, da bereits eigener Weg gefunden wurde um Modell Template zu erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = []\n",
    "\n",
    "# for idx, row in examples_10.iterrows():\n",
    "#     input = row['txt']\n",
    "#     output = row['json']\n",
    "#     results.append({'input': input, 'output': output})\n",
    "\n",
    "# # save the results in a dataframe\n",
    "# examples_df = pd.DataFrame(results)\n",
    "# examples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beipsiele als Input-Output-Liste\n",
    "# examples_list = [f\"## Input: {row['txt']} \\n## Output: {row['json']}\" for idx, row in examples_10.iterrows()] \n",
    "# examples_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_str_1, example_str_2, example_str_3, example_str_4, example_str_5, example_str_6, example_str_7, example_str_8, example_str_9, example_str_10 = examples_list\n",
    "# print(example_str_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import FewShotChatMessagePromptTemplate\n",
    "\n",
    "# zero_shot = examples_df[examples_df['prompt_file'] == 'zero-shot']['prompt'].values[0] \n",
    "\n",
    "# # examples = [\n",
    "# #     {\"input\": row['txt'], \"output\": row['json']} for idx, row in examples_10.iterrows()\n",
    "# # ]\n",
    "# example_prompt = ChatPromptTemplate.from_messages(\n",
    "#     [('user', '{input}'), ('assistent', '{output}')] # user, system,\n",
    "#     )\n",
    "\n",
    "# few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "#     examples=examples_list,\n",
    "#     # This is a ormpt template used to format each individual example\n",
    "#     example_prompt=example_prompt,\n",
    "#     # prefix = \"\"\n",
    "#     # suffix = \"Text: {input}\\n Output:\",\n",
    "#     # input_variable_names = ['input'],\n",
    "# )\n",
    "\n",
    "# final_prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", zero_shot),\n",
    "#         few_shot_prompt,\n",
    "#         (\"user\", '{input}'),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# print(final_prompt.format(input=test_df_sample['txt'][25]))\n",
    "\n",
    "\n",
    "# Quelle: https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.few_shot.FewShotChatMessagePromptTemplate.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # invoke the chain\n",
    "# few_shot_answer = final_prompt.invoke({\"input\": test_df_sample['txt'][25]})\n",
    "# print(few_shot_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FS 10 - 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few-shot prompt - 10 examples from the training set\n",
    "examples_10 = train_df.sample(10, random_state=42)\n",
    "\n",
    "few_shot_examples_10 = f\"\\nHere are 10 examples of text and their corresponding json data:\\n\" # adding task description to the beginning of the prompt\n",
    "example_counter = 1\n",
    "for idx, row in examples_10.iterrows():\n",
    "    example_str = f\"\\n# Example {example_counter}\\n## Input:\\n{row['txt']}\\n## Output:\\n{row['json']}\"\n",
    "    few_shot_examples_10 += example_str\n",
    "    example_counter += 1\n",
    "\n",
    "fs = task_description + few_shot_examples_10\n",
    "fs_persona = persona + task_description + few_shot_examples_10\n",
    "fs_cot = task_description + '\\n' + cot + few_shot_examples_10\n",
    "fs_persona_cot = persona + task_description + '\\n' + cot + few_shot_examples_10\n",
    "\n",
    "# save the prompts to files\n",
    "with open(PROMPTS_PATH + 'few-shot-10.txt', 'w') as f:\n",
    "    f.write(fs)\n",
    "\n",
    "with open(PROMPTS_PATH + 'few-shot-10-persona.txt', 'w') as f:\n",
    "    f.write(fs_persona)\n",
    "\n",
    "with open(PROMPTS_PATH + 'few-shot-10-cot.txt', 'w') as f:\n",
    "    f.write(fs_cot)\n",
    "\n",
    "with open(PROMPTS_PATH + 'few-shot-10-persona-cot.txt', 'w') as f:\n",
    "    f.write(fs_persona_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few-shot prompt - 20 examples from the training set\n",
    "examples_20 = train_df.sample(20, random_state=42)\n",
    "\n",
    "few_shot_examples_20 = f\"\\nHere are 20 examples of text and their corresponding json data:\\n\" # adding task description to the beginning of the prompt\n",
    "example_counter = 1\n",
    "for idx, row in examples_20.iterrows():\n",
    "    example_str = f\"\\n# Example {example_counter}\\n## Input:\\n{row['txt']}\\n## Output:\\n{row['json']}\"\n",
    "    few_shot_examples_20 += example_str\n",
    "    example_counter += 1\n",
    "\n",
    "fs = task_description + few_shot_examples_20\n",
    "fs_persona = persona + task_description + few_shot_examples_20\n",
    "fs_cot = task_description + '\\n' + cot + few_shot_examples_20\n",
    "fs_persona_cot = persona + task_description + '\\n' + cot + few_shot_examples_20\n",
    "\n",
    "# save the prompts to files\n",
    "with open(PROMPTS_PATH + 'few-shot-20.txt', 'w') as f:\n",
    "    f.write(fs)\n",
    "\n",
    "with open(PROMPTS_PATH + 'few-shot-20-persona.txt', 'w') as f:\n",
    "    f.write(fs_persona)\n",
    "\n",
    "with open(PROMPTS_PATH + 'few-shot-20-cot.txt', 'w') as f:\n",
    "    f.write(fs_cot)\n",
    "\n",
    "with open(PROMPTS_PATH + 'few-shot-20-persona-cot.txt', 'w') as f:\n",
    "    f.write(fs_persona_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few-shot prompt - 40 examples from the training set\n",
    "examples_40 = train_df.sample(40, random_state=42)\n",
    "\n",
    "few_shot_str_40 = f\"\\nHere are 40 examples of text and their corresponding json data:\\n\" # adding task description to the beginning of the prompt\n",
    "example_counter = 1\n",
    "for idx, row in examples_40.iterrows():\n",
    "    example_str = f\"\\n# Example {example_counter}\\n## Input:\\n{row['txt']}\\n## Output:\\n{row['json']}\"\n",
    "    few_shot_str_40 += example_str\n",
    "    example_counter += 1\n",
    "\n",
    "fs = task_description + few_shot_str_40\n",
    "fs_persona = persona + task_description + few_shot_str_40\n",
    "fs_cot = task_description + '\\n' + cot + few_shot_str_40\n",
    "fs_persona_cot = persona + task_description + '\\n' + cot + few_shot_str_40\n",
    "\n",
    "# save the prompts to files\n",
    "with open(PROMPTS_PATH + 'few-shot-40.txt', 'w') as f:\n",
    "    f.write(fs)\n",
    "\n",
    "with open(PROMPTS_PATH + 'few-shot-40-persona.txt', 'w') as f:\n",
    "    f.write(fs_persona)\n",
    "\n",
    "with open(PROMPTS_PATH + 'few-shot-40-cot.txt', 'w') as f:\n",
    "    f.write(fs_cot)\n",
    "\n",
    "with open(PROMPTS_PATH + 'few-shot-40-persona-cot.txt', 'w') as f:\n",
    "    f.write(fs_persona_cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# list prompt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es gibt 20 Prompts\n"
     ]
    }
   ],
   "source": [
    "# Eigentlich wird die os-Bibliothek bereits obengeladen. Da es aber vereinzelt zu Fehlermeldungen kam, wird sie hier nochmals geladen.\n",
    "import os\n",
    "\n",
    "prompt_files_directory_list = list_files_with_extension_directory(PROMPTS_PATH, '.txt')\n",
    "prompt_files_directory_list\n",
    "prompt_files_list = [os.path.basename(x) for x in prompt_files_directory_list]\n",
    "# remove the .txt extension\n",
    "prompt_names = [x.split('.')[0] for x in prompt_files_list]\n",
    "\n",
    "prompt_df = pd.DataFrame()\n",
    "# get the file name without the extension from prompt_files, 'str' object has no attribute 'path'\n",
    "prompt_df['prompt_name'] = prompt_names\n",
    "prompt_df['prompt_txt'] = prompt_files_directory_list\n",
    "prompt_df['prompt_txt'] = prompt_df['prompt_txt'].apply(load_text)\n",
    "print(F\"Es gibt {prompt_df.shape[0]} Prompts\")\n",
    "#prompt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Berechnung der Tokenanzahl\n",
    "Todo:\n",
    "- explain how to get access to the model\n",
    "- explain how to get Hugging Face token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face\n",
    "Kann entfallen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the API key from the .env file\n",
    "# load_dotenv() \n",
    "# llama_api = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# #TO ggf. anderes Modell als Tokenizer verwenden, bspw. passend zum verwendeten Modell GPT-4o-mini\n",
    "# model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# # model_id = \"meta-llama/Llama-3.3-70B-Instruct\" # requires HugginFace Pro subscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate token count\n",
    "# def calculate_token_count(prompt):\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#     tokenized_prompt = tokenizer(prompt, return_tensors='pt') # pt for PyTorch tensors\n",
    "#     return tokenized_prompt.input_ids.size(1)\n",
    "\n",
    "# # Apply the function to the 'prompt' column and create a new column 'token_count'\n",
    "# prompt_df['token_count_hf'] = prompt_df['prompt_txt'].apply(calculate_token_count)\n",
    "\n",
    "# prompt_df = prompt_df.sort_values(by='token_count_hf')\n",
    "# prompt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenanzahl bestimmen mit Tiktoken\n",
    "Open-Source Tokenizer von OpenAI. Schneller als über AutoTokenizer von Hugging Face mit Llama 3.2-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Encoding 'o200k_base'>\n",
      "Beispieltext: This is a sample text.\n",
      "Encodierter Text (Integer): [2500, 382, 261, 10176, 2201, 13]\n",
      "Encodierter Text (Bytes): [b'This', b' is', b' a', b' sample', b' text', b'.']\n",
      "Anzahl Tokens: 6\n"
     ]
    }
   ],
   "source": [
    "model = 'gpt-4o-mini'\n",
    "\n",
    "# Beispiel zur Nachvollziehbarkeit der Tokenisierung\n",
    "encoding = tiktoken.encoding_for_model(model)\n",
    "print(encoding)\n",
    "\n",
    "sample_txt = \"This is a sample text.\"\n",
    "# Get the token count for the sample text\n",
    "token_integer = encoding.encode(sample_txt) # mit .decode() kann der Text wieder dekodiert werden. \n",
    "token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integer] # Integer Token können mit wiederum in Bytes umgewandelt werden, die sie repräsentieren.\n",
    "print(f\"Beispieltext: {sample_txt}\")\n",
    "print(f\"Encodierter Text (Integer): {token_integer}\")\n",
    "print(f\"Encodierter Text (Bytes): {token_bytes}\")\n",
    "\n",
    "\n",
    "count_tokens = num_tokens_from_string(sample_txt, model)\n",
    "print(f\"Anzahl Tokens: {count_tokens}\")\n",
    "\n",
    "# Quelle: https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>prompt_txt</th>\n",
       "      <th>token_count</th>\n",
       "      <th>max_lines_jsonl</th>\n",
       "      <th>#batches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>zero-shot</td>\n",
       "      <td>You will be given a text. Extract the argument...</td>\n",
       "      <td>82</td>\n",
       "      <td>243902.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>zero-shot-persona</td>\n",
       "      <td>You are a expert in Argument Mining and theref...</td>\n",
       "      <td>105</td>\n",
       "      <td>190476.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>zero-shot-cot</td>\n",
       "      <td>You will be given a text. Extract the argument...</td>\n",
       "      <td>480</td>\n",
       "      <td>41667.0</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>zero-shot-persona-cot</td>\n",
       "      <td>You are a expert in Argument Mining and theref...</td>\n",
       "      <td>503</td>\n",
       "      <td>39761.0</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>one-shot</td>\n",
       "      <td>You will be given a text. Extract the argument...</td>\n",
       "      <td>1780</td>\n",
       "      <td>11236.0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>one-shot-persona</td>\n",
       "      <td>You are a expert in Argument Mining and theref...</td>\n",
       "      <td>1790</td>\n",
       "      <td>11173.0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>one-shot-cot</td>\n",
       "      <td>You will be given a text. Extract the argument...</td>\n",
       "      <td>2166</td>\n",
       "      <td>9234.0</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>one-shot-persona-cot</td>\n",
       "      <td>You are a expert in Argument Mining and theref...</td>\n",
       "      <td>2189</td>\n",
       "      <td>9137.0</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>few-shot-10</td>\n",
       "      <td>You will be given a text. Extract the argument...</td>\n",
       "      <td>13848</td>\n",
       "      <td>1444.0</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>few-shot-10-persona</td>\n",
       "      <td>You are a expert in Argument Mining and theref...</td>\n",
       "      <td>13871</td>\n",
       "      <td>1442.0</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>few-shot-10-cot</td>\n",
       "      <td>You will be given a text. Extract the argument...</td>\n",
       "      <td>14247</td>\n",
       "      <td>1404.0</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>few-shot-10-persona-cot</td>\n",
       "      <td>You are a expert in Argument Mining and theref...</td>\n",
       "      <td>14270</td>\n",
       "      <td>1402.0</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>few-shot-20</td>\n",
       "      <td>You will be given a text. Extract the argument...</td>\n",
       "      <td>27681</td>\n",
       "      <td>723.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>few-shot-20-persona</td>\n",
       "      <td>You are a expert in Argument Mining and theref...</td>\n",
       "      <td>27704</td>\n",
       "      <td>722.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>few-shot-20-cot</td>\n",
       "      <td>You will be given a text. Extract the argument...</td>\n",
       "      <td>28080</td>\n",
       "      <td>712.0</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>few-shot-20-persona-cot</td>\n",
       "      <td>You are a expert in Argument Mining and theref...</td>\n",
       "      <td>28103</td>\n",
       "      <td>712.0</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>few-shot-40</td>\n",
       "      <td>You will be given a text. Extract the argument...</td>\n",
       "      <td>54048</td>\n",
       "      <td>370.0</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>few-shot-40-persona</td>\n",
       "      <td>You are a expert in Argument Mining and theref...</td>\n",
       "      <td>54071</td>\n",
       "      <td>370.0</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>few-shot-40-cot</td>\n",
       "      <td>You will be given a text. Extract the argument...</td>\n",
       "      <td>54447</td>\n",
       "      <td>367.0</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>few-shot-40-persona-cot</td>\n",
       "      <td>You are a expert in Argument Mining and theref...</td>\n",
       "      <td>54470</td>\n",
       "      <td>367.0</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                prompt_name  \\\n",
       "19                zero-shot   \n",
       "18        zero-shot-persona   \n",
       "16            zero-shot-cot   \n",
       "17    zero-shot-persona-cot   \n",
       "15                 one-shot   \n",
       "14         one-shot-persona   \n",
       "12             one-shot-cot   \n",
       "13     one-shot-persona-cot   \n",
       "3               few-shot-10   \n",
       "2       few-shot-10-persona   \n",
       "0           few-shot-10-cot   \n",
       "1   few-shot-10-persona-cot   \n",
       "7               few-shot-20   \n",
       "6       few-shot-20-persona   \n",
       "4           few-shot-20-cot   \n",
       "5   few-shot-20-persona-cot   \n",
       "11              few-shot-40   \n",
       "10      few-shot-40-persona   \n",
       "8           few-shot-40-cot   \n",
       "9   few-shot-40-persona-cot   \n",
       "\n",
       "                                           prompt_txt  token_count  \\\n",
       "19  You will be given a text. Extract the argument...           82   \n",
       "18  You are a expert in Argument Mining and theref...          105   \n",
       "16  You will be given a text. Extract the argument...          480   \n",
       "17  You are a expert in Argument Mining and theref...          503   \n",
       "15  You will be given a text. Extract the argument...         1780   \n",
       "14  You are a expert in Argument Mining and theref...         1790   \n",
       "12  You will be given a text. Extract the argument...         2166   \n",
       "13  You are a expert in Argument Mining and theref...         2189   \n",
       "3   You will be given a text. Extract the argument...        13848   \n",
       "2   You are a expert in Argument Mining and theref...        13871   \n",
       "0   You will be given a text. Extract the argument...        14247   \n",
       "1   You are a expert in Argument Mining and theref...        14270   \n",
       "7   You will be given a text. Extract the argument...        27681   \n",
       "6   You are a expert in Argument Mining and theref...        27704   \n",
       "4   You will be given a text. Extract the argument...        28080   \n",
       "5   You are a expert in Argument Mining and theref...        28103   \n",
       "11  You will be given a text. Extract the argument...        54048   \n",
       "10  You are a expert in Argument Mining and theref...        54071   \n",
       "8   You will be given a text. Extract the argument...        54447   \n",
       "9   You are a expert in Argument Mining and theref...        54470   \n",
       "\n",
       "    max_lines_jsonl  #batches  \n",
       "19         243902.0      0.00  \n",
       "18         190476.0      0.00  \n",
       "16          41667.0      0.01  \n",
       "17          39761.0      0.01  \n",
       "15          11236.0      0.03  \n",
       "14          11173.0      0.03  \n",
       "12           9234.0      0.04  \n",
       "13           9137.0      0.04  \n",
       "3            1444.0      0.25  \n",
       "2            1442.0      0.25  \n",
       "0            1404.0      0.26  \n",
       "1            1402.0      0.26  \n",
       "7             723.0      0.50  \n",
       "6             722.0      0.50  \n",
       "4             712.0      0.51  \n",
       "5             712.0      0.51  \n",
       "11            370.0      0.98  \n",
       "10            370.0      0.98  \n",
       "8             367.0      0.99  \n",
       "9             367.0      0.99  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_df['token_count'] = prompt_df['prompt_txt'].apply(num_tokens_from_string, model_name=model)\n",
    "prompt_df = prompt_df.sort_values(by='token_count')\n",
    "prompt_df['max_lines_jsonl'] = round(20_000_000 / prompt_df['token_count'], 0) # Maximale Anzahl an Tokens pro JSONL-Datei (enqueued tokens)\n",
    "prompt_df['#batches'] = round(test_df.shape[0] / prompt_df['max_lines_jsonl'], 2) # Anzahl der Batches, die für die Verarbeitung des Testdatensatzes benötigt werden\n",
    "prompt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schätzung der anfallenden Kosten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input-Token:\n",
      "Die Summe der Input-Tokenanzahl aller Prompts beträgt: 393,935 Tokens\n",
      "Multipliziert mit der Anzahl der Testdurchläufe ergibt das: 142,604,470 Tokens für Input-Token\n",
      "Das entspicht bei einer maximalen Anzahl von 20,000,000 Tokens pro JSONL-Datei: 7.13 Batches\n",
      "Die Kosten für die Input-Tokens betragen: 21.39 $\n",
      "\n",
      "Output-Token:\n",
      "Die Output-Tokenanzahl multipliziert mit der Anzahl der Aufsätze im Testdatensaatz beträgt zwischen: 108,600 und 398,200 Tokens\n",
      "Die Kosten für die Output-Tokens betragen zwischen: 0.07 $ und 0.24 $\n",
      "\n",
      "Gesamt:\n",
      "Die Gesamtkosten liegen schätzungsweise in einem Bereich von 21.46 $ und 21.63 $\n",
      "Bei der Anwendung der Batch-API gibt es einen Rabatt von 50% auf die Tokenpreise. Damit würden die Kosten zwischen 10.73 $ und 10.81 $ liegen.\n"
     ]
    }
   ],
   "source": [
    "#TODO: Hier fehlen die Token für die Essays, die zusätzlich zum Input-Tokenanzahl anfallen.\n",
    "# Schätzung der anfallenden Kosten anhand der Tokenanzahl\n",
    "prompt_token_sum = prompt_df['token_count'].sum()\n",
    "print(f\"Input-Token:\\nDie Summe der Input-Tokenanzahl aller Prompts beträgt: {prompt_token_sum:,} Tokens\")\n",
    "test_token_sum = prompt_token_sum * test_df.shape[0]\n",
    "print(f\"Multipliziert mit der Anzahl der Testdurchläufe ergibt das: {test_token_sum:,} Tokens für Input-Token\")\n",
    "max_enqueued_tokens = 20_000_000 # Maximale Anzahl an Tokens, die von der Batch API\n",
    "print(f\"Das entspicht bei einer maximalen Anzahl von {max_enqueued_tokens:,} Tokens pro JSONL-Datei: {test_token_sum/20_000_000:.2f} Batches\") \n",
    "input_token_price = 0.15 # input token price per 1 Mio tokens\n",
    "output_token_price = 0.6 # output token price per 1 Mio tokens\n",
    "input_token_cost = input_token_price * test_token_sum/1_000_000\n",
    "print(f\"Die Kosten für die Input-Tokens betragen: {input_token_cost:.2f} $\") \n",
    "#TODO: Ggf. wäre es genauer die JSON-Dateien zu verwenden\n",
    "min_output_token_count = 300 # aufgerundeter Minimalwert für Tokenanzahl für ann-Dateien aus EDA. Umfang der Ausgabe des LLMs kann auch außerhalb des Bereichs liegen. \n",
    "max_output_token_count = 1_100 # aufgerundeter Maximalwert für Tokenanzahl für ann-Dateien aus EDA\n",
    "min_output_token_sum = min_output_token_count * test_df.shape[0]\n",
    "max_output_token_sum = max_output_token_count * test_df.shape[0]\n",
    "print(f\"\\nOutput-Token:\\nDie Output-Tokenanzahl multipliziert mit der Anzahl der Aufsätze im Testdatensaatz beträgt zwischen: {min_output_token_sum:,} und {max_output_token_sum:,} Tokens\")\n",
    "min_output_token_cost = output_token_price * min_output_token_sum/1_000_000\n",
    "max_output_token_cost = output_token_price * max_output_token_sum/1_000_000\n",
    "print(f\"Die Kosten für die Output-Tokens betragen zwischen: {min_output_token_cost:.2f} $ und {max_output_token_cost:.2f} $\")\n",
    "total_cost_min = input_token_cost + min_output_token_cost\n",
    "total_cost_max = input_token_cost + max_output_token_cost\n",
    "print(f\"\\nGesamt:\\nDie Gesamtkosten liegen schätzungsweise in einem Bereich von {total_cost_min:.2f} $ und {total_cost_max:.2f} $\")\n",
    "print(f\"Bei der Anwendung der Batch-API gibt es einen Rabatt von 50% auf die Tokenpreise. Damit würden die Kosten zwischen {total_cost_min/2:.2f} $ und {total_cost_max/2:.2f} $ liegen.\")\n",
    "\n",
    "# Quelle für Tokenpreise: https://openai.com/api/pricing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem mit Llama 3.2-3B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sofern die Summe aus Input und Output Token die Grenze von 4096 Token überschreiten landet die Abfrage im folgenden Error:\n",
    "\"\"\"\n",
    "422 Client Error: Unprocessable Entity for url: https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-3B-Instruct\n",
    "\n",
    "Input validation error: `inputs` tokens + `max_new_tokens` must be <= 4096. Given: 12479 `inputs` tokens and 256 `max_new_tokens`\n",
    "\"\"\"\n",
    "\n",
    "Die Verwendung von Speicher (Memory) um die Anzahl der Tokens pro Anfrage zu reduzieren und das Kontext-Fenster des LLM auszunutzen, hat nicht funktioniert und landet im gleichen Error.\n",
    "\n",
    "```python\t\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory = ConversationBufferMemory(size=10)\n",
    "buffer = ConversationChain(llm= llm, memory=memory)\n",
    "buffer.invoke(intro_text)\n",
    "buffer.invoke(example_str_1)\n",
    "buffer.invoke(example_str_2)\n",
    "buffer.invoke(example_str_3)\n",
    "buffer.invoke(example_str_4)\n",
    "buffer.invoke(\"Text: \" test_text)\n",
    "\n",
    "buffer.get_memory()\n",
    "```\n",
    "\n",
    "Laut Forenbeiträgen ist das ein Limit von der Hugging Face API (Quelle: https://huggingface.co/spaces/huggingchat/chat-ui/discussions/430). Ein Test mit Google Collab, bei dem das Modell heruntergeladen wurde anstatt die HuggingFace API zu verwenden, hat mit 6082 Input Tokens funktioniert. \n",
    "\n",
    "\n",
    "LangChain unterstützt die anwendung der Batch API nicht. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Abfrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api = os.getenv(\"OPENAI_API_KEY\")\n",
    "                       \n",
    "client = OpenAI(api_key=openai_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strukturierte Ausgabe des LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # structured output \n",
    "# from pydantic import BaseModel, Field\n",
    "# from typing import List, Dict\n",
    "\n",
    "# class ArgumentRelation(BaseModel):\n",
    "#     \"\"\"Argumentative relation between the origin and target\"\"\"\n",
    "#     Origin: str = Field(description=\"ID of the origin (e.g.Claim or Premise)\")\n",
    "#     Relation: str = Field(description=\"Type of relation (e.g., 'For', 'Against', 'Support', 'Attack')\")\n",
    "#     Target: str = Field(description=\"ID of the target (e.g., MajorClaim, Claim or Premise)\")\n",
    "\n",
    "# class ArgumentMiningExtraction(BaseModel):\n",
    "#     \"\"\"Extraction of argument components and relations from a text\"\"\"\n",
    "#     MajorClaims: Dict[str, str] = Field(description=\"Dictionary of major claims with their IDs as keys and text as values\")\n",
    "#     Claims: Dict[str, str] = Field(description=\"Dictionary of claims with their IDs as keys and text as values\")\n",
    "#     Premises: Dict[str, str] = Field(description=\"Dictionary of premises with their IDs as keys and text as values\")\n",
    "#     ArgumentativeRelations: List[ArgumentRelation] = Field(description=\"List of Dictionaries containing the argumentative relations between origin and target\")\n",
    "\n",
    "\n",
    "# response_format = ArgumentMiningExtraction.model_json_schema()\n",
    "\n",
    "\n",
    "# response_format\n",
    "\n",
    "# landet weiterhin im error: \n",
    "# \"response\": {\"status_code\": 400, \"request_id\": \"249a9d8849ab386154c2eb12f27b0a19\", \"body\": {\"error\": {\"message\": \"Invalid schema for response_format 'ArgumentMiningExtraction': In context=(), 'additionalProperties' is required to be supplied and to be false.\", \"type\": \"invalid_request_error\"\n",
    "    \n",
    "# Quellen Structured Outputs:\n",
    "# - https://platform.openai.com/docs/guides/structured-outputs\n",
    "# - https://cookbook.openai.com/examples/structured_outputs_intro\n",
    "# - https://python.langchain.com/docs/concepts/structured_outputs/\n",
    "# - https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_format2 = {\n",
    "#     \"type\": \"object\",\n",
    "#     \"properties\": {\n",
    "#         \"MajorClaims\": {\n",
    "#             \"type\": \"object\",\n",
    "#             \"additionalProperties\": {\n",
    "#                 \"type\": \"string\"\n",
    "#             }\n",
    "#         },\n",
    "#         \"Claims\": {\n",
    "#             \"type\": \"object\",\n",
    "#             \"additionalProperties\": {\n",
    "#                 \"type\": \"string\"\n",
    "#             }\n",
    "#         },\n",
    "#         \"Premises\": {\n",
    "#             \"type\": \"object\",\n",
    "#             \"additionalProperties\": {\n",
    "#                 \"type\": \"string\"\n",
    "#             }\n",
    "#         },\n",
    "#         \"ArgumentativeRelations\": {\n",
    "#             \"type\": \"array\",\n",
    "#             \"items\": {\n",
    "#                 \"type\": \"object\",\n",
    "#                 \"properties\": {\n",
    "#                     \"Origin\": {\n",
    "#                         \"type\": \"string\"\n",
    "#                     },\n",
    "#                     \"Relation\": {\n",
    "#                         \"type\": \"string\",\n",
    "#                         \"enum\": [\"for\", \"against\", \"supports\", \"attacks\"]\n",
    "#                     },\n",
    "#                     \"Target\": {\n",
    "#                         \"type\": \"string\"\n",
    "#                     }\n",
    "#                 },\n",
    "#                 \"required\": [\"Origin\", \"Relation\", \"Target\"],\n",
    "#                 \"additionalProperties\": False\n",
    "#             }\n",
    "#         }\n",
    "#     },\n",
    "#     \"required\": [\"MajorClaims\", \"Claims\", \"Premises\", \"ArgumentativeRelations\"],\n",
    "#     \"additionalProperties\": False\n",
    "# }\n",
    "\n",
    "# response_format2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai.lib._pydantic import to_strict_json_schema\n",
    "\n",
    "# response_format = to_strict_json_schema(ArgumentMiningExtraction)\n",
    "# response_format\n",
    "\n",
    "# Ansatz von: https://community.openai.com/t/structured-outputs-with-batch-processing/911076/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## standard-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_seed = 123\n",
    "# # Llama via HuggingFaceAPI\n",
    "# # max_new_tokens = 1024  # standard 512. Orientiert an der Tokenanzahl der JSON-Dateien (Ground-Truth) \n",
    "# # llm = HuggingFaceEndpoint(repo_id=model_id,\n",
    "# #                           huggingfacehub_api_token=llama_api,\n",
    "# #                           max_new_tokens=max_new_tokens,\n",
    "# #                           max_input_tokens=1024,\n",
    "# #                           #top_k=, # standard None\n",
    "# #                           #top_p=, # standard 0.95\n",
    "# #                           temperature=0.1, # standard 0.8\n",
    "# #                           )\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"gpt-4o-mini\",\n",
    "#     #max_tokens=1024,\n",
    "#     #max_tokens_input=1024,\n",
    "#     # timeout=None,\n",
    "#     # max_retries=2,\n",
    "#     api_key=openai_api,\n",
    "#     temperature=0,\n",
    "#     seed=llm_seed,\n",
    "#     # system_fingerprint will be returned in the response\n",
    "#     model_kwargs={\"response_format\": ArgumentMiningExtraction}\n",
    "# )\n",
    "\n",
    "# Quelle Verwendung OpenAI via LangChain: https://python.langchain.com/docs/integrations/chat/openai/\n",
    "# Quelle Reproduzierbarkeit von LLM-Ausgaben: https://cookbook.openai.com/examples/reproducible_outputs_with_the_seed_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_input_tokens = 4096 - max_new_tokens\n",
    "# print(f\"Der Input darf die Tokenanzahl von {max_input_tokens} Token nicht überschreiten.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template\n",
    "# prompt_template = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", \"{system_message}\"),\n",
    "#         (\"user\", \"Text: {argument_text}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # output_parser = StrOutputParser() # turns the output into a string \n",
    "\n",
    "# # combine the prompt template, llm and output parser\n",
    "# llm_chain = prompt_template | llm #| output_parser\n",
    "\n",
    "# # invoke the chain\n",
    "# one_shot_answer = llm_chain.invoke({\"system_message\": one_shot,\n",
    "#                            \"argument_text\": test_df_sample[0]})\n",
    "# print(one_shot_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# das würde über die normale API gehen, aber nicht über die Batch-API\n",
    "\n",
    "# # dataframe to store the input and output of the llm chain\n",
    "# results_df = pd.DataFrame()\n",
    "\n",
    "# # iterate over the zero-shot prompt dataframe\n",
    "# for _, prompt_row in zero_shot_df.iterrows():\n",
    "#     # iterate over the  dataframe with the test data\n",
    "#     for _, test_df_row in test_df.iterrows():\n",
    "#         # invoke the chain\n",
    "#         try: \n",
    "#             answer = llm_chain.invoke({\"system_message\": prompt_row['prompt'],\n",
    "#                                        \"argument_text\": test_df_row['txt']})\n",
    "#             # store the input and output in the dataframe\n",
    "#             new_row = pd.DataFrame({'prompt_file': [prompt_row['prompt_file']],\n",
    "#                                     'txt_file': [test_df_row['txt_file']],\n",
    "#                                     'json_file': [test_df_row['json_file']],\n",
    "#                                     'ground_truth': [test_df_row['json']],\n",
    "#                                     'answer': [answer]\n",
    "#                                     })\n",
    "#             results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "#         except Exception as e: # catch errors like HTTPError, HfHubHTTPError\n",
    "#             new_row = pd.DataFrame({'prompt_file': [prompt_row['prompt_file']],\n",
    "#                                     'txt_file': [test_df_row['txt_file']],\n",
    "#                                     'json_file': [test_df_row['json_file']],\n",
    "#                                     'ground_truth': [test_df_row['json']],\n",
    "#                                     'answer': e})\n",
    "#             results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "#         print(f\"Finished {test_df_row['txt_file']} with prompt {prompt_row['prompt_file']}\")\n",
    "\n",
    "# # save the results to a csv file\n",
    "# results_df.to_csv('results.csv', header=True index=False)\n",
    "\n",
    "\n",
    "# # iterate over the one-shot prompt dataframe\n",
    "# for _, prompt_row in one_shot_df.iterrows():\n",
    "#     # iterate over the  dataframe with the test data\n",
    "#     for _, test_df_row in test_df.iterrows():\n",
    "#         # invoke the chain\n",
    "#         try: \n",
    "#             answer = llm_chain.invoke({\"system_message\": prompt_row['prompt'],\n",
    "#                                        \"argument_text\": test_df_row['txt']})\n",
    "#             # store the input and output in the dataframe\n",
    "#             new_row = pd.DataFrame({'prompt_file': [prompt_row['prompt_file']],\n",
    "#                                     'txt_file': [test_df_row['txt_file']],\n",
    "#                                     'json_file': [test_df_row['json_file']],\n",
    "#                                     'ground_truth': [test_df_row['json']],\n",
    "#                                     'answer': [answer]\n",
    "#                                     })\n",
    "#             results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "#         except Exception as e: # catch errors like HTTPError, HfHubHTTPError\n",
    "#             new_row = pd.DataFrame({'prompt_file': [prompt_row['prompt_file']],\n",
    "#                                     'txt_file': [test_df_row['txt_file']],\n",
    "#                                     'json_file': [test_df_row['json_file']],\n",
    "#                                     'ground_truth': [test_df_row['json']],\n",
    "#                                     'answer': e})\n",
    "#             results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "#         print(f\"Finished {test_df_row['txt_file']} with prompt {prompt_row['prompt_file']}\")\n",
    "\n",
    "# # append the results to the csv file\n",
    "# results_df.to_csv('results.csv', mode='a', header=False, index=False) # mode='a' for append\n",
    "\n",
    "\n",
    "\n",
    "# # Create a batch api request for the openai api to reduce costs\n",
    "# {custom_id: 'request_1', prompt: 'prompt_1', text: 'text_1'},\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test buffer memory und sequential chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sequential chain prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains import SequentialChain\n",
    "# from langchain.chains import LLMChain\n",
    "\n",
    "# template = \"\"\"\n",
    "# You will be given a text. Extract the argumentative units “major claim”, “claim”, and “premise” as parts from the text. Also extract the argumentative relationships between the units. Claims can be “for” or “against” the major claims. Premises, on the other hand, can “support” or “attack” a claim or another premise. It is possible that there are several major claims. Return only the argumentative units and relationships between them. Return as a JSON object.\n",
    "\n",
    "# # Example\n",
    "# ## Input:\\nShould students be required to attend classes?\\n\\nThe issue at hand is whether it should be obligatory for university students to attend classes. This is an interesting question because it affects a great amount of students worldwide. Nonetheless there are various different policies regarding this topic. An important aspect might be whether one desires to optimize the learned knowledge or the amount of valuable experiences for the students. However, to my mind students should be free not to attend classes because it improves the quality of student life as well as their learning motivation and teaches important life skills.\\nFirstly, a liberal policy is very feasible. Checking for attendance requires a lot of bureaucracy. Especially for larger classes it is impossible to check for everybody to attend. So an optional attendance saves time and money.\\nSecondly, some students might learn better at home on their own, for instance, by reading the textbook. This problem occurs especially if the lecturer is lethargic. By letting students choose not to attend class you give them the opportunity to escape bad teaching. Thus they are able to save precious study time and dive into the course syllabus independently.\\nIn addition, being free to stay away from classes improves flexibility and therefore quality of student life. Sometimes the wild party on Thursday night is too good to end already at midnight only because of a lecture on Friday in the morning. With a liberal policy students are able to postpone the learning to the afternoon which gives a feeling of freedom and improves time efficiency. Research has shown that the more satisfied the students are with those life aspects, the better they perform in academic areas.\\nFinally, psychology knows two types of motivation. There is intrinsic motivation which comes from your own mindset. And there is extrinsic motivation which comes from the praise and laud of other people. Intrinsic motivation is known to be much more desirable because it leads to better learning and well-being. However, in order to gain intrinsic motivation students need to become aware of their strengths and aims. By giving students the freedom to choose about class attendance they might rather be thinking about why they decided to study and learn to motivate themselves. These are crucial skills for the duration of their study and their whole life time.\\nTo conclude, it is clear that going to classes should be optional for students. I hold this belief due to the improvement of students current experience as well as the valuable skills and knowledge they obtain for their whole life afterwards.\n",
    "# ## Output:\\n{{\\n  'MajorClaims': {{\\n    'MC1': 'students should be free not to attend classes',\\n    'MC2': 'it is clear that going to classes should be optional for students'\\n  }},\\n  'Claims': {{\\n    'C1': 'it improves the quality of student life as well as their learning motivation and teaches important life skills',\\n    'C2': 'I hold this belief due to the improvement of students\\' current experience as well as the valuable skills and knowledge they obtain for their whole life afterwards',\\n    'C3': 'some students might learn better at home on their own, for instance, by reading the textbook',\\n    'C4': 'being free to stay away from classes improves flexibility and therefore quality of student life',\\n    'C5': 'By giving students the freedom to choose about class attendance they might rather be thinking about why they decided to study and learn to motivate themselves'\\n  }},\\n  'Premises': {{\\n    'P1': 'This problem occurs especially if the lecturer is lethargic',\\n    'P2': 'By letting students choose not to attend class you give them the opportunity to escape bad teaching',\\n    'P3': 'they are able to save precious study time and dive into the course syllabus independently',\\n    'P4': 'Sometimes the wild party on Thursday night is too good to end already at midnight only because of a lecture on Friday in the morning',\\n    'P5': 'With a liberal policy students are able to postpone the learning to the afternoon which gives a feeling of freedom and improves time efficiency',\\n    'P6': 'Research has shown that the more satisfied the students are with those life aspects, the better they perform in academic areas',\\n    'P7': 'Intrinsic motivation is known to be much more desirable because it leads to better learning and well-being',\\n    'P8': 'These are crucial skills for the duration of their study and their whole life time',\\n    'P9': 'in order to gain intrinsic motivation students need to become aware of their strengths and aims'\\n  }},\\n  'ArgumentativeRelations': [\\n    {{\\n      'Claim': 'C1',\\n      'Relation': 'For',\\n      'Target': 'MC'\\n    }},\\n    {{\\n      'Claim': 'C2',\\n      'Relation': 'For',\\n      'Target': 'MC'\\n    }},\\n    {{\\n      'Claim': 'C3',\\n      'Relation': 'For',\\n      'Target': 'MC'\\n    }},\\n    {{\\n      'Claim': 'P2',\\n      'Relation': 'supports',\\n      'Target': 'P3'\\n    }},\\n    {{\\n      'Claim': 'P3',\\n      'Relation': 'supports',\\n      'Target': 'C3'\\n    }},\\n    {{\\n      'Claim': 'P1',\\n      'Relation': 'supports',\\n      'Target': 'C3'\\n    }},\\n    {{\\n      'Claim': 'C4',\\n      'Relation': 'For',\\n      'Target': 'MC'\\n    }},\\n    {{\\n      'Claim': 'P4',\\n      'Relation': 'supports',\\n      'Target': 'C4'\\n    }},\\n    {{\\n      'Claim': 'P5',\\n      'Relation': 'supports',\\n      'Target': 'C4'\\n    }},\\n    {{\\n      'Claim': 'P6',\\n      'Relation': 'supports',\\n      'Target': 'C4'\\n    }},\\n    {{\\n      'Claim': 'C5',\\n      'Relation': 'For',\\n      'Target': 'MC'\\n    }},\\n    {{\\n      'Claim': 'P9',\\n      'Relation': 'supports',\\n      'Target': 'C5'\\n    }},\\n    {{\\n      'Claim': 'P7',\\n      'Relation': 'supports',\\n      'Target': 'C5'\\n    }},\\n    {{\\n      'Claim': 'P8',\\n      'Relation': 'supports',\\n      'Target': 'C5'\\n    }}\\n  ]\\n}}\n",
    "\n",
    "# Text: {{Text}}\n",
    "# \"\"\"\n",
    "# template = template.replace('{', '{{').replace('}', '}}')\n",
    "\n",
    "# prompt_template = PromptTemplate(template=template, input_variables=['Text'])\n",
    "\n",
    "# first_chain = prompt_template | llm | output_parser\n",
    "\n",
    "# template2 = \"\"\"{input}\n",
    "# Refine this output by looking at these examples: \"\"\" + sample_str.replace('{', '{{').replace('}', '}}')\n",
    "\n",
    "# prompt_template2 = PromptTemplate(template=template2, input_variables=['input'])\n",
    "# second_chain = prompt_template2 | llm | output_parser \n",
    "\n",
    "# # invoke the chain\n",
    "# first_chain_answer = first_chain.invoke({\"Text\": test_df_sample['txt'][25]})\n",
    "# print(first_chain_answer)\n",
    "\n",
    "# second_chain_answer = second_chain.invoke({\"input\": first_chain_answer})\n",
    "# print(second_chain_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### buffer memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intro_text = task_description + \" You will be given a some examples of text input and the corresponding JSON output. Wait with your answer until you will be given a text to analyze.\"\n",
    "# #intro_text\n",
    "# example_text = \"Here is an example: \"\n",
    "# example_str_1, example_str_2, example_str_3, example_str_4, example_str_5, example_str_6, example_str_7, example_str_8, example_str_9, example_str_10 = examples\n",
    "# print(example_str_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.memory import ConversationBufferMemory\n",
    "# from langchain.chains import ConversationChain\n",
    "\n",
    "# memory = ConversationBufferMemory(size=10)\n",
    "# buffer = ConversationChain(llm= llm, memory=memory)\n",
    "# buffer.invoke(intro_text)\n",
    "# buffer.invoke(example_text + example_str_1)\n",
    "# buffer.invoke(example_text + example_str_2)\n",
    "# buffer.invoke(example_text + example_str_3)\n",
    "# buffer.invoke(example_text + example_str_4)\n",
    "# # buffer.invoke(example_text + example_str_5)\n",
    "# #buffer.invoke(\"Text: \" + test_df_sample['txt'][25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Text: \" + test_df_sample['txt'][25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buffer.memory.chat_memory.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer_chain = prompt_template | buffer | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch API anfragen \n",
    "Langchain hat bisher noch keine Möglichkeit die Batch API von OpenAI zu verwenden, sondern nur über die Standard API. Dies geht unter anderem aus dem folgenden Forenbeitrag hervor (Stand 03.01.24): https://github.com/langchain-ai/langchain/discussions/21643\n",
    "\n",
    "Um die Kosten der Anfragen weiter zu reduzieren wurde der ursprüngliche Ansatz über LangChain verworfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              prompt_name  token_count  #batches\n",
      "19              zero-shot           82      0.00\n",
      "18      zero-shot-persona          105      0.00\n",
      "16          zero-shot-cot          480      0.01\n",
      "17  zero-shot-persona-cot          503      0.01\n",
      "\n",
      "             prompt_name  token_count  #batches\n",
      "15              one-shot         1780      0.03\n",
      "14      one-shot-persona         1790      0.03\n",
      "12          one-shot-cot         2166      0.04\n",
      "13  one-shot-persona-cot         2189      0.04\n",
      "\n",
      "               prompt_name  token_count  #batches\n",
      "3              few-shot-10        13848      0.25\n",
      "2      few-shot-10-persona        13871      0.25\n",
      "0          few-shot-10-cot        14247      0.26\n",
      "1  few-shot-10-persona-cot        14270      0.26\n",
      "\n",
      "               prompt_name  token_count  #batches\n",
      "7              few-shot-20        27681      0.50\n",
      "6      few-shot-20-persona        27704      0.50\n",
      "4          few-shot-20-cot        28080      0.51\n",
      "5  few-shot-20-persona-cot        28103      0.51\n",
      "\n",
      "                prompt_name  token_count  #batches\n",
      "11              few-shot-40        54048      0.98\n",
      "10      few-shot-40-persona        54071      0.98\n",
      "8           few-shot-40-cot        54447      0.99\n",
      "9   few-shot-40-persona-cot        54470      0.99\n"
     ]
    }
   ],
   "source": [
    "# Ppromp_df in 4 Teile aufteilen, damit die Prompts nacheinander an das Modell übergeben werden können.\n",
    "# Für den Fall, dass es zu Fehlermeldungen kommen sollte, bspw. aufgrund Tokenanzahl, muss man so nicht von vorne beginnen und reduziert eventuell anfallende Mehrkosten.\n",
    "zs_prompt_df = prompt_df[prompt_df['prompt_name'].str.contains('zero-shot')]\n",
    "os_prompt_df = prompt_df[prompt_df['prompt_name'].str.contains('one-shot')]\n",
    "fs10_prompt_df = prompt_df[prompt_df['prompt_name'].str.contains('few-shot-10')]\n",
    "fs20_prompt_df = prompt_df[prompt_df['prompt_name'].str.contains('few-shot-20')]\n",
    "fs40_prompt_df = prompt_df[prompt_df['prompt_name'].str.contains('few-shot-40')]\n",
    "\n",
    "print(f\"{zs_prompt_df[['prompt_name', 'token_count', '#batches']]}\")\n",
    "print(f\"\\n{os_prompt_df[['prompt_name', 'token_count', '#batches']]}\")\n",
    "print(f\"\\n{fs10_prompt_df[['prompt_name', 'token_count', '#batches']]}\")\n",
    "print(f\"\\n{fs20_prompt_df[['prompt_name', 'token_count', '#batches']]}\")\n",
    "print(f\"\\n{fs40_prompt_df[['prompt_name', 'token_count', '#batches']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es gibt insgesamt 7240 Kombinationen, die verarbeitet werden müssen.\n",
      "Davon entfallen 1448 auf Zero-Shot-Prompts.\n",
      "Davon entfallen 1448 auf One-Shot-Prompts.\n",
      "Davon entfallen 1448 auf Few-Shot-Prompts mit 10 Beispielen.\n",
      "Davon entfallen 1448 auf Few-Shot-Prompts mit 20 Beispielen.\n",
      "Davon entfallen 1448 auf Few-Shot-Prompts mit 40 Beispielen.\n"
     ]
    }
   ],
   "source": [
    "# count rows in the dataframes\n",
    "zs_rows = zs_prompt_df.shape[0]\n",
    "os_rows = os_prompt_df.shape[0]\n",
    "fs10_rows = fs10_prompt_df.shape[0]\n",
    "fs20_rows = fs20_prompt_df.shape[0]\n",
    "fs40_rows = fs40_prompt_df.shape[0]\n",
    "\n",
    "# calculate the amount of combinations to be processed if all prompts are used for all test essays\n",
    "combinations = test_df.shape[0] * (zs_rows + os_rows + fs10_rows + fs20_rows + fs40_rows)\n",
    "print(f\"Es gibt insgesamt {combinations} Kombinationen, die verarbeitet werden müssen.\")\n",
    "print(f\"Davon entfallen {test_df.shape[0] * zs_rows} auf Zero-Shot-Prompts.\")\n",
    "print(f\"Davon entfallen {test_df.shape[0] * os_rows} auf One-Shot-Prompts.\")\n",
    "print(f\"Davon entfallen {test_df.shape[0] * fs10_rows} auf Few-Shot-Prompts mit 10 Beispielen.\")\n",
    "print(f\"Davon entfallen {test_df.shape[0] * fs20_rows} auf Few-Shot-Prompts mit 20 Beispielen.\")\n",
    "print(f\"Davon entfallen {test_df.shape[0] * fs40_rows} auf Few-Shot-Prompts mit 40 Beispielen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot-Prompts: 0.02 Batches mit 1170 Tokens\n",
      "One-Shot-Prompts: 0.14 Batches mit 7925 Tokens\n",
      "Few-Shot-Prompts mit 10 Beispielen: 1.02 Batches mit 56236 Tokens\n",
      "Few-Shot-Prompts mit 20 Beispielen: 2.02 Batches mit 111568 Tokens\n",
      "Few-Shot-Prompts mit 40 Beispielen: 3.94 Batches mit 217036 Tokens\n"
     ]
    }
   ],
   "source": [
    "# Berechnung des Tokenverbrauchs pro Batch. Das limit des verwendeten Clients liegt bei 2,000,000 enqueued tokens\n",
    "zs_prompt_df_sum = zs_prompt_df['token_count'].sum()\n",
    "os_prompt_df_sum = os_prompt_df['token_count'].sum()\n",
    "fs10_df_sum = fs10_prompt_df['token_count'].sum()\n",
    "fs20_df_sum = fs20_prompt_df['token_count'].sum()\n",
    "fs40_df_sum = fs40_prompt_df['token_count'].sum()\n",
    "requests = 1448 # Anzahl der Anfragen, die in einem Batch verarbeitet werden\n",
    "\n",
    "# Anzahl der Batch-Anfragen\n",
    "zs_batches = round(zs_prompt_df['#batches'].sum(), 2)\n",
    "os_batches = round(os_prompt_df['#batches'].sum(), 2)\n",
    "fs10_batches = round(fs10_prompt_df['#batches'].sum(), 2)\n",
    "fs20_batches = round(fs20_prompt_df['#batches'].sum(), 2)\n",
    "fs40_batches = round(fs40_prompt_df['#batches'].sum(), 2)\n",
    "\n",
    "print(f\"Zero-Shot-Prompts: {zs_batches} Batches mit {zs_prompt_df_sum} Tokens\")\n",
    "print(f\"One-Shot-Prompts: {os_batches} Batches mit {os_prompt_df_sum} Tokens\")\n",
    "print(f\"Few-Shot-Prompts mit 10 Beispielen: {fs10_batches} Batches mit {fs10_df_sum} Tokens\")\n",
    "print(f\"Few-Shot-Prompts mit 20 Beispielen: {fs20_batches} Batches mit {fs20_df_sum} Tokens\")\n",
    "print(f\"Few-Shot-Prompts mit 40 Beispielen: {fs40_batches} Batches mit {fs40_df_sum} Tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of #batches for zero-shot prompts: 0.02\n",
      "Sum of #batches for one-shot prompts: 0.14\n",
      "Sum of #batches for few-shot prompts with 10 examples: 1.020000\n",
      "Sum of #batches for few-shot prompts with 20 examples: 2.020000\n",
      "Sum of #batches for few-shot prompts with 40 examples: 3.940000\n"
     ]
    }
   ],
   "source": [
    "# Group by prompt_name and calculate the sum of #batches\n",
    "zero_shot_batches_sum = prompt_df[prompt_df['prompt_name'].str.contains('zero-shot')]['#batches'].sum()\n",
    "one_shot_batches_sum = prompt_df[prompt_df['prompt_name'].str.contains('one-shot')]['#batches'].sum()\n",
    "few_shot_10_batches_sum = prompt_df[prompt_df['prompt_name'].str.contains('few-shot-10')]['#batches'].sum()\n",
    "few_shot_20_batches_sum = prompt_df[prompt_df['prompt_name'].str.contains('few-shot-20')]['#batches'].sum()\n",
    "few_shot_40_batches_sum = prompt_df[prompt_df['prompt_name'].str.contains('few-shot-40')]['#batches'].sum()\n",
    "\n",
    "print(f\"Sum of #batches for zero-shot prompts: {zero_shot_batches_sum}\")\n",
    "print(f\"Sum of #batches for one-shot prompts: {one_shot_batches_sum}\")\n",
    "print(f\"Sum of #batches for few-shot prompts with 10 examples: {few_shot_10_batches_sum:2f}\")\n",
    "print(f\"Sum of #batches for few-shot prompts with 20 examples: {few_shot_20_batches_sum:2f}\")\n",
    "print(f\"Sum of #batches for few-shot prompts with 40 examples: {few_shot_40_batches_sum:2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZS: 0.0\n",
      "OS: 0.0\n",
      "FS10: 1.2\n",
      "FS20: 2.0\n",
      "FS40: 4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ben-s\\AppData\\Local\\Temp\\ipykernel_8048\\2003250429.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  zs_prompt_df['#lines_jsonl'] = round(20_000_000 / zs_prompt_df['token_count'], 0)\n",
      "C:\\Users\\ben-s\\AppData\\Local\\Temp\\ipykernel_8048\\2003250429.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  zs_prompt_df['#batches'] = round(test_df.shape[0] / zs_prompt_df['#lines_jsonl'], 1)\n",
      "C:\\Users\\ben-s\\AppData\\Local\\Temp\\ipykernel_8048\\2003250429.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  os_prompt_df['#lines_jsonl'] = round(20_000_000 / os_prompt_df['token_count'], 0)\n",
      "C:\\Users\\ben-s\\AppData\\Local\\Temp\\ipykernel_8048\\2003250429.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  os_prompt_df['#batches'] = round(test_df.shape[0] / os_prompt_df['#lines_jsonl'], 1)\n",
      "C:\\Users\\ben-s\\AppData\\Local\\Temp\\ipykernel_8048\\2003250429.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fs10_prompt_df['#lines_jsonl'] = round(20_000_000 / fs10_prompt_df['token_count'], 0)\n",
      "C:\\Users\\ben-s\\AppData\\Local\\Temp\\ipykernel_8048\\2003250429.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fs10_prompt_df['#batches'] = round(test_df.shape[0] / fs10_prompt_df['#lines_jsonl'], 1)\n",
      "C:\\Users\\ben-s\\AppData\\Local\\Temp\\ipykernel_8048\\2003250429.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fs20_prompt_df['#lines_jsonl'] = round(20_000_000 / fs20_prompt_df['token_count'], 0)\n",
      "C:\\Users\\ben-s\\AppData\\Local\\Temp\\ipykernel_8048\\2003250429.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fs20_prompt_df['#batches'] = round(test_df.shape[0] / fs20_prompt_df['#lines_jsonl'], 1)\n",
      "C:\\Users\\ben-s\\AppData\\Local\\Temp\\ipykernel_8048\\2003250429.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fs40_prompt_df['#lines_jsonl'] = round(20_000_000 / fs40_prompt_df['token_count'], 0)\n",
      "C:\\Users\\ben-s\\AppData\\Local\\Temp\\ipykernel_8048\\2003250429.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  fs40_prompt_df['#batches'] = round(test_df.shape[0] / fs40_prompt_df['#lines_jsonl'], 0)\n"
     ]
    }
   ],
   "source": [
    "zs_prompt_df['#lines_jsonl'] = round(20_000_000 / zs_prompt_df['token_count'], 0)\n",
    "zs_prompt_df['#batches'] = round(test_df.shape[0] / zs_prompt_df['#lines_jsonl'], 1)\n",
    "print(f\"ZS: {zs_prompt_df['#batches'].sum()}\")\n",
    "\n",
    "os_prompt_df['#lines_jsonl'] = round(20_000_000 / os_prompt_df['token_count'], 0)\n",
    "os_prompt_df['#batches'] = round(test_df.shape[0] / os_prompt_df['#lines_jsonl'], 1)\n",
    "print(f\"OS: {os_prompt_df['#batches'].sum()}\")\n",
    "\n",
    "fs10_prompt_df['#lines_jsonl'] = round(20_000_000 / fs10_prompt_df['token_count'], 0)\n",
    "fs10_prompt_df['#batches'] = round(test_df.shape[0] / fs10_prompt_df['#lines_jsonl'], 1)\n",
    "print(f\"FS10: {fs10_prompt_df['#batches'].sum()}\")\n",
    "\n",
    "fs20_prompt_df['#lines_jsonl'] = round(20_000_000 / fs20_prompt_df['token_count'], 0)\n",
    "fs20_prompt_df['#batches'] = round(test_df.shape[0] / fs20_prompt_df['#lines_jsonl'], 1)\n",
    "print(f\"FS20: {fs20_prompt_df['#batches'].sum()}\")\n",
    "\n",
    "fs40_prompt_df['#lines_jsonl'] = round(20_000_000 / fs40_prompt_df['token_count'], 0)\n",
    "fs40_prompt_df['#batches'] = round(test_df.shape[0] / fs40_prompt_df['#lines_jsonl'], 0)\n",
    "print(f\"FS40: {fs40_prompt_df['#batches'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_format = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"MajorClaims\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"ID\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"Text\": {\n",
    "                        \"type\": \"string\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"ID\", \"Text\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        },\n",
    "        \"Claims\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"ID\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"Text\": {\n",
    "                        \"type\": \"string\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"ID\", \"Text\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        },\n",
    "        \"Premises\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"ID\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"Text\": {\n",
    "                        \"type\": \"string\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"ID\", \"Text\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        },\n",
    "        \"ArgumentativeRelations\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"Origin\": {\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"Relation\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"for\", \"against\", \"supports\", \"attacks\"]\n",
    "                    },\n",
    "                    \"Target\": {\n",
    "                        \"type\": \"string\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"Origin\", \"Relation\", \"Target\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"MajorClaims\", \"Claims\", \"Premises\", \"ArgumentativeRelations\"],\n",
    "    \"additionalProperties\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch API Input Dateien erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_input(test_df: pd.DataFrame, prompt_df: pd.DataFrame, file_name: str):\n",
    "    temperature = 0\n",
    "    llm_seed = 123\n",
    "    model = \"gpt-4o-mini\"\n",
    "\n",
    "    # response format for structured output\n",
    "    # response_format = {\n",
    "    # \"type\": \"object\",\n",
    "    # \"properties\": {\n",
    "    #     \"MajorClaims\": {\n",
    "    #         \"type\": \"array\",\n",
    "    #         \"items\": {\n",
    "    #             \"type\": \"object\",\n",
    "    #             \"properties\": {\n",
    "    #                 \"ID\": {\n",
    "    #                     \"type\": \"string\"\n",
    "    #                 },\n",
    "    #                 \"Text\": {\n",
    "    #                     \"type\": \"string\"\n",
    "    #                 }\n",
    "    #             },\n",
    "    #             \"required\": [\"ID\", \"Text\"],\n",
    "    #             \"additionalProperties\": False\n",
    "    #         }\n",
    "    #     },\n",
    "    #     \"Claims\": {\n",
    "    #         \"type\": \"array\",\n",
    "    #         \"items\": {\n",
    "    #             \"type\": \"object\",\n",
    "    #             \"properties\": {\n",
    "    #                 \"ID\": {\n",
    "    #                     \"type\": \"string\"\n",
    "    #                 },\n",
    "    #                 \"Text\": {\n",
    "    #                     \"type\": \"string\"\n",
    "    #                 }\n",
    "    #             },\n",
    "    #             \"required\": [\"ID\", \"Text\"],\n",
    "    #             \"additionalProperties\": False\n",
    "    #         }\n",
    "    #     },\n",
    "    #     \"Premises\": {\n",
    "    #         \"type\": \"array\",\n",
    "    #         \"items\": {\n",
    "    #             \"type\": \"object\",\n",
    "    #             \"properties\": {\n",
    "    #                 \"ID\": {\n",
    "    #                     \"type\": \"string\"\n",
    "    #                 },\n",
    "    #                 \"Text\": {\n",
    "    #                     \"type\": \"string\"\n",
    "    #                 }\n",
    "    #             },\n",
    "    #             \"required\": [\"ID\", \"Text\"],\n",
    "    #             \"additionalProperties\": False\n",
    "    #         }\n",
    "    #     },\n",
    "    #     \"ArgumentativeRelations\": {\n",
    "    #         \"type\": \"array\",\n",
    "    #         \"items\": {\n",
    "    #             \"type\": \"object\",\n",
    "    #             \"properties\": {\n",
    "    #                 \"Origin\": {\n",
    "    #                     \"type\": \"string\"\n",
    "    #                 },\n",
    "    #                 \"Relation\": {\n",
    "    #                     \"type\": \"string\",\n",
    "    #                     \"enum\": [\"for\", \"against\", \"supports\", \"attacks\"]\n",
    "    #                 },\n",
    "    #                 \"Target\": {\n",
    "    #                     \"type\": \"string\"\n",
    "    #                 }\n",
    "    #             },\n",
    "    #             \"required\": [\"Origin\", \"Relation\", \"Target\"],\n",
    "    #             \"additionalProperties\": False\n",
    "    #         }\n",
    "    #     }\n",
    "    # },\n",
    "    # \"required\": [\"MajorClaims\", \"Claims\", \"Premises\", \"ArgumentativeRelations\"],\n",
    "    # \"additionalProperties\": False\n",
    "    # }\n",
    "\n",
    "    dict_list = []\n",
    "    # iteration über Zero-Shot-Prompts\n",
    "    for _, prompt_row in prompt_df.iterrows():\n",
    "        # Iteration über Testdaten\n",
    "        for _, test_df_row in test_df.iterrows():\n",
    "            custom_id_str = prompt_row['prompt_name'] + \"_\" + test_df_row['txt_file']# + \"_\" + str(id_counter)\n",
    "            # write batch input for jsonl file\n",
    "            input_dict = {\"custom_id\": custom_id_str, \n",
    "                          \"method\": \"POST\", \"url\": \"/v1/chat/completions\",\n",
    "                          \"body\": {\"model\": model,\n",
    "                                   \"messages\": [{\"role\": \"developer\", \"content\": prompt_row['prompt_txt']}, # system Rolle wurde in developer umbenannt\n",
    "                                                {\"role\": \"user\", \"content\": \"Text: \" + test_df_row['txt']}], # user Rolle für Eingaben des Nutzers wie bei ChatGPT \n",
    "                                                \"temperature\": temperature,\n",
    "                                                \"seed\": llm_seed,\n",
    "                                                \"response_format\": {\n",
    "                                                    \"type\": \"json_schema\", # wichtig festzulegen, da sonst Fehlermeldung\n",
    "                                                    \"json_schema\": {\n",
    "                                                        \"name\": \"ArgumentMiningExtraction\", # wichtig festzulegen, da sonst Fehlermeldung\n",
    "                                                        \"schema\": response_format,\n",
    "                                                        \"strict\": True \n",
    "                                                    }\n",
    "                                                    }\n",
    "                                                }\n",
    "                                     }\n",
    "            dict_list.append(input_dict)\n",
    "\n",
    "    jsonl_output = \"\\n\".join(json.dumps(item) for item in dict_list)\n",
    "\n",
    "    # Output in JSONL-Datei schreiben\n",
    "    with open(\"batch_api/input/\" + file_name + \".jsonl\", 'w') as f:\n",
    "        f.write(jsonl_output)\n",
    "\n",
    "    return jsonl_output\n",
    "\n",
    "# Quelle Batch API: https://platform.openai.com/docs/guides/batch?lang=python\n",
    "# Quelle text generation: https://platform.openai.com/docs/guides/text-generation\n",
    "\n",
    "# Quellen Structured Outputs:\n",
    "# - https://platform.openai.com/docs/guides/structured-outputs\n",
    "# - https://cookbook.openai.com/examples/structured_outputs_intro\n",
    "# - https://python.langchain.com/docs/concepts/structured_outputs/\n",
    "# - https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_input_split(test_df: pd.DataFrame, prompt_df: pd.DataFrame, file_name: str, num_files: int):\n",
    "    if num_files <= 0:\n",
    "        raise ValueError(\"num_files muss größer als 0 sein.\")\n",
    "\n",
    "    temperature = 0\n",
    "    llm_seed = 123\n",
    "    model = \"gpt-4o-mini\"\n",
    "\n",
    "    # response format for structured output\n",
    "    # response_format = {\n",
    "    # \"type\": \"object\",\n",
    "    # \"properties\": {\n",
    "    #     \"MajorClaims\": {\n",
    "    #         \"type\": \"array\",\n",
    "    #         \"items\": {\n",
    "    #             \"type\": \"object\",\n",
    "    #             \"properties\": {\n",
    "    #                 \"ID\": {\n",
    "    #                     \"type\": \"string\"\n",
    "    #                 },\n",
    "    #                 \"Text\": {\n",
    "    #                     \"type\": \"string\"\n",
    "    #                 }\n",
    "    #             },\n",
    "    #             \"required\": [\"ID\", \"Text\"],\n",
    "    #             \"additionalProperties\": False\n",
    "    #         }\n",
    "    #     },\n",
    "    #     \"Claims\": {\n",
    "    #         \"type\": \"array\",\n",
    "    #         \"items\": {\n",
    "    #             \"type\": \"object\",\n",
    "    #             \"properties\": {\n",
    "    #                 \"ID\": {\n",
    "    #                     \"type\": \"string\"\n",
    "    #                 },\n",
    "    #                 \"Text\": {\n",
    "    #                     \"type\": \"string\"\n",
    "    #                 }\n",
    "    #             },\n",
    "    #             \"required\": [\"ID\", \"Text\"],\n",
    "    #             \"additionalProperties\": False\n",
    "    #         }\n",
    "    #     },\n",
    "    #     \"Premises\": {\n",
    "    #         \"type\": \"array\",\n",
    "    #         \"items\": {\n",
    "    #             \"type\": \"object\",\n",
    "    #             \"properties\": {\n",
    "    #                 \"ID\": {\n",
    "    #                     \"type\": \"string\"\n",
    "    #                 },\n",
    "    #                 \"Text\": {\n",
    "    #                     \"type\": \"string\"\n",
    "    #                 }\n",
    "    #             },\n",
    "    #             \"required\": [\"ID\", \"Text\"],\n",
    "    #             \"additionalProperties\": False\n",
    "    #         }\n",
    "    #     },\n",
    "    #     \"ArgumentativeRelations\": {\n",
    "    #         \"type\": \"array\",\n",
    "    #         \"items\": {\n",
    "    #             \"type\": \"object\",\n",
    "    #             \"properties\": {\n",
    "    #                 \"Origin\": {\n",
    "    #                     \"type\": \"string\"\n",
    "    #                 },\n",
    "    #                 \"Relation\": {\n",
    "    #                     \"type\": \"string\",\n",
    "    #                     \"enum\": [\"for\", \"against\", \"supports\", \"attacks\"]\n",
    "    #                 },\n",
    "    #                 \"Target\": {\n",
    "    #                     \"type\": \"string\"\n",
    "    #                 }\n",
    "    #             },\n",
    "    #             \"required\": [\"Origin\", \"Relation\", \"Target\"],\n",
    "    #             \"additionalProperties\": False\n",
    "    #         }\n",
    "    #     }\n",
    "    # },\n",
    "    # \"required\": [\"MajorClaims\", \"Claims\", \"Premises\", \"ArgumentativeRelations\"],\n",
    "    # \"additionalProperties\": False\n",
    "    # }\n",
    "\n",
    "    dict_list = []\n",
    "    # iteration über Zero-Shot-Prompts\n",
    "    for _, prompt_row in prompt_df.iterrows():\n",
    "        # Iteration über Testdaten\n",
    "        for _, test_df_row in test_df.iterrows():\n",
    "            custom_id_str = prompt_row['prompt_name'] + \"_\" + test_df_row['txt_file']# + \"_\" + str(id_counter)\n",
    "            # write batch input for jsonl file\n",
    "            input_dict = {\"custom_id\": custom_id_str, \n",
    "                          \"method\": \"POST\", \"url\": \"/v1/chat/completions\",\n",
    "                          \"body\": {\"model\": model,\n",
    "                                   \"messages\": [{\"role\": \"developer\", \"content\": prompt_row['prompt_txt']}, # system Rolle wurde in developer umbenannt\n",
    "                                                {\"role\": \"user\", \"content\": \"Text: \" + test_df_row['txt']}], # user Rolle für Eingaben des Nutzers wie bei ChatGPT \n",
    "                                                \"temperature\": temperature,\n",
    "                                                \"seed\": llm_seed,\n",
    "                                                \"response_format\": {\n",
    "                                                    \"type\": \"json_schema\", # wichtig festzulegen, da sonst Fehlermeldung\n",
    "                                                    \"json_schema\": {\n",
    "                                                        \"name\": \"ArgumentMiningExtraction\", # wichtig festzulegen, da sonst Fehlermeldung\n",
    "                                                        \"schema\": response_format, # strukturiertes Output-Format von oben\n",
    "                                                        \"strict\": True \n",
    "                                                    }\n",
    "                                                    }\n",
    "                                                }\n",
    "                                     }\n",
    "            dict_list.append(input_dict)\n",
    "\n",
    "    \n",
    "    chunk_size = len(dict_list) // num_files + (len(dict_list) % num_files > 0) # Ermittelt die größe der Chunks, indem die Anzahl der Elemente durch die Anzahl der Dateien geteilt wird. Bei einem Rest wird ein Chunk mehr erstellt.\n",
    "    # Floor-Operator \"//\" dividiert und rundet auf die nächste ganze Zahl ab (Bsp.: 7 / 2 = 3.5 . 7 // 2 = 3). Modulo Operator \"%\" gibt den Rest der Division an. Wenn \n",
    "    chunks = [dict_list[i:i + chunk_size] for i in range(0, len(dict_list), chunk_size)] # Teilt die Liste in gleich große Teile auf. \n",
    "    # Beispiel für 7 Elemente (dict_list) und 2 Dateien (num_files): chunk_size = 7 // 2 + (7 % 2 > 0) = 4. cunks = [dict_list[0:4], dict_list[4:7]]. Intervall [0:4] = 0, 1, 2, 3. Intervall [4:7] = 4, 5, 6. Der letzte Index wird nicht mit eingeschlossen. \n",
    "\n",
    "    # Output in JSONL-Dateien schreiben\n",
    "    for i, chunk in enumerate(chunks): # enumerate iteriert über die Chunks-Liste und gibt den Index und das Element zurück\n",
    "        jsonl_output = \"\\n\".join(json.dumps(item) for item in chunk) # Schreibt die Elemente in einzelne Zeilen in einen JSONL-String\n",
    "        with open(f\"batch_api/input/{file_name}_{i + 1}.jsonl\", 'w') as f: # Schreibt den JSONL-String in eine Datei mit dem übergebenen Dateinamen und der fortlaufender Nummerierung\n",
    "            f.write(jsonl_output)\n",
    "\n",
    "    return [f\"{file_name}_{i + 1}.jsonl\" for i in range(num_files)] # Bezeichnet die Dateien fortlaufend anhand der Anzahl der Dateien\n",
    "\n",
    "\n",
    "# Quelle Batch API: https://platform.openai.com/docs/guides/batch?lang=python\n",
    "# Quelle text generation: https://platform.openai.com/docs/guides/text-generation\n",
    "\n",
    "# Quellen Structured Outputs:\n",
    "# - https://platform.openai.com/docs/guides/structured-outputs\n",
    "# - https://cookbook.openai.com/examples/structured_outputs_intro\n",
    "# - https://python.langchain.com/docs/concepts/structured_outputs/\n",
    "# - https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3], [4, 5, 6, 7]]\n",
      "Output 1:\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Output 2:\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "list = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "chunk_size = 4\n",
    "chunks = [list[i:i + chunk_size] for i in range(0, len(list), chunk_size)]\n",
    "print(chunks)\n",
    "\n",
    "for i , chunk in enumerate(chunks):\n",
    "    output = \"\\n\".join(str(item) for item in chunk)\n",
    "    print(f\"Output {i + 1}:\\n{output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test splitting full json file into smaller files using token limit of 20 Mio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_jsonl = generate_batch_input(test_df, prompt_df, \"full_batch_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_INPUT_PATH = \"batch_api/input/\"\n",
    "BATCH_OUTPUT_PATH = \"batch_api/output/\"\n",
    "\n",
    "def split_jsonl_file(input_file, max_tokens=20_000_000):\n",
    "    \n",
    "    def write_to_file(data, file_index):\n",
    "        with open(f\"{BATCH_INPUT_PATH}batch_input_{file_index}.jsonl\", 'w') as f:\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "    current_tokens = 0\n",
    "    file_index = 1\n",
    "    current_data = []\n",
    "\n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f:\n",
    "            json_obj = json.loads(line)\n",
    "            messages = json_obj.get(\"body\", {}).get(\"messages\", []) # get messages from body inside json_obj\n",
    "            messages_str = json.dumps(messages)\n",
    "            tokens = num_tokens_from_string(messages_str, model_name=\"gpt-4o-mini\") # calculate tokens from messages\n",
    "\n",
    "            if current_tokens + tokens > max_tokens: # wenn die Anzahl der bisherigen Tokens plus die Tokens des aktuellen Objekts größer als das Limit ist\n",
    "                write_to_file(current_data, file_index) # schreibe die Daten in die Datei\n",
    "                file_index += 1 # erhöhe den Dateiindex\n",
    "                current_data = [] # setze die aktuellen Daten zurück\n",
    "                current_tokens = 0 # setze die aktuellen Tokens zurück\n",
    "            \n",
    "            # wenn die Anzahl der bisherigen Tokens plus die Tokens des aktuellen Objekts kleiner als das Limit ist. Mit einem Else-Statement würde das letzte Objekt nicht in die Datei geschrieben werden.\n",
    "            current_data.append(json_obj) # füge das aktuelle Objekt zu den aktuellen Daten hinzu\n",
    "            current_tokens += tokens # erhöhe die Anzahl der aktuellen Tokens um die Tokens des aktuellen Objekts\n",
    "\n",
    "    if current_data: # wenn es nach der Iteration noch Daten gibt, die noch nicht in eine Datei geschrieben wurden\n",
    "        write_to_file(current_data, file_index) # schreibe die Daten in die Datei\n",
    "\n",
    "# Example usage\n",
    "split_jsonl_file(\"batch_api/input/full_batch_input.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7240, 3816, 878, 622, 549, 323, 322, 320, 320, 90]\n"
     ]
    }
   ],
   "source": [
    "# count lines of all imput_batch files\n",
    "BATCH_INPUT_PATH = \"batch_api/input/\"\n",
    "input_files = ['full_batch_input.jsonl', 'batch_input_1.jsonl', 'batch_input_2.jsonl', 'batch_input_3.jsonl', 'batch_input_4.jsonl', 'batch_input_5.jsonl', 'batch_input_6.jsonl', 'batch_input_7.jsonl', 'batch_input_8.jsonl','batch_input_9.jsonl']\n",
    "line_counts = [sum(1 for line in open(BATCH_INPUT_PATH + file)) for file in input_files]\n",
    "print(line_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7240\n"
     ]
    }
   ],
   "source": [
    "count=[3816, 878, 622, 549, 323, 322, 320, 320, 90]\n",
    "print(sum(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['few-shot-10_1.jsonl',\n",
       " 'few-shot-10_2.jsonl',\n",
       " 'few-shot-20_1.jsonl',\n",
       " 'few-shot-20_2.jsonl',\n",
       " 'few-shot-20_3.jsonl',\n",
       " 'few-shot-40_1.jsonl',\n",
       " 'few-shot-40_2.jsonl',\n",
       " 'few-shot-40_3.jsonl',\n",
       " 'few-shot-40_4.jsonl',\n",
       " 'few-shot-40_5.jsonl',\n",
       " 'one-shot.jsonl',\n",
       " 'zero-shot.jsonl']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input_files_list = list_files(BATCH_INPUT_PATH)\n",
    "batch_input_files_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches hochladen und erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_INPUT_PATH = \"batch_api/input/\"\n",
    "BATCH_OUTPUT_PATH = \"batch_api/output/\"\n",
    "\n",
    "# Hilfsfunktionen für die Batch-API\n",
    "def upload_batch_file(filepath):\n",
    "    response = client.files.create(\n",
    "        file=open(filepath, 'rb'),\n",
    "        purpose='batch'\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def create_batch(input_file_id, metadata_dict):\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata=metadata_dict\n",
    "    )\n",
    "    return batch\n",
    "\n",
    "# Check the status of the batch\n",
    "def check_batch_status(batch_id):\n",
    "    batch = client.batches.retrieve(batch_id)\n",
    "    print(f\"Status: {batch.status}\")\n",
    "\n",
    "    if batch.status == \"failed\":\n",
    "        print(f\"Error: {batch.errors}\")\n",
    "    elif batch.status == \"in_progress\" or batch.status == \"validating\" or batch.status == \"finalizing\":\n",
    "        print(\"Der Batch wird noch verarbeitet. Bitte warten und später erneut prüfen.\")\n",
    "\n",
    "    print(f\"\\nBeschreibung des Batches: {batch.metadata[\"description\"]}\")\n",
    "    print(f\"Anfragen gesamt: {batch.request_counts.total}\")\n",
    "    print(f\"Davon erfolgreich: {batch.request_counts.completed}\")\n",
    "    print(f\"Davon fehlerhaft: {batch.request_counts.failed}\")\n",
    "    if batch.output_file_id is not None:\n",
    "        print(f\"Erfolgreiche Abfragen können abgerufen werden mit ID: {batch.output_file_id}\")\n",
    "    else:\n",
    "        print(\"Keine erfolgreichen Abfragen zum herunterladen vorhanden.\")\n",
    "    \n",
    "    if batch.error_file_id is not None:\n",
    "        print(f\"Für weiter Informationen zum Fehler Abfrage an Error-File mit ID: {batch.error_file_id}\\n\")\n",
    "    else:\n",
    "        print(\"Keine fehlerhaften Abfragen zum herunterladen vorhanden.\\n\")\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# retrieving the results\n",
    "def retrieve_and_save_batch_results(batch_file_id, file_name):\n",
    "    file_response = client.files.content(batch_file_id)\n",
    "    results = file_response.text\n",
    "    with open(BATCH_OUTPUT_PATH + file_name + \".jsonl\", 'w') as f:\n",
    "        f.write(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-73X2eNMUnxhwLndGXvNdZ5', bytes=84798929, created_at=1736407855, filename='batch_input_1.jsonl', object='file', purpose='batch', status='processed', status_details=None)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uploading der Batch Input-Datei auf die OpenAI-Plattform\n",
    "batch_file_1 = upload_batch_file(BATCH_INPUT_PATH + \"batch_input_1.jsonl\")\n",
    "batch_file_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_677f7b3b1b48819086a5a27b7173c2c3', completion_window='24h', created_at=1736407867, endpoint='/v1/chat/completions', input_file_id='file-73X2eNMUnxhwLndGXvNdZ5', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1736494267, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Batch 1/9 for Argument Mining'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Batches\n",
    "metadata_dict = {\"description\": \"Batch 1/9 for Argument Mining\"}\n",
    "batch_1 = create_batch(batch_file_1.id, metadata_dict) # sofern die Batch Datei bereits hochgeladen wurde, aber nicht erfolgreich war, kann die Batch-ID erneut verwendet werden. Ein erneuter Uplaod ist nicht notwendig und würde der Datei eine neue ID zuweisen.\n",
    "print(batch_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: completed\n",
      "\n",
      "Beschreibung des Batches: Batch 1/9 for Argument Mining\n",
      "Anfragen gesamt: 3816\n",
      "Davon erfolgreich: 3816\n",
      "Davon fehlerhaft: 0\n",
      "Erfolgreiche Abfragen können abgerufen werden mit ID: file-Bwq5PPm4rnvL2jk7KFt2cL\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_677f7b3b1b48819086a5a27b7173c2c3', completion_window='24h', created_at=1736407867, endpoint='/v1/chat/completions', input_file_id='file-73X2eNMUnxhwLndGXvNdZ5', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736411765, error_file_id=None, errors=None, expired_at=None, expires_at=1736494267, failed_at=None, finalizing_at=1736411356, in_progress_at=1736407871, metadata={'description': 'Batch 1/9 for Argument Mining'}, output_file_id='file-Bwq5PPm4rnvL2jk7KFt2cL', request_counts=BatchRequestCounts(completed=3816, failed=0, total=3816))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "# Die Bearbeitung des Batches kann bis zu 24 Stunden dauern, funktioniert aber in der Regel schneller.\n",
    "batch_1_status = check_batch_status(batch_1.id)\n",
    "print(batch_1_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"batch_req_677f88dcb8c881908e9275650fab34fa\", \"custom_id\": \"zero-shot_essay001.txt\", \"response\": {\"status_code\": 200, \"request_id\": \"82330270e662bb47722cf42d0c0ab869\", \"body\": {\"id\": \"chatcmpl-AnhIbjUujngIZN41Ogf87fCAje0x7\", \"object\": \"chat.completion\", \"created\": 1736407877, \"model\": \"gpt-4o-mini-2024-07-18\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"{\\\"MajorClaims\\\":[{\\\"ID\\\":\\\"MC1\\\",\\\"Text\\\":\\\"We should attach more importance to cooperation during primary education.\\\"}],\\\"Claims\\\":[{\\\"ID\\\":\\\"C1\\\",\\\"Text\\\":\\\"Competition can effectively promote the development of economy.\\\"},{\\\"ID\\\":\\\"C2\\\",\\\"Text\\\":\\\"Cooperation helps children learn interpersonal skills.\\\"},{\\\"ID\\\":\\\"C3\\\",\\\"Text\\\":\\\"Competition makes society more effective.\\\"},{\\\"ID\\\":\\\"C4\\\",\\\"Text\\\":\\\"Victory in competition often requires cooperation.\\\"}],\\\"Premises\\\":[{\\\"ID\\\":\\\"P1\\\",\\\"Text\\\":\\\"Companies improve their products and services to survive in competition.\\\"},{\\\"ID\\\":\\\"P2\\\",\\\"Text\\\":\\\"Inter\n"
     ]
    }
   ],
   "source": [
    "# Abrufen und Speichern der Batch-Ergebnisse (Output-Datei)\n",
    "batch_1_results = retrieve_and_save_batch_results(batch_1_status.output_file_id, \"output-batch-1\")\n",
    "print(batch_1_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-NyvUx7xYfyFu6C4QNZ6qYs', bytes=80171245, created_at=1736498896, filename='batch_input_2.jsonl', object='file', purpose='batch', status='processed', status_details=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uploading der Batch Input-Datei auf die OpenAI-Plattform\n",
    "batch_file_2 = upload_batch_file(BATCH_INPUT_PATH + \"batch_input_2.jsonl\")\n",
    "batch_file_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_6780ded3cd58819080b492f6647492b4', completion_window='24h', created_at=1736498899, endpoint='/v1/chat/completions', input_file_id='file-NyvUx7xYfyFu6C4QNZ6qYs', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1736585299, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Batch 2/9 for Argument Mining'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Batches\n",
    "metadata_dict = {\"description\": \"Batch 2/9 for Argument Mining\"}\n",
    "batch_2 = create_batch(batch_file_2.id, metadata_dict) # sofern die Batch Datei bereits hochgeladen wurde, aber nicht erfolgreich war, kann die Batch-ID erneut verwendet werden. Ein erneuter Uplaod ist nicht notwendig und würde der Datei eine neue ID zuweisen.\n",
    "print(batch_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: completed\n",
      "\n",
      "Beschreibung des Batches: Batch 2/9 for Argument Mining\n",
      "Anfragen gesamt: 878\n",
      "Davon erfolgreich: 878\n",
      "Davon fehlerhaft: 0\n",
      "Erfolgreiche Abfragen können abgerufen werden mit ID: file-Fg7gsDHnWPhPKX5BLzQhaH\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_6780ded3cd58819080b492f6647492b4', completion_window='24h', created_at=1736498899, endpoint='/v1/chat/completions', input_file_id='file-NyvUx7xYfyFu6C4QNZ6qYs', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736499712, error_file_id=None, errors=None, expired_at=None, expires_at=1736585299, failed_at=None, finalizing_at=1736499609, in_progress_at=1736498902, metadata={'description': 'Batch 2/9 for Argument Mining'}, output_file_id='file-Fg7gsDHnWPhPKX5BLzQhaH', request_counts=BatchRequestCounts(completed=878, failed=0, total=878))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "batch_2_status = check_batch_status(batch_2.id)\n",
    "print(batch_2_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"batch_req_6780e1bbdd408190b622fa829f21b366\", \"custom_id\": \"few-shot-10-cot_essay220.txt\", \"response\": {\"status_code\": 200, \"request_id\": \"7efa009d49bf99540b29b3fb45a5b4d6\", \"body\": {\"id\": \"chatcmpl-Ao4yrOhe57gU4dz9cROmIgRKN2NNL\", \"object\": \"chat.completion\", \"created\": 1736498909, \"model\": \"gpt-4o-mini-2024-07-18\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"{\\n  \\\"MajorClaims\\\": [\\n    {\\n      \\\"ID\\\": \\\"MC1\\\",\\n      \\\"Text\\\": \\\"Learning to be independent is essential for young adults\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"MC2\\\",\\n      \\\"Text\\\": \\\"staying longer with parents is a better choice\\\"\\n    }\\n  ],\\n  \\\"Claims\\\": [\\n    {\\n      \\\"ID\\\": \\\"C1\\\",\\n      \\\"Text\\\": \\\"staying with the parents for longer time does more benefits than disadvantages to the young adult\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C2\\\",\\n      \\\"Text\\\": \\\"the young adult can have more experience with his parents\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C3\\\",\\n      \\\"Text\\\": \\\"living at ho\n"
     ]
    }
   ],
   "source": [
    "# Abrufen und Speichern der Batch-Ergebnisse (Output-Datei)\n",
    "batch_2_results = retrieve_and_save_batch_results(batch_2_status.output_file_id, \"output-batch-2\")\n",
    "print(batch_2_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-1VeM9QWs9utTGbREGjvauV', bytes=79782274, created_at=1736585156, filename='batch_input_3.jsonl', object='file', purpose='batch', status='processed', status_details=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uploading der Batch Input-Datei auf die OpenAI-Plattform\n",
    "batch_file_3 = upload_batch_file(BATCH_INPUT_PATH + \"batch_input_3.jsonl\")\n",
    "batch_file_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_67822fc8058c8190b864f42b92ad646d', completion_window='24h', created_at=1736585160, endpoint='/v1/chat/completions', input_file_id='file-1VeM9QWs9utTGbREGjvauV', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1736671560, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Batch 3/9 for Argument Mining'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Batches\n",
    "metadata_dict = {\"description\": \"Batch 3/9 for Argument Mining\"}\n",
    "batch_3 = create_batch(batch_file_3.id, metadata_dict) # sofern die Batch Datei bereits hochgeladen wurde, aber nicht erfolgreich war, kann die Batch-ID erneut verwendet werden. Ein erneuter Uplaod ist nicht notwendig und würde der Datei eine neue ID zuweisen.\n",
    "print(batch_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: completed\n",
      "\n",
      "Beschreibung des Batches: Batch 3/9 for Argument Mining\n",
      "Anfragen gesamt: 622\n",
      "Davon erfolgreich: 622\n",
      "Davon fehlerhaft: 0\n",
      "Erfolgreiche Abfragen können abgerufen werden mit ID: file-Dn8EPeVM9BjT1VpRaaAnvS\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_67822fc8058c8190b864f42b92ad646d', completion_window='24h', created_at=1736585160, endpoint='/v1/chat/completions', input_file_id='file-1VeM9QWs9utTGbREGjvauV', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736586694, error_file_id=None, errors=None, expired_at=None, expires_at=1736671560, failed_at=None, finalizing_at=1736586509, in_progress_at=1736585162, metadata={'description': 'Batch 3/9 for Argument Mining'}, output_file_id='file-Dn8EPeVM9BjT1VpRaaAnvS', request_counts=BatchRequestCounts(completed=622, failed=0, total=622))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "batch_3_status = check_batch_status(batch_3.id)\n",
    "print(batch_3_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"batch_req_6782350f50148190ae1ecbe23a044ce3\", \"custom_id\": \"few-shot-20_essay389.txt\", \"response\": {\"status_code\": 200, \"request_id\": \"429b27845229d782546ed0cbc4fb17c3\", \"body\": {\"id\": \"chatcmpl-AoRVz57qqhjznstY4shTiOqFnQRQC\", \"object\": \"chat.completion\", \"created\": 1736585531, \"model\": \"gpt-4o-mini-2024-07-18\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"{\\n  \\\"MajorClaims\\\": [\\n    {\\n      \\\"ID\\\": \\\"MC1\\\",\\n      \\\"Text\\\": \\\"education plays an important role in the socioeconomic status of a country\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"MC2\\\",\\n      \\\"Text\\\": \\\"education is the single most important factor in the development of a country\\\"\\n    }\\n  ],\\n  \\\"Claims\\\": [\\n    {\\n      \\\"ID\\\": \\\"C1\\\",\\n      \\\"Text\\\": \\\"education is undeniably an economic necessity\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C2\\\",\\n      \\\"Text\\\": \\\"not many can afford to send their children to school in a developing country\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C3\\\",\\n      \\\"Text\\\": \\\"\n"
     ]
    }
   ],
   "source": [
    "# Abrufen und Speichern der Batch-Ergebnisse (Output-Datei)\n",
    "batch_3_results = retrieve_and_save_batch_results(batch_3_status.output_file_id, \"output-batch-3\")\n",
    "print(batch_3_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-CUxcAUKiZJZ64KkV8YAsG4', bytes=79467621, created_at=1736670809, filename='batch_input_4.jsonl', object='file', purpose='batch', status='processed', status_details=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uploading der Batch Input-Datei auf die OpenAI-Plattform\n",
    "batch_file_4 = upload_batch_file(BATCH_INPUT_PATH + \"batch_input_4.jsonl\")\n",
    "batch_file_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_67837fec4f04819088f396053ef790f2', completion_window='24h', created_at=1736671212, endpoint='/v1/chat/completions', input_file_id='file-CUxcAUKiZJZ64KkV8YAsG4', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1736757612, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Batch 4/9 for Argument Mining'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Batches\n",
    "metadata_dict = {\"description\": \"Batch 4/9 for Argument Mining\"}\n",
    "batch_4 = create_batch(batch_file_4.id, metadata_dict) # sofern die Batch Datei bereits hochgeladen wurde, aber nicht erfolgreich war, kann die Batch-ID erneut verwendet werden. Ein erneuter Uplaod ist nicht notwendig und würde der Datei eine neue ID zuweisen.\n",
    "print(batch_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: completed\n",
      "\n",
      "Beschreibung des Batches: Batch 4/9 for Argument Mining\n",
      "Anfragen gesamt: 549\n",
      "Davon erfolgreich: 549\n",
      "Davon fehlerhaft: 0\n",
      "Erfolgreiche Abfragen können abgerufen werden mit ID: file-CTPtvsw1e79hupNn1R1DXf\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_67837fec4f04819088f396053ef790f2', completion_window='24h', created_at=1736671212, endpoint='/v1/chat/completions', input_file_id='file-CUxcAUKiZJZ64KkV8YAsG4', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736673370, error_file_id=None, errors=None, expired_at=None, expires_at=1736757612, failed_at=None, finalizing_at=1736673312, in_progress_at=1736671215, metadata={'description': 'Batch 4/9 for Argument Mining'}, output_file_id='file-CTPtvsw1e79hupNn1R1DXf', request_counts=BatchRequestCounts(completed=549, failed=0, total=549))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "batch_4_status = check_batch_status(batch_4.id)\n",
    "print(batch_4_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"batch_req_67838820cb908190bb1db05670129112\", \"custom_id\": \"few-shot-20-cot_essay276.txt\", \"response\": {\"status_code\": 200, \"request_id\": \"856774c31c28b8df1dfb7b6a6a981df5\", \"body\": {\"id\": \"chatcmpl-AonoLHzczRc9vHXpuohv76KsztCIX\", \"object\": \"chat.completion\", \"created\": 1736671237, \"model\": \"gpt-4o-mini-2024-07-18\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"{\\n  \\\"MajorClaims\\\": [\\n    {\\n      \\\"ID\\\": \\\"MC1\\\",\\n      \\\"Text\\\": \\\"dancing is an important part of culture\\\"\\n    }\\n  ],\\n  \\\"Claims\\\": [\\n    {\\n      \\\"ID\\\": \\\"C1\\\",\\n      \\\"Text\\\": \\\"dancing are significant part of culture that could show to something that people believe\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C2\\\",\\n      \\\"Text\\\": \\\"dancing can represent to civilization of that culture\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C3\\\",\\n      \\\"Text\\\": \\\"dancing is one of the ways people entertain themselves\\\"\\n    }\\n  ],\\n  \\\"Premises\\\": [\\n    {\\n      \\\"ID\\\": \\\"P1\\\",\\n      \\\"Text\\\": \\\"some cultur\n"
     ]
    }
   ],
   "source": [
    "# Abrufen und Speichern der Batch-Ergebnisse (Output-Datei)\n",
    "batch_4_results = retrieve_and_save_batch_results(batch_4_status.output_file_id, \"output-batch-4\")\n",
    "print(batch_4_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-BAg7ZagvUiaPgNUeoPjxVc', bytes=79286180, created_at=1736757463, filename='batch_input_5.jsonl', object='file', purpose='batch', status='processed', status_details=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uploading der Batch Input-Datei auf die OpenAI-Plattform\n",
    "batch_file_5 = upload_batch_file(BATCH_INPUT_PATH + \"batch_input_5.jsonl\")\n",
    "batch_file_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_6784d168bb288190bda2b11e3b42da3c', completion_window='24h', created_at=1736757608, endpoint='/v1/chat/completions', input_file_id='file-BAg7ZagvUiaPgNUeoPjxVc', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1736844008, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Batch 5/9 for Argument Mining'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Batches\n",
    "metadata_dict = {\"description\": \"Batch 5/9 for Argument Mining\"}\n",
    "batch_5 = create_batch(batch_file_5.id, metadata_dict) # sofern die Batch Datei bereits hochgeladen wurde, aber nicht erfolgreich war, kann die Batch-ID erneut verwendet werden. Ein erneuter Uplaod ist nicht notwendig und würde der Datei eine neue ID zuweisen.\n",
    "print(batch_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: completed\n",
      "\n",
      "Beschreibung des Batches: Batch 5/9 for Argument Mining\n",
      "Anfragen gesamt: 323\n",
      "Davon erfolgreich: 323\n",
      "Davon fehlerhaft: 0\n",
      "Erfolgreiche Abfragen können abgerufen werden mit ID: file-XCidZa5LAvqwrL52vCSaS2\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_6784d168bb288190bda2b11e3b42da3c', completion_window='24h', created_at=1736757608, endpoint='/v1/chat/completions', input_file_id='file-BAg7ZagvUiaPgNUeoPjxVc', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736791620, error_file_id=None, errors=None, expired_at=None, expires_at=1736844008, failed_at=None, finalizing_at=1736791592, in_progress_at=1736757613, metadata={'description': 'Batch 5/9 for Argument Mining'}, output_file_id='file-XCidZa5LAvqwrL52vCSaS2', request_counts=BatchRequestCounts(completed=323, failed=0, total=323))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "batch_5_status = check_batch_status(batch_5.id)\n",
    "print(batch_5_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"batch_req_6785562970d48190890e1b3db0550e4e\", \"custom_id\": \"few-shot-40_essay081.txt\", \"response\": {\"status_code\": 200, \"request_id\": \"961d841d6ad27aec5dccc2ac0ef053e5\", \"body\": {\"id\": \"chatcmpl-ApJ1kDVOSD1DoKECkc3fctxXCi9xt\", \"object\": \"chat.completion\", \"created\": 1736791232, \"model\": \"gpt-4o-mini-2024-07-18\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"{\\n  \\\"MajorClaims\\\": [\\n    {\\n      \\\"ID\\\": \\\"MC1\\\",\\n      \\\"Text\\\": \\\"artists must be given freedom so that they will produce some really marvelous masterpiece\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"MC2\\\",\\n      \\\"Text\\\": \\\"there should not be any restrictions on artists' work\\\"\\n    }\\n  ],\\n  \\\"Claims\\\": [\\n    {\\n      \\\"ID\\\": \\\"C1\\\",\\n      \\\"Text\\\": \\\"if there is control over artists' ideas, they will definitely lose their sense of creativity in the long run\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C2\\\",\\n      \\\"Text\\\": \\\"it is every human's right to be able to voice out their opinions in any ways as lo\n"
     ]
    }
   ],
   "source": [
    "# Abrufen und Speichern der Batch-Ergebnisse (Output-Datei)\n",
    "batch_5_results = retrieve_and_save_batch_results(batch_5_status.output_file_id, \"output-batch-5\")\n",
    "print(batch_5_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-58BfmzR7AzjHXBQKiRdeTG', bytes=79088105, created_at=1736788702, filename='batch_input_6.jsonl', object='file', purpose='batch', status='processed', status_details=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uploading der Batch Input-Datei auf die OpenAI-Plattform\n",
    "batch_file_6 = upload_batch_file(BATCH_INPUT_PATH + \"batch_input_6.jsonl\")\n",
    "batch_file_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_67860a74a03481909107e2a89f10a187', completion_window='24h', created_at=1736837748, endpoint='/v1/chat/completions', input_file_id='file-58BfmzR7AzjHXBQKiRdeTG', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1736924148, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Batch 6/9 for Argument Mining'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Batches\n",
    "metadata_dict = {\"description\": \"Batch 6/9 for Argument Mining\"}\n",
    "batch_6 = create_batch(batch_file_6.id, metadata_dict) # sofern die Batch Datei bereits hochgeladen wurde, aber nicht erfolgreich war, kann die Batch-ID erneut verwendet werden. Ein erneuter Uplaod ist nicht notwendig und würde der Datei eine neue ID zuweisen.\n",
    "print(batch_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: completed\n",
      "\n",
      "Beschreibung des Batches: Batch 6/9 for Argument Mining\n",
      "Anfragen gesamt: 322\n",
      "Davon erfolgreich: 322\n",
      "Davon fehlerhaft: 0\n",
      "Erfolgreiche Abfragen können abgerufen werden mit ID: file-11jo846QQ1pmGXxp5JDNyZ\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_67860a74a03481909107e2a89f10a187', completion_window='24h', created_at=1736837748, endpoint='/v1/chat/completions', input_file_id='file-58BfmzR7AzjHXBQKiRdeTG', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736839559, error_file_id=None, errors=None, expired_at=None, expires_at=1736924148, failed_at=None, finalizing_at=1736839526, in_progress_at=1736837752, metadata={'description': 'Batch 6/9 for Argument Mining'}, output_file_id='file-11jo846QQ1pmGXxp5JDNyZ', request_counts=BatchRequestCounts(completed=322, failed=0, total=322))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "batch_6_status = check_batch_status(batch_6.id)\n",
    "print(batch_6_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"batch_req_6786116710148190b712ba1d6fd42154\", \"custom_id\": \"few-shot-40-persona_essay037.txt\", \"response\": {\"status_code\": 200, \"request_id\": \"8dddee82cd827ecaaa9339f9b31859c8\", \"body\": {\"id\": \"chatcmpl-ApV8ul7jazbBrLlmszvK1SEYlAgly\", \"object\": \"chat.completion\", \"created\": 1736837804, \"model\": \"gpt-4o-mini-2024-07-18\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"{\\n  \\\"MajorClaims\\\": [\\n    {\\n      \\\"ID\\\": \\\"MC1\\\",\\n      \\\"Text\\\": \\\"international sporting occasions are essential in easing international tensions\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"MC2\\\",\\n      \\\"Text\\\": \\\"International sporting events will make the world more peaceful\\\"\\n    }\\n  ],\\n  \\\"Claims\\\": [\\n    {\\n      \\\"ID\\\": \\\"C1\\\",\\n      \\\"Text\\\": \\\"international sporting events are a good change to create a multi-nation community of fans having the same passion\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C2\\\",\\n      \\\"Text\\\": \\\"people around the world understand each other more\\\"\\n    },\\n    {\\\n"
     ]
    }
   ],
   "source": [
    "# Abrufen und Speichern der Batch-Ergebnisse (Output-Datei)\n",
    "batch_6_results = retrieve_and_save_batch_results(batch_6_status.output_file_id, \"output-batch-6\")\n",
    "print(batch_6_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-FVN2b1jfp2RS5JKC9ng7Em', bytes=79095032, created_at=1736924736, filename='batch_input_7.jsonl', object='file', purpose='batch', status='processed', status_details=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uploading der Batch Input-Datei auf die OpenAI-Plattform\n",
    "batch_file_7 = upload_batch_file(BATCH_INPUT_PATH + \"batch_input_7.jsonl\")\n",
    "batch_file_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_67875e439c0081909378f77faa6010ce', completion_window='24h', created_at=1736924739, endpoint='/v1/chat/completions', input_file_id='file-FVN2b1jfp2RS5JKC9ng7Em', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1737011139, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Batch 7/9 for Argument Mining'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Erstellen eines Batches\n",
    "metadata_dict = {\"description\": \"Batch 7/9 for Argument Mining\"}\n",
    "batch_7 = create_batch(batch_file_7.id, metadata_dict) # sofern die Batch Datei bereits hochgeladen wurde, aber nicht erfolgreich war, kann die Batch-ID erneut verwendet werden. Ein erneuter Uplaod ist nicht notwendig und würde der Datei eine neue ID zuweisen.\n",
    "print(batch_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: completed\n",
      "\n",
      "Beschreibung des Batches: Batch 7/9 for Argument Mining\n",
      "Anfragen gesamt: 320\n",
      "Davon erfolgreich: 320\n",
      "Davon fehlerhaft: 0\n",
      "Erfolgreiche Abfragen können abgerufen werden mit ID: file-SntzBR9MciREe9ZepXmZ6o\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_67875e439c0081909378f77faa6010ce', completion_window='24h', created_at=1736924739, endpoint='/v1/chat/completions', input_file_id='file-FVN2b1jfp2RS5JKC9ng7Em', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736925178, error_file_id=None, errors=None, expired_at=None, expires_at=1737011139, failed_at=None, finalizing_at=1736925160, in_progress_at=1736924742, metadata={'description': 'Batch 7/9 for Argument Mining'}, output_file_id='file-SntzBR9MciREe9ZepXmZ6o', request_counts=BatchRequestCounts(completed=320, failed=0, total=320))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "batch_7_status = check_batch_status(batch_7.id)\n",
    "print(batch_7_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"batch_req_67875fe848308190b163df1129df2b14\", \"custom_id\": \"few-shot-40-persona_essay395.txt\", \"response\": {\"status_code\": 200, \"request_id\": \"e075021751db2fedcaf1daf7e6408d10\", \"body\": {\"id\": \"chatcmpl-AprlxQdnDoYgdeBHR0ieSwayx0UsH\", \"object\": \"chat.completion\", \"created\": 1736924793, \"model\": \"gpt-4o-mini-2024-07-18\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"{\\n  \\\"MajorClaims\\\": [\\n    {\\n      \\\"ID\\\": \\\"MC1\\\",\\n      \\\"Text\\\": \\\"these taxes are absolutely essential\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"MC2\\\",\\n      \\\"Text\\\": \\\"taxes paying for state schools are necessary to be compulsory for all members of society no matter where their children enroll in\\\"\\n    }\\n  ],\\n  \\\"Claims\\\": [\\n    {\\n      \\\"ID\\\": \\\"C1\\\",\\n      \\\"Text\\\": \\\"affluent people effectively contribute to narrowing down the gap between rich and poor\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C2\\\",\\n      \\\"Text\\\": \\\"the tax reduction for parents of children studying in private schools wou\n"
     ]
    }
   ],
   "source": [
    "# Abrufen und Speichern der Batch-Ergebnisse (Output-Datei)\n",
    "batch_7_results = retrieve_and_save_batch_results(batch_7_status.output_file_id, \"output-batch-7\")\n",
    "print(batch_7_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading der Batch Input-Datei auf die OpenAI-Plattform\n",
    "batch_file_8 = upload_batch_file(BATCH_INPUT_PATH + \"batch_input_8.jsonl\")\n",
    "batch_file_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen eines Batches\n",
    "metadata_dict = {\"description\": \"Batch 8/9 for Argument Mining\"}\n",
    "batch_8 = create_batch(batch_file_8.id, metadata_dict) # sofern die Batch Datei bereits hochgeladen wurde, aber nicht erfolgreich war, kann die Batch-ID erneut verwendet werden. Ein erneuter Uplaod ist nicht notwendig und würde der Datei eine neue ID zuweisen.\n",
    "print(batch_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "batch_8_status = check_batch_status(batch_8.id)\n",
    "print(batch_8_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrufen und Speichern der Batch-Ergebnisse (Output-Datei)\n",
    "batch_8_results = retrieve_and_save_batch_results(batch_8_status.output_file_id, \"output-batch-8\")\n",
    "print(batch_8_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading der Batch Input-Datei auf die OpenAI-Plattform\n",
    "batch_file_9 = upload_batch_file(BATCH_INPUT_PATH + \"batch_input_9.jsonl\")\n",
    "batch_file_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen eines Batches\n",
    "metadata_dict = {\"description\": \"Batch 9/9 for Argument Mining\"}\n",
    "batch_9 = create_batch(batch_file_9.id, metadata_dict) # sofern die Batch Datei bereits hochgeladen wurde, aber nicht erfolgreich war, kann die Batch-ID erneut verwendet werden. Ein erneuter Uplaod ist nicht notwendig und würde der Datei eine neue ID zuweisen.\n",
    "print(batch_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "batch_9_status = check_batch_status(batch_9.id)\n",
    "print(batch_9_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrufen und Speichern der Batch-Ergebnisse (Output-Datei)\n",
    "batch_9_results = retrieve_and_save_batch_results(batch_9_status.output_file_id, \"output-batch-9\")\n",
    "print(batch_9_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ende Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot-Batch-Input:\n",
      "{\"custom_id\": \"zero-shot_essay001.txt\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o-mini\", \"messages\": [{\"role\": \"developer\", \"content\": \"You will be given a text. Extract the argumentative units major claim, claim, and premise as parts of the text. Also extract the argumentative relationships between the units. Claims can be for or against the major claims. Premises, on the other hand, can support or attack a claim or another premise. There may be several major claims. Return the argumentative units and the relationships between them as a JSON object.\"}, {\"role\": \"user\", \"content\": \"Text: Should students be taught to compete or to cooperate?\\n\\nIt is always said that competition can effectively promote the development of economy. In order to survive in the competition, companies continue to improve their products and service, and as a result, the whole society prospers. However, when we discuss the issue of competition or cooperation, what we are concern\n",
      "\n",
      "One-Shot-Batch-Input:\n",
      "{\"custom_id\": \"one-shot_essay001.txt\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4o-mini\", \"messages\": [{\"role\": \"developer\", \"content\": \"You will be given a text. Extract the argumentative units major claim, claim, and premise as parts of the text. Also extract the argumentative relationships between the units. Claims can be for or against the major claims. Premises, on the other hand, can support or attack a claim or another premise. There may be several major claims. Return the argumentative units and the relationships between them as a JSON object.Here is one example of a text and its corresponding json data:\\n## Input:\\nDo you think it is good for teenagers to work while schooling?\\n\\nIn my opinion, it is not the good idea for teenagers to have job while they are still students. Although, many argue that it provide good working experience, but I think it can interfere with their life in various ways. Having jobs would affect the health of the student.\n",
      "\n",
      "Few-Shot-10-Batch-Input:\n",
      "['few-shot-10_1.jsonl', 'few-shot-10_2.jsonl']\n",
      "\n",
      "Few-Shot-20-Batch-Input:\n",
      "['few-shot-20_1.jsonl', 'few-shot-20_2.jsonl', 'few-shot-20_3.jsonl']\n",
      "\n",
      "Few-Shot-40-Batch-Input:\n",
      "['few-shot-40_1.jsonl', 'few-shot-40_2.jsonl', 'few-shot-40_3.jsonl', 'few-shot-40_4.jsonl']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Erstellung der Batch-API-Dateien\n",
    "# zs_batch_input = generate_batch_input(test_df=test_df, prompt_df=zs_prompt_df, file_name='zero-shot')\n",
    "# os_batch_input = generate_batch_input(test_df=test_df, prompt_df=os_prompt_df, file_name='one-shot')\n",
    "# fs10_batch_input = generate_batch_input_split(test_df=test_df, prompt_df=fs10_prompt_df, file_name='few-shot-10', num_files=2)\n",
    "# fs20_batch_input = generate_batch_input_split(test_df=test_df, prompt_df=fs20_prompt_df, file_name='few-shot-20', num_files=3)\n",
    "# fs40_batch_input = generate_batch_input_split(test_df=test_df, prompt_df=fs40_prompt_df, file_name='few-shot-40', num_files=4)\n",
    "\n",
    "# # Ausgabe der ersten 1000 Zeichen der Batch-API-Dateien\n",
    "# print(f\"Zero-Shot-Batch-Input:\\n{zs_batch_input[:1000]}\")\n",
    "# print(f\"\\nOne-Shot-Batch-Input:\\n{os_batch_input[:1000]}\")\n",
    "# print(f\"\\nFew-Shot-10-Batch-Input:\\n{fs10_batch_input[:1000]}\")\n",
    "# print(f\"\\nFew-Shot-20-Batch-Input:\\n{fs20_batch_input[:1000]}\")\n",
    "# print(f\"\\nFew-Shot-40-Batch-Input:\\n{fs40_batch_input[:1000]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batches hochladen und erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Batches wurden aufgeteilt, damit sie einzeln hochgeladen werden können. Das wurde gemacht, dass falls es vereinzelt zu Fehlern kommt, nicht alle Batches neu erstellt und hochgeladen werden müssen. Damit sollen die Kosten reduziert werden. \n",
    "\n",
    "Beschreibung der Status Codes:\n",
    "\n",
    "| Status       | Description                                                                 |\n",
    "|--------------|-----------------------------------------------------------------------------|\n",
    "| validating   | the input file is being validated before the batch can begin                |\n",
    "| failed       | the input file has failed the validation process                            |\n",
    "| in_progress  | the input file was successfully validated and the batch is currently being run |\n",
    "| finalizing   | the batch has completed and the results are being prepared                  |\n",
    "| completed    | the batch has been completed and the results are ready                      |\n",
    "| expired      | the batch was not able to be completed within the 24-hour time window       |\n",
    "| cancelling   | the batch is being cancelled (may take up to 10 minutes)                    |\n",
    "| cancelled    | the batch was cancelled                                                     |\n",
    "\n",
    "Tabelle entnommen aus: https://platform.openai.com/docs/guides/batch/batch-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zero-shot batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot-Batch-File:\n",
      "FileObject(id='file-6hoKEvaGuHd6z4DPf1ACLc', bytes=6835954, created_at=1736100157, filename='zero-shot.jsonl', object='file', purpose='batch', status='processed', status_details=None)\n",
      "Batch(id='batch_677ac93e91848190b7c82d16cddf2a30', completion_window='24h', created_at=1736100158, endpoint='/v1/chat/completions', input_file_id='file-6hoKEvaGuHd6z4DPf1ACLc', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1736186558, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Zero-shot prompts'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Uploading the batch input file to OpenAI\n",
    "zs_batch_file = upload_batch_file(BATCH_INPUT_PATH + \"zero-shot.jsonl\")\n",
    "print(f\"Zero-Shot-Batch-File:\\n{zs_batch_file}\")\t\n",
    "\n",
    "# creating a batch\n",
    "metadata_dict = {\"description\": \"Zero-shot prompts\"}\n",
    "zs_batch = create_batch(zs_batch_file.id, metadata_dict) # sofern die Batch Datei bereits hochgeladen wurde, aber nicht erfolgreich war, kann die Batch-ID erneut verwendet werden. Ein erneuter Uplaod ist nicht notwendig und würde der Datei eine neue ID zuweisen.\n",
    "print(zs_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: completed\n",
      "\n",
      "Beschreibung des Batches: Zero-shot prompts\n",
      "Anfragen gesamt: 1448\n",
      "Davon erfolgreich: 1448\n",
      "Davon fehlerhaft: 0\n",
      "Erfolgreiche Abfragen können abgerufen werden mit ID: file-W59dkF32eVSB6pqiXp6zAF\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_677ac93e91848190b7c82d16cddf2a30', completion_window='24h', created_at=1736100158, endpoint='/v1/chat/completions', input_file_id='file-6hoKEvaGuHd6z4DPf1ACLc', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736102265, error_file_id=None, errors=None, expired_at=None, expires_at=1736186558, failed_at=None, finalizing_at=1736102103, in_progress_at=1736100160, metadata={'description': 'Zero-shot prompts'}, output_file_id='file-W59dkF32eVSB6pqiXp6zAF', request_counts=BatchRequestCounts(completed=1448, failed=0, total=1448))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "# Die Bearbeitung des Batches kann bis zu 24 Stunden dauern, funktioniert aber in der Regel schneller.\n",
    "zs_batch_id = zs_batch.id\n",
    "zs_batch_status = check_batch_status(zs_batch_id)\n",
    "print(zs_batch_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"batch_req_677ad0d784408190aaf56abea0bb14c8\", \"custom_id\": \"zero-shot_essay001.txt\", \"response\": {\"status_code\": 200, \"request_id\": \"8e5859b52be80ea2511383311d3c7c7a\", \"body\": {\"id\": \"chatcmpl-AmPFZZDNNOznxV2py3BY7O7iFRwh5\", \"object\": \"chat.completion\", \"created\": 1736100169, \"model\": \"gpt-4o-mini-2024-07-18\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"{\\n  \\\"MajorClaims\\\": [\\n    {\\n      \\\"ID\\\": \\\"MC1\\\",\\n      \\\"Text\\\": \\\"We should attach more importance to cooperation during primary education.\\\"\\n    }\\n  ],\\n  \\\"Claims\\\": [\\n    {\\n      \\\"ID\\\": \\\"C1\\\",\\n      \\\"Text\\\": \\\"Competition can effectively promote the development of economy.\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C2\\\",\\n      \\\"Text\\\": \\\"Competition makes the society more effective.\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C3\\\",\\n      \\\"Text\\\": \\\"A more cooperative attitude towards life is more profitable in one's success.\\\"\\n    }\\n  ],\\n  \\\"Premises\\\": [\\n    {\\n      \\\"ID\\\": \\\"P1\\\",\\n      \\\"Tex\n"
     ]
    }
   ],
   "source": [
    "# retrieve the results\n",
    "zs_batch_results = retrieve_and_save_batch_results(zs_batch_status.output_file_id, \"zero-shot\")\n",
    "print(zs_batch_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Achtung**\n",
    "\n",
    "Die Batches können nicht alle auf einmal erstellt werden, da es zu einem Fehler kommt. Es wird empfohlen die Batches einzeln in Auftrag zu geben, sobald der vorherige Batch abgeschlossen ist. Sofern eine Batch-Datei hochgeladen wurde und der Batch an sich jedoch fehlschlägt, kann die Batch-Datei weiterhin verwendet werden anhand der ID der Batch-Datei.\n",
    "\n",
    "\n",
    "Es kommt derzeit der Fehler:\n",
    "```\n",
    "'Enqueued token limit reached for gpt-4o-mini in organization. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.\n",
    "```\n",
    "Das Problem ist, dass keinerlei Batches noch im Status `in_progress` sind, wie anhand der Batch-Übersicht hervorgeht. Es wurde deshalb ein kleiner Batch erstellt, um zu sehen, ob das Problem weiterhin besteht. Es hat sich herausgestellt, dass es an der Größe der Batches liegt. Die Batches wurden deshalb weiter aufgeteilt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-Shot-Batch-File:\n",
      "FileObject(id='file-XLZRkTjEt4eqasQXqeb1E3', bytes=6835954, created_at=1736108729, filename='zero-shot.jsonl', object='file', purpose='batch', status='processed', status_details=None)\n",
      "Batch(id='batch_677aeabb672c8190a426ef895e470861', completion_window='24h', created_at=1736108731, endpoint='/v1/chat/completions', input_file_id='file-XLZRkTjEt4eqasQXqeb1E3', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1736195131, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'TEST-Batch für enqueued token limit error'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# # Testbatch, da enqueued token limit besteht, ohne in_progress batches.\n",
    "# # Uploading the batch input file to OpenAI\n",
    "# test_batch_file = upload_batch_file(BATCH_INPUT_PATH + \"zero-shot.jsonl\")\n",
    "# print(f\"Zero-Shot-Batch-File:\\n{test_batch_file}\")\t\n",
    "\n",
    "# # creating a batch\n",
    "# metadata_dict = {\"description\": \"TEST-Batch für enqueued token limit error\"}\n",
    "# test_batch = create_batch(test_batch_file.id, metadata_dict)\n",
    "# print(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: in_progress\n",
      "Der Batch wird noch verarbeitet. Bitte warten und später erneut prüfen.\n",
      "\n",
      "Beschreibung des Batches: TEST-Batch für enqueued token limit error\n",
      "Anfragen gesamt: 1448\n",
      "Davon erfolgreich: 1368\n",
      "Davon fehlerhaft: 0\n",
      "Keine erfolgreichen Abfragen zum herunterladen vorhanden.\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_677aeabb672c8190a426ef895e470861', completion_window='24h', created_at=1736108731, endpoint='/v1/chat/completions', input_file_id='file-XLZRkTjEt4eqasQXqeb1E3', object='batch', status='in_progress', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1736195131, failed_at=None, finalizing_at=None, in_progress_at=1736108733, metadata={'description': 'TEST-Batch für enqueued token limit error'}, output_file_id=None, request_counts=BatchRequestCounts(completed=1368, failed=0, total=1448))\n"
     ]
    }
   ],
   "source": [
    "# test_batch_id = test_batch.id\n",
    "# test_batch_status = check_batch_status(test_batch_id)\n",
    "# print(test_batch_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-shot batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Shot-Batch-File:\n",
      "FileObject(id='file-Be32Y3pStfTVfh4K4RfbAQ', bytes=17295582, created_at=1736108327, filename='one-shot.jsonl', object='file', purpose='batch', status='processed', status_details=None)\n"
     ]
    }
   ],
   "source": [
    "# Uploading the batch input file to OpenAI\n",
    "os_batch_file = upload_batch_file(BATCH_INPUT_PATH + \"one-shot.jsonl\")\n",
    "print(f\"One-Shot-Batch-File:\\n{os_batch_file}\")\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_677d78e3f50c8190b0fcb8f6952d790e', completion_window='24h', created_at=1736276196, endpoint='/v1/chat/completions', input_file_id='file-Be32Y3pStfTVfh4K4RfbAQ', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1736362596, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'One-shot prompts'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# creating a batch\n",
    "metadata_dict = {\"description\": \"One-shot prompts\"}\n",
    "os_batch = create_batch(os_batch_file.id, metadata_dict) \n",
    "print(os_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: completed\n",
      "\n",
      "Beschreibung des Batches: One-shot prompts\n",
      "Anfragen gesamt: 1448\n",
      "Davon erfolgreich: 1448\n",
      "Davon fehlerhaft: 0\n",
      "Erfolgreiche Abfragen können abgerufen werden mit ID: file-43SEJ54s21fWrHYpCzLpWJ\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_677d78e3f50c8190b0fcb8f6952d790e', completion_window='24h', created_at=1736276196, endpoint='/v1/chat/completions', input_file_id='file-Be32Y3pStfTVfh4K4RfbAQ', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736276955, error_file_id=None, errors=None, expired_at=None, expires_at=1736362596, failed_at=None, finalizing_at=1736276761, in_progress_at=1736276197, metadata={'description': 'One-shot prompts'}, output_file_id='file-43SEJ54s21fWrHYpCzLpWJ', request_counts=BatchRequestCounts(completed=1448, failed=0, total=1448))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "# Die Bearbeitung des Batches kann bis zu 24 Stunden dauern, funktioniert aber in der Regel schneller.\n",
    "os_batch_id = os_batch.id\n",
    "os_batch_status = check_batch_status(os_batch_id)\n",
    "print(os_batch_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"batch_req_677d7b1a53488190b868416d1ed544d4\", \"custom_id\": \"one-shot_essay001.txt\", \"response\": {\"status_code\": 200, \"request_id\": \"4eca3263efd58756290f8c00ec7c661e\", \"body\": {\"id\": \"chatcmpl-An92rPeQW7pbvzJn5N55rZXXqNvIe\", \"object\": \"chat.completion\", \"created\": 1736276205, \"model\": \"gpt-4o-mini-2024-07-18\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"{\\n  \\\"MajorClaims\\\": [\\n    {\\n      \\\"ID\\\": \\\"MC1\\\",\\n      \\\"Text\\\": \\\"we should attach more importance to cooperation during primary education\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"MC2\\\",\\n      \\\"Text\\\": \\\"a more cooperative attitude towards life is more profitable in one's success\\\"\\n    }\\n  ],\\n  \\\"Claims\\\": [\\n    {\\n      \\\"ID\\\": \\\"C1\\\",\\n      \\\"Text\\\": \\\"competition can effectively promote the development of economy\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C2\\\",\\n      \\\"Text\\\": \\\"competition makes the society more effective\\\"\\n    },\\n    {\\n      \\\"ID\\\": \\\"C3\\\",\\n      \\\"Text\\\": \\\"children can learn ab\n"
     ]
    }
   ],
   "source": [
    "# retrieve the results\n",
    "os_batch_results = retrieve_and_save_batch_results(os_batch_status.output_file_id, \"one-shot\")\n",
    "print(os_batch_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### few-shot 10 batch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FS 10 - Teil 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot-10-Batch-File:\n",
      "FileObject(id='file-Fb3NLF3YvyEeJpaWownQjk', bytes=47485684, created_at=1736325559, filename='few-shot-10_1.jsonl', object='file', purpose='batch', status='processed', status_details=None)\n"
     ]
    }
   ],
   "source": [
    "# Uploading the batch input file to OpenAI\n",
    "fs10_batch_file_1 = upload_batch_file(BATCH_INPUT_PATH + \"few-shot-10_1.jsonl\")\n",
    "print(f\"Few-Shot-10-Batch-File:\\n{fs10_batch_file_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_677ebf42a2d08190a61ba2683e773e9f', completion_window='24h', created_at=1736359746, endpoint='/v1/chat/completions', input_file_id='file-Fb3NLF3YvyEeJpaWownQjk', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1736446146, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Few-shot-10 prompts, Teil 1/2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# creating a batch\n",
    "metadata_dict = {\"description\": \"Few-shot-10 prompts, Teil 1/2\"}\n",
    "fs10_batch_1 = create_batch(fs10_batch_file_1.id, metadata_dict)\n",
    "print(fs10_batch_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: failed\n",
      "Error: Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 1,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list')\n",
      "\n",
      "Beschreibung des Batches: Few-shot-10 prompts, Teil 1/2\n",
      "Anfragen gesamt: 0\n",
      "Davon erfolgreich: 0\n",
      "Davon fehlerhaft: 0\n",
      "Keine erfolgreichen Abfragen zum herunterladen vorhanden.\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_677e39c2095c8190bb51b6e828fecc21', completion_window='24h', created_at=1736325570, endpoint='/v1/chat/completions', input_file_id='file-Fb3NLF3YvyEeJpaWownQjk', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 1,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736411970, failed_at=1736325573, finalizing_at=None, in_progress_at=None, metadata={'description': 'Few-shot-10 prompts, Teil 1/2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "# Die Bearbeitung des Batches kann bis zu 24 Stunden dauern, funktioniert aber in der Regel schneller.\n",
    "fs10_batch_1_status = check_batch_status('batch_677e39c2095c8190bb51b6e828fecc21')#(fs10_batch_1.id)\n",
    "print(fs10_batch_1_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the results\n",
    "fs10_batch_1_results = retrieve_and_save_batch_results(fs10_batch_1_status.output_file_id, \"few-shot-10_1\")\n",
    "print(fs10_batch_1_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FS 10 - Teil 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### few-shot 20 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot-20-Batch-File:\n",
      "FileObject(id='file-3MVA5GNc2ViHzsPZafh1u7', bytes=185953554, created_at=1736101926, filename='few-shot-20.jsonl', object='file', purpose='batch', status='processed', status_details=None)\n",
      "Batch(id='batch_677ad02cc7f48190baffdb04373f14c8', completion_window='24h', created_at=1736101932, endpoint='/v1/chat/completions', input_file_id='file-3MVA5GNc2ViHzsPZafh1u7', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1736188332, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Few-shot-20 prompts'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Uploading the batch input file to OpenAI\n",
    "fs20_batch_file = upload_batch_file(BATCH_INPUT_PATH + \"few-shot-20.jsonl\")\n",
    "print(f\"Few-Shot-20-Batch-File:\\n{fs20_batch_file}\")\n",
    "\n",
    "# creating a batch\n",
    "metadata_dict = {\"description\": \"Few-shot-20 prompts\"}\n",
    "fs20_batch = create_batch(fs20_batch_file.id, metadata_dict)\n",
    "print(fs20_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: failed\n",
      "Error: Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list')\n",
      "\n",
      "Beschreibung des Batches: Few-shot-20 prompts\n",
      "Anfragen gesamt: 0\n",
      "Davon erfolgreich: 0\n",
      "Davon fehlerhaft: 0\n",
      "Keine erfolgreichen Abfragen zum herunterladen vorhanden.\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_677ad02cc7f48190baffdb04373f14c8', completion_window='24h', created_at=1736101932, endpoint='/v1/chat/completions', input_file_id='file-3MVA5GNc2ViHzsPZafh1u7', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736188332, failed_at=1736101941, finalizing_at=None, in_progress_at=None, metadata={'description': 'Few-shot-20 prompts'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "# Die Bearbeitung des Batches kann bis zu 24 Stunden dauern, funktioniert aber in der Regel schneller.\n",
    "fs20_batch_id = fs20_batch.id\n",
    "fs20_batch_status = check_batch_status(fs20_batch_id)\n",
    "print(fs20_batch_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the results\n",
    "fs20_batch_results = retrieve_and_save_batch_results(fs20_batch_status.output_file_id, \"few-shot-20\")\n",
    "print(fs20_batch_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### few-shot 40 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot-40-Batch-File:\n",
      "FileObject(id='file-QgaYQyBBzLraPmpqW8xMtD', bytes=356775562, created_at=1736102017, filename='few-shot-40.jsonl', object='file', purpose='batch', status='processed', status_details=None)\n",
      "Batch(id='batch_677ad08ba60c8190af7404af995ec827', completion_window='24h', created_at=1736102027, endpoint='/v1/chat/completions', input_file_id='file-QgaYQyBBzLraPmpqW8xMtD', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1736188427, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Few-shot-40 prompts'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Uploading the batch input file to OpenAI\n",
    "fs40_batch_file = upload_batch_file(BATCH_INPUT_PATH + \"few-shot-40.jsonl\")\n",
    "print(f\"Few-Shot-40-Batch-File:\\n{fs40_batch_file}\")\n",
    "\n",
    "# creating a batch\n",
    "metadata_dict = {\"description\": \"Few-shot-40 prompts\"}\n",
    "fs40_batch = create_batch(fs40_batch_file.id, metadata_dict)\n",
    "print(fs40_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: failed\n",
      "Error: Errors(data=[BatchError(code='maximum_input_file_size_exceeded', line=None, message='The batch input file is larger than the 209715200 maximum for the gpt-4o-mini model. Please try again with a smaller batch.', param=None)], object='list')\n",
      "\n",
      "Beschreibung des Batches: Few-shot-40 prompts\n",
      "Anfragen gesamt: 0\n",
      "Davon erfolgreich: 0\n",
      "Davon fehlerhaft: 0\n",
      "Keine erfolgreichen Abfragen zum herunterladen vorhanden.\n",
      "Keine fehlerhaften Abfragen zum herunterladen vorhanden.\n",
      "\n",
      "Batch(id='batch_677ad08ba60c8190af7404af995ec827', completion_window='24h', created_at=1736102027, endpoint='/v1/chat/completions', input_file_id='file-QgaYQyBBzLraPmpqW8xMtD', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='maximum_input_file_size_exceeded', line=None, message='The batch input file is larger than the 209715200 maximum for the gpt-4o-mini model. Please try again with a smaller batch.', param=None)], object='list'), expired_at=None, expires_at=1736188427, failed_at=1736102033, finalizing_at=None, in_progress_at=None, metadata={'description': 'Few-shot-40 prompts'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "# Status des Batches abfragen - Diese Funktion kann mehrfach aufgerufen werden, um den Status des Batches zu überprüfen, ohne Zusatzkosten zu verursachen.\n",
    "# Die Bearbeitung des Batches kann bis zu 24 Stunden dauern, funktioniert aber in der Regel schneller.\n",
    "fs40_batch_id = fs40_batch.id\n",
    "fs40_batch_status = check_batch_status(fs40_batch_id)\n",
    "print(fs40_batch_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the results\n",
    "fs40_batch_results = retrieve_and_save_batch_results(fs40_batch_status.output_file_id, \"few-shot-40\")\n",
    "print(fs40_batch_results[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übersicht Batches für Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Batch(id='batch_6784d168bb288190bda2b11e3b42da3c', completion_window='24h', created_at=1736757608, endpoint='/v1/chat/completions', input_file_id='file-BAg7ZagvUiaPgNUeoPjxVc', object='batch', status='in_progress', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1736844008, failed_at=None, finalizing_at=None, in_progress_at=1736757613, metadata={'description': 'Batch 5/9 for Argument Mining'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=323)),\n",
       " Batch(id='batch_67837fec4f04819088f396053ef790f2', completion_window='24h', created_at=1736671212, endpoint='/v1/chat/completions', input_file_id='file-CUxcAUKiZJZ64KkV8YAsG4', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736673370, error_file_id=None, errors=None, expired_at=None, expires_at=1736757612, failed_at=None, finalizing_at=1736673312, in_progress_at=1736671215, metadata={'description': 'Batch 4/9 for Argument Mining'}, output_file_id='file-CTPtvsw1e79hupNn1R1DXf', request_counts=BatchRequestCounts(completed=549, failed=0, total=549)),\n",
       " Batch(id='batch_67822fc8058c8190b864f42b92ad646d', completion_window='24h', created_at=1736585160, endpoint='/v1/chat/completions', input_file_id='file-1VeM9QWs9utTGbREGjvauV', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736586694, error_file_id=None, errors=None, expired_at=None, expires_at=1736671560, failed_at=None, finalizing_at=1736586509, in_progress_at=1736585162, metadata={'description': 'Batch 3/9 for Argument Mining'}, output_file_id='file-Dn8EPeVM9BjT1VpRaaAnvS', request_counts=BatchRequestCounts(completed=622, failed=0, total=622)),\n",
       " Batch(id='batch_6780ded3cd58819080b492f6647492b4', completion_window='24h', created_at=1736498899, endpoint='/v1/chat/completions', input_file_id='file-NyvUx7xYfyFu6C4QNZ6qYs', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736499712, error_file_id=None, errors=None, expired_at=None, expires_at=1736585299, failed_at=None, finalizing_at=1736499609, in_progress_at=1736498902, metadata={'description': 'Batch 2/9 for Argument Mining'}, output_file_id='file-Fg7gsDHnWPhPKX5BLzQhaH', request_counts=BatchRequestCounts(completed=878, failed=0, total=878)),\n",
       " Batch(id='batch_677f7b3b1b48819086a5a27b7173c2c3', completion_window='24h', created_at=1736407867, endpoint='/v1/chat/completions', input_file_id='file-73X2eNMUnxhwLndGXvNdZ5', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736411765, error_file_id=None, errors=None, expired_at=None, expires_at=1736494267, failed_at=None, finalizing_at=1736411356, in_progress_at=1736407871, metadata={'description': 'Batch 1/9 for Argument Mining'}, output_file_id='file-Bwq5PPm4rnvL2jk7KFt2cL', request_counts=BatchRequestCounts(completed=3816, failed=0, total=3816)),\n",
       " Batch(id='batch_677ebf42a2d08190a61ba2683e773e9f', completion_window='24h', created_at=1736359746, endpoint='/v1/chat/completions', input_file_id='file-Fb3NLF3YvyEeJpaWownQjk', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736378056, error_file_id=None, errors=None, expired_at=None, expires_at=1736446146, failed_at=None, finalizing_at=1736377994, in_progress_at=1736359749, metadata={'description': 'Few-shot-10 prompts, Teil 1/2'}, output_file_id='file-L43RDXxdqRSDKCahDkoavj', request_counts=BatchRequestCounts(completed=724, failed=0, total=724)),\n",
       " Batch(id='batch_677e39c2095c8190bb51b6e828fecc21', completion_window='24h', created_at=1736325570, endpoint='/v1/chat/completions', input_file_id='file-Fb3NLF3YvyEeJpaWownQjk', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 1,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736411970, failed_at=1736325573, finalizing_at=None, in_progress_at=None, metadata={'description': 'Few-shot-10 prompts, Teil 1/2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677d78e3f50c8190b0fcb8f6952d790e', completion_window='24h', created_at=1736276196, endpoint='/v1/chat/completions', input_file_id='file-Be32Y3pStfTVfh4K4RfbAQ', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736276955, error_file_id=None, errors=None, expired_at=None, expires_at=1736362596, failed_at=None, finalizing_at=1736276761, in_progress_at=1736276197, metadata={'description': 'One-shot prompts'}, output_file_id='file-43SEJ54s21fWrHYpCzLpWJ', request_counts=BatchRequestCounts(completed=1448, failed=0, total=1448)),\n",
       " Batch(id='batch_677aeabb672c8190a426ef895e470861', completion_window='24h', created_at=1736108731, endpoint='/v1/chat/completions', input_file_id='file-XLZRkTjEt4eqasQXqeb1E3', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736112680, error_file_id=None, errors=None, expired_at=None, expires_at=1736195131, failed_at=None, finalizing_at=1736112532, in_progress_at=1736108733, metadata={'description': 'TEST-Batch für enqueued token limit error'}, output_file_id='file-FXKaN6kVXysq8NpXz6gAjb', request_counts=BatchRequestCounts(completed=1448, failed=0, total=1448)),\n",
       " Batch(id='batch_677ae92d6f98819092bf91bf3b65f966', completion_window='24h', created_at=1736108333, endpoint='/v1/chat/completions', input_file_id='file-Be32Y3pStfTVfh4K4RfbAQ', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736194733, failed_at=1736108336, finalizing_at=None, in_progress_at=None, metadata={'description': 'One-shot prompts'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677add2f73708190976dbcd1e7134cb0', completion_window='24h', created_at=1736105263, endpoint='/v1/chat/completions', input_file_id='file-UAhepbJUSWf38YwvKeVJJU', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736191663, failed_at=1736105265, finalizing_at=None, in_progress_at=None, metadata={'description': 'One-shot prompts'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677add0ea45c8190a812f521198b439e', completion_window='24h', created_at=1736105230, endpoint='/v1/chat/completions', input_file_id='file-65feVvReKp8XvrT8cwNETh', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736191630, failed_at=1736105231, finalizing_at=None, in_progress_at=None, metadata={'description': 'One-shot prompts'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677ad61953688190b3935d5c04120ed6', completion_window='24h', created_at=1736103449, endpoint='/v1/chat/completions', input_file_id='file-65feVvReKp8XvrT8cwNETh', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736189849, failed_at=1736103450, finalizing_at=None, in_progress_at=None, metadata={'description': 'One-shot prompts'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677ad08ba60c8190af7404af995ec827', completion_window='24h', created_at=1736102027, endpoint='/v1/chat/completions', input_file_id='file-QgaYQyBBzLraPmpqW8xMtD', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='maximum_input_file_size_exceeded', line=None, message='The batch input file is larger than the 209715200 maximum for the gpt-4o-mini model. Please try again with a smaller batch.', param=None)], object='list'), expired_at=None, expires_at=1736188427, failed_at=1736102033, finalizing_at=None, in_progress_at=None, metadata={'description': 'Few-shot-40 prompts'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677ad02cc7f48190baffdb04373f14c8', completion_window='24h', created_at=1736101932, endpoint='/v1/chat/completions', input_file_id='file-3MVA5GNc2ViHzsPZafh1u7', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736188332, failed_at=1736101941, finalizing_at=None, in_progress_at=None, metadata={'description': 'Few-shot-20 prompts'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677acffa215881909572908ee70f06ab', completion_window='24h', created_at=1736101882, endpoint='/v1/chat/completions', input_file_id='file-7PKrydrhPXnrzYdQd9GKSV', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736188282, failed_at=1736101884, finalizing_at=None, in_progress_at=None, metadata={'description': 'Few-shot-10 prompts'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677acfd983f0819086e2cb0b5a1c34f3', completion_window='24h', created_at=1736101849, endpoint='/v1/chat/completions', input_file_id='file-F4pyhUAZYchv7zywqHVxb7', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1736188249, failed_at=1736101851, finalizing_at=None, in_progress_at=None, metadata={'description': 'One-shot prompts'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0)),\n",
       " Batch(id='batch_677ac93e91848190b7c82d16cddf2a30', completion_window='24h', created_at=1736100158, endpoint='/v1/chat/completions', input_file_id='file-6hoKEvaGuHd6z4DPf1ACLc', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736102265, error_file_id=None, errors=None, expired_at=None, expires_at=1736186558, failed_at=None, finalizing_at=1736102103, in_progress_at=1736100160, metadata={'description': 'Zero-shot prompts'}, output_file_id='file-W59dkF32eVSB6pqiXp6zAF', request_counts=BatchRequestCounts(completed=1448, failed=0, total=1448)),\n",
       " Batch(id='batch_67797cb01b808190ad989fd916a1ad82', completion_window='24h', created_at=1736015024, endpoint='/v1/chat/completions', input_file_id='file-VQxa3cv37h2ZFQWQRoABiF', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736015103, error_file_id=None, errors=None, expired_at=None, expires_at=1736101424, failed_at=None, finalizing_at=1736015101, in_progress_at=1736015025, metadata={'description': 'Zero-shot prompts with 10 examples from the training set'}, output_file_id='file-YAdMgKGUNG5jRXbnyzjvF9', request_counts=BatchRequestCounts(completed=5, failed=0, total=5)),\n",
       " Batch(id='batch_67795bbda14c8190aea9620c503ef9b8', completion_window='24h', created_at=1736006589, endpoint='/v1/chat/completions', input_file_id='file-Guihg7JaKnVzic1n1BDLFC', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1736006728, error_file_id='file-B5AHnbVnR8wHfyfbqg5egr', errors=None, expired_at=None, expires_at=1736092989, failed_at=None, finalizing_at=1736006726, in_progress_at=1736006590, metadata={'description': 'Zero-shot prompts with 10 examples from the training set'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=10, total=10))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all batches\n",
    "batches_data = client.batches.list().data\n",
    "batches_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConflictError",
     "evalue": "Error code: 409 - {'error': {'message': \"Cannot cancel a batch with status 'completed'.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConflictError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[147], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Cancel the batch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_id \u001b[38;5;129;01min\u001b[39;00m cancel_batch:\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_id\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[1;32mc:\\Users\\ben-s\\anaconda3\\envs\\llm\\Lib\\site-packages\\openai\\resources\\batches.py:226\u001b[0m, in \u001b[0;36mBatches.cancel\u001b[1;34m(self, batch_id, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch_id:\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a non-empty value for `batch_id` but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_id\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/batches/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbatch_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/cancel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ben-s\\anaconda3\\envs\\llm\\Lib\\site-packages\\openai\\_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1268\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1275\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1277\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1278\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1279\u001b[0m     )\n\u001b[1;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\ben-s\\anaconda3\\envs\\llm\\Lib\\site-packages\\openai\\_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    955\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ben-s\\anaconda3\\envs\\llm\\Lib\\site-packages\\openai\\_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1045\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\ben-s\\anaconda3\\envs\\llm\\Lib\\site-packages\\openai\\_base_client.py:1095\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ben-s\\anaconda3\\envs\\llm\\Lib\\site-packages\\openai\\_base_client.py:1046\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1045\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1046\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\ben-s\\anaconda3\\envs\\llm\\Lib\\site-packages\\openai\\_base_client.py:1095\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ben-s\\anaconda3\\envs\\llm\\Lib\\site-packages\\openai\\_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1058\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1060\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1065\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1070\u001b[0m )\n",
      "\u001b[1;31mConflictError\u001b[0m: Error code: 409 - {'error': {'message': \"Cannot cancel a batch with status 'completed'.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "# cancel_batch = [batch.id for batch in batches_data if batch.metadata[\"description\"] == \"Zero-shot prompts with 10 examples from the training set\"]\n",
    "# cancel_batch\n",
    "\n",
    "# # Cancel the batch\n",
    "# for batch_id in cancel_batch:\n",
    "#     client.batches.cancel(batch_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: Batch 5/9 for Argument Mining\n",
      "Status: in_progress\n",
      "-------------------\n",
      "Metadata: Batch 4/9 for Argument Mining\n",
      "Status: completed\n",
      "-------------------\n",
      "Metadata: Batch 3/9 for Argument Mining\n",
      "Status: completed\n",
      "-------------------\n",
      "Metadata: Batch 2/9 for Argument Mining\n",
      "Status: completed\n",
      "-------------------\n",
      "Metadata: Batch 1/9 for Argument Mining\n",
      "Status: completed\n",
      "-------------------\n",
      "Metadata: Few-shot-10 prompts, Teil 1/2\n",
      "Status: completed\n",
      "-------------------\n",
      "Metadata: Few-shot-10 prompts, Teil 1/2\n",
      "Status: failed\n",
      "Errors: Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 1,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list')\n",
      "-------------------\n",
      "Metadata: One-shot prompts\n",
      "Status: completed\n",
      "-------------------\n",
      "Metadata: TEST-Batch für enqueued token limit error\n",
      "Status: completed\n",
      "-------------------\n",
      "Metadata: One-shot prompts\n",
      "Status: failed\n",
      "Errors: Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list')\n",
      "-------------------\n",
      "Metadata: One-shot prompts\n",
      "Status: failed\n",
      "Errors: Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list')\n",
      "-------------------\n",
      "Metadata: One-shot prompts\n",
      "Status: failed\n",
      "Errors: Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list')\n",
      "-------------------\n",
      "Metadata: One-shot prompts\n",
      "Status: failed\n",
      "Errors: Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list')\n",
      "-------------------\n",
      "Metadata: Few-shot-40 prompts\n",
      "Status: failed\n",
      "Errors: Errors(data=[BatchError(code='maximum_input_file_size_exceeded', line=None, message='The batch input file is larger than the 209715200 maximum for the gpt-4o-mini model. Please try again with a smaller batch.', param=None)], object='list')\n",
      "-------------------\n",
      "Metadata: Few-shot-20 prompts\n",
      "Status: failed\n",
      "Errors: Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list')\n",
      "-------------------\n",
      "Metadata: Few-shot-10 prompts\n",
      "Status: failed\n",
      "Errors: Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list')\n",
      "-------------------\n",
      "Metadata: One-shot prompts\n",
      "Status: failed\n",
      "Errors: Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4o-mini in organization org-1gP3awMqey1RnJpTBBhMoMPk. Limit: 2,000,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list')\n",
      "-------------------\n",
      "Metadata: Zero-shot prompts\n",
      "Status: completed\n",
      "-------------------\n",
      "Metadata: Zero-shot prompts with 10 examples from the training set\n",
      "Status: completed\n",
      "-------------------\n",
      "Metadata: Zero-shot prompts with 10 examples from the training set\n",
      "Status: completed\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "# Extract the status and errors of batches_data\n",
    "for batch in batches_data:\n",
    "    #print(f\"Batch ID: {batch.id}\")\n",
    "    print(f\"Metadata: {batch.metadata['description']}\")\n",
    "    print(f\"Status: {batch.status}\")\n",
    "    if batch.status == \"failed\":\n",
    "        print(f\"Errors: {batch.errors}\")\n",
    "    print(\"-------------------\")\n",
    "\n",
    "# batch_status = [batch.status for batch in batches_data]\n",
    "# meta_data = [batch.metadata for batch in batches_data]\n",
    "# if batch_status == \"failed\":\n",
    "#     batch_errors = [batch.errors for batch in batches_data]\n",
    "# else:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Übersicht hochgeladener Dateien für Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FileObject(id='file-BAg7ZagvUiaPgNUeoPjxVc', bytes=79286180, created_at=1736757463, filename='batch_input_5.jsonl', object='file', purpose='batch', status='processed', status_details=None),\n",
       " FileObject(id='file-CTPtvsw1e79hupNn1R1DXf', bytes=2157139, created_at=1736673370, filename='batch_67837fec4f04819088f396053ef790f2_output.jsonl', object='file', purpose='batch_output', status='processed', status_details=None),\n",
       " FileObject(id='file-CUxcAUKiZJZ64KkV8YAsG4', bytes=79467621, created_at=1736670809, filename='batch_input_4.jsonl', object='file', purpose='batch', status='processed', status_details=None),\n",
       " FileObject(id='file-Dn8EPeVM9BjT1VpRaaAnvS', bytes=2470578, created_at=1736586694, filename='batch_67822fc8058c8190b864f42b92ad646d_output.jsonl', object='file', purpose='batch_output', status='processed', status_details=None),\n",
       " FileObject(id='file-1VeM9QWs9utTGbREGjvauV', bytes=79782274, created_at=1736585156, filename='batch_input_3.jsonl', object='file', purpose='batch', status='processed', status_details=None),\n",
       " FileObject(id='file-Fg7gsDHnWPhPKX5BLzQhaH', bytes=3465988, created_at=1736499712, filename='batch_6780ded3cd58819080b492f6647492b4_output.jsonl', object='file', purpose='batch_output', status='processed', status_details=None),\n",
       " FileObject(id='file-NyvUx7xYfyFu6C4QNZ6qYs', bytes=80171245, created_at=1736498896, filename='batch_input_2.jsonl', object='file', purpose='batch', status='processed', status_details=None),\n",
       " FileObject(id='file-Bwq5PPm4rnvL2jk7KFt2cL', bytes=14172353, created_at=1736411765, filename='batch_677f7b3b1b48819086a5a27b7173c2c3_output.jsonl', object='file', purpose='batch_output', status='processed', status_details=None),\n",
       " FileObject(id='file-73X2eNMUnxhwLndGXvNdZ5', bytes=84798929, created_at=1736407855, filename='batch_input_1.jsonl', object='file', purpose='batch', status='processed', status_details=None),\n",
       " FileObject(id='file-L43RDXxdqRSDKCahDkoavj', bytes=2866004, created_at=1736378055, filename='batch_677ebf42a2d08190a61ba2683e773e9f_output.jsonl', object='file', purpose='batch_output', status='processed', status_details=None),\n",
       " FileObject(id='file-Fb3NLF3YvyEeJpaWownQjk', bytes=47485684, created_at=1736325559, filename='few-shot-10_1.jsonl', object='file', purpose='batch', status='processed', status_details=None),\n",
       " FileObject(id='file-43SEJ54s21fWrHYpCzLpWJ', bytes=6255340, created_at=1736276955, filename='batch_677d78e3f50c8190b0fcb8f6952d790e_output.jsonl', object='file', purpose='batch_output', status='processed', status_details=None),\n",
       " FileObject(id='file-FXKaN6kVXysq8NpXz6gAjb', bytes=4298125, created_at=1736112680, filename='batch_677aeabb672c8190a426ef895e470861_output.jsonl', object='file', purpose='batch_output', status='processed', status_details=None),\n",
       " FileObject(id='file-XLZRkTjEt4eqasQXqeb1E3', bytes=6835954, created_at=1736108729, filename='zero-shot.jsonl', object='file', purpose='batch', status='processed', status_details=None),\n",
       " FileObject(id='file-Be32Y3pStfTVfh4K4RfbAQ', bytes=17295582, created_at=1736108327, filename='one-shot.jsonl', object='file', purpose='batch', status='processed', status_details=None),\n",
       " FileObject(id='file-6hoKEvaGuHd6z4DPf1ACLc', bytes=6835954, created_at=1736100157, filename='zero-shot.jsonl', object='file', purpose='batch', status='processed', status_details=None)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_data = client.files.list().data\n",
    "files_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['file-UAhepbJUSWf38YwvKeVJJU', 'file-65feVvReKp8XvrT8cwNETh']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del_files_id = [file.id for file in files_data if file.filename.startswith(\"one-shot\")]\n",
    "# del_files_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Löschen einer hochgeladenen Datei\n",
    "# client.files.delete(\"file-7PKrydrhPXnrzYdQd9GKSV\")\n",
    "# for file_id in del_files_id:\n",
    "#     client.files.delete(file_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
