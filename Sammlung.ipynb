{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Notebook werden wird der Code gesammelt, der für die finale Abgabe der Masterarbeit nicht relevant ist, aber dennoch interessant sein könnte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "#eigene Module\n",
    "from src.dataimport import load_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON zu Pandas Dataframes\n",
    "MajorClaims, Claims, Premises und Argumentationsbeziehungen werden in einzelne Dataframes umgewandelt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_dataframe(json_data):\n",
    "    data = json.loads(json_data)\n",
    "    # Create a DataFrame for MajorClaims\n",
    "    major_claims_df = pd.DataFrame(data[\"MajorClaims\"])\n",
    "    major_claims_df[\"Type\"] = \"MajorClaim\"\n",
    "    # Create a DataFrame for Claims\n",
    "    claims_df = pd.DataFrame(data[\"Claims\"])\n",
    "    claims_df[\"Type\"] = \"Claim\"\n",
    "    # Create a DataFrame for Premises\n",
    "    premises_df = pd.DataFrame(data[\"Premises\"])\n",
    "    premises_df[\"Type\"] = \"Premise\"\n",
    "    # Create a DataFrame for ArgumentativeRelations\n",
    "    relations_df = pd.DataFrame(data[\"ArgumentativeRelations\"])\n",
    "\n",
    "    # for relations df calculate the types\n",
    "    relations_df[\"OriginType\"] = relations_df[\"Origin\"].apply(lambda x: x[0])\n",
    "    relations_df[\"TargetType\"] = relations_df[\"Target\"].apply(lambda x: x[0])\n",
    "\n",
    "    return major_claims_df, claims_df, premises_df, relations_df\n",
    "\n",
    "# Example usage\n",
    "json_data = load_text(transformed_json_files[0])\n",
    "\n",
    "major_claims_df, claims_df, premises_df, relations_df = json_to_dataframe(json_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "major_claims_df\n",
    "\n",
    "ID\tText\tType\n",
    "0\tMC1\twe should attach more importance to cooperatio...\tMajorClaim\n",
    "1\tMC2\ta more cooperative attitudes towards life is m...\tMajorClaim\n",
    "\n",
    "\n",
    "claims_df\n",
    "\n",
    "ID\tText\tType\n",
    "0\tC1\tthrough cooperation, children can learn about ...\tClaim\n",
    "1\tC2\tcompetition makes the society more effective\tClaim\n",
    "2\tC3\twithout the cooperation, there would be no vic...\tClaim\n",
    "\n",
    "\n",
    "premises_df\n",
    "\n",
    "ID\tText\tType\n",
    "0\tP1\tWhat we acquired from team work is not only ho...\tPremise\n",
    "1\tP2\tDuring the process of cooperation, children ca...\tPremise\n",
    "2\tP3\tAll of these skills help them to get on well w...\tPremise\n",
    "3\tP4\tthe significance of competition is that how to...\tPremise\n",
    "4\tP5\twhen we consider about the question that how t...\tPremise\n",
    "5\tP6\tTake Olympic games which is a form of competit...\tPremise\n",
    "\n",
    "\n",
    "relations_df\n",
    "\n",
    "Origin\tRelation\tTarget\tOriginType\n",
    "0\tC1\tFor\tMC\tC\n",
    "1\tP1\tsupports\tC1\tP\n",
    "2\tP2\tsupports\tC1\tP\n",
    "3\tP3\tsupports\tC1\tP\n",
    "4\tC2\tAgainst\tMC\tC\n",
    "5\tC3\tFor\tMC\tC\n",
    "6\tP6\tsupports\tC3\tP\n",
    "7\tP5\tsupports\tC3\tP\n",
    "8\tP4\tsupports\tC2\tP\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain hat bisher (Stand 12/24) noch keine Möglichkeit die Batch API von OpenAI zu verwenden, sondern nur über die Standard API. Dies geht unter anderem aus dem folgenden Forenbeitrag hervor: https://github.com/langchain-ai/langchain/discussions/21643\n",
    "\n",
    "Um die Kosten der Anfragen weiter zu reduzieren wurde der ursprüngliche Ansatz über LangChain verworfen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unterteilung in gleichgroße Teile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3], [4, 5, 6, 7]]\n",
      "Output 1:\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Output 2:\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "list = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "chunk_size = 4\n",
    "chunks = [list[i:i + chunk_size] for i in range(0, len(list), chunk_size)]\n",
    "print(chunks)\n",
    "\n",
    "for i , chunk in enumerate(chunks):\n",
    "    output = \"\\n\".join(str(item) for item in chunk)\n",
    "    print(f\"Output {i + 1}:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erstellung einer Batch API Anfrage als JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_input_split(test_df: pd.DataFrame, prompt_df: pd.DataFrame, file_name: str, num_files: int):\n",
    "    if num_files <= 0:\n",
    "        raise ValueError(\"num_files muss größer als 0 sein.\")\n",
    "\n",
    "    temperature = 0\n",
    "    llm_seed = 123\n",
    "    model = \"gpt-4o-mini\"  \n",
    "\n",
    "    dict_list = []\n",
    "    # iteration über Zero-Shot-Prompts\n",
    "    for _, prompt_row in prompt_df.iterrows():\n",
    "        # Iteration über Testdaten\n",
    "        for _, test_df_row in test_df.iterrows():\n",
    "            custom_id_str = prompt_row['prompt_name'] + \"_\" + test_df_row['txt_file']# + \"_\" + str(id_counter)\n",
    "            # write batch input for jsonl file\n",
    "            input_dict = {\"custom_id\": custom_id_str, \n",
    "                          \"method\": \"POST\", \"url\": \"/v1/chat/completions\",\n",
    "                          \"body\": {\"model\": model,\n",
    "                                   \"messages\": [{\"role\": \"developer\", \"content\": prompt_row['prompt_txt']}, # system Rolle wurde in developer umbenannt\n",
    "                                                {\"role\": \"user\", \"content\": \"Text: \" + test_df_row['txt']}], # user Rolle für Eingaben des Nutzers wie bei ChatGPT \n",
    "                                                \"temperature\": temperature,\n",
    "                                                \"seed\": llm_seed,\n",
    "                                                \"response_format\": {\n",
    "                                                    \"type\": \"json_schema\", # wichtig festzulegen, da sonst Fehlermeldung\n",
    "                                                    \"json_schema\": {\n",
    "                                                        \"name\": \"ArgumentMiningExtraction\", # wichtig festzulegen, da sonst Fehlermeldung\n",
    "                                                        \"schema\": response_format, # strukturiertes Output-Format von oben\n",
    "                                                        \"strict\": True \n",
    "                                                    }\n",
    "                                                    }\n",
    "                                                }\n",
    "                                     }\n",
    "            dict_list.append(input_dict)\n",
    "\n",
    "    \n",
    "    chunk_size = len(dict_list) // num_files + (len(dict_list) % num_files > 0) # Ermittelt die größe der Chunks, indem die Anzahl der Elemente durch die Anzahl der Dateien geteilt wird. Bei einem Rest wird ein Chunk mehr erstellt.\n",
    "    # Floor-Operator \"//\" dividiert und rundet auf die nächste ganze Zahl ab (Bsp.: 7 / 2 = 3.5 . 7 // 2 = 3). Modulo Operator \"%\" gibt den Rest der Division an. Wenn \n",
    "    chunks = [dict_list[i:i + chunk_size] for i in range(0, len(dict_list), chunk_size)] # Teilt die Liste in gleich große Teile auf. \n",
    "    # Beispiel für 7 Elemente (dict_list) und 2 Dateien (num_files): chunk_size = 7 // 2 + (7 % 2 > 0) = 4. cunks = [dict_list[0:4], dict_list[4:7]]. Intervall [0:4] = 0, 1, 2, 3. Intervall [4:7] = 4, 5, 6. Der letzte Index wird nicht mit eingeschlossen. \n",
    "\n",
    "    # Output in JSONL-Dateien schreiben\n",
    "    for i, chunk in enumerate(chunks): # enumerate iteriert über die Chunks-Liste und gibt den Index und das Element zurück\n",
    "        jsonl_output = \"\\n\".join(json.dumps(item) for item in chunk) # Schreibt die Elemente in einzelne Zeilen in einen JSONL-String\n",
    "        with open(f\"batch_api/input/{file_name}_{i + 1}.jsonl\", 'w') as f: # Schreibt den JSONL-String in eine Datei mit dem übergebenen Dateinamen und der fortlaufender Nummerierung\n",
    "            f.write(jsonl_output)\n",
    "\n",
    "    return [f\"{file_name}_{i + 1}.jsonl\" for i in range(num_files)] # Bezeichnet die Dateien fortlaufend anhand der Anzahl der Dateien\n",
    "\n",
    "\n",
    "# Quelle Batch API: https://platform.openai.com/docs/guides/batch?lang=python\n",
    "# Quelle text generation: https://platform.openai.com/docs/guides/text-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batches unterbrechen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancel_batch = [batch.id for batch in batches_data if batch.metadata[\"description\"] == \"Zero-shot prompts with 10 examples from the training set\"]\n",
    "# cancel_batch\n",
    "\n",
    "# # Cancel the batch\n",
    "# for batch_id in cancel_batch:\n",
    "#     client.batches.cancel(batch_id) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übersicht der auf OpenAI Platform hochgeladene Dateien für Client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_data = client.files.list().data\n",
    "for file in files_data:\n",
    "    print(f\"File-ID: {file.id}\")\n",
    "    print(f\"File-Name: {file.filename}\")\n",
    "    print(f\"File-Status: {file.status}\")\n",
    "    print(f\"-\" * 40) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# auf OpenAI Platform hochgeladene Dateien löschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Löschen einer hochgeladenen Datei\n",
    "# einzelne Datei\n",
    "client.files.delete(\"file-LEPdU2QustjMSEY5e1RSzR\")\n",
    "\n",
    "# Liste von Dateien\n",
    "del_files_id = [file.id for file in files_data if file.filename.startswith(\"one-shot\")]\n",
    "#del_files_id\n",
    "\n",
    "for file_id in del_files_id:\n",
    "     client.files.delete(file_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pfad aus Pfad inkl. Dateiname extrahieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'batch_api/input'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(\"batch_api/input/full_batch_input.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"{\"-\"*30}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframes als csv speichern und laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv speichern\n",
    "df.to_csv('dataframe.csv', index=False)\n",
    "\n",
    "# dataframe laden\n",
    "df = pd.read_csv('dataframe.csv')\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
